{"abstraction": "A common source of defects in manufacturing miniature Printed Circuits Boards\n(PCB) is the attachment of silicon die or other wire bondable components on a\nLiquid Crystal Polymer (LCP) substrate. Typically, a conductive glue is\ndispensed prior to attachment with defects caused either by insufficient or\nexcessive glue. The current practice in electronics industry is to examine the\ndeposited glue by a human operator a process that is both time consuming and\ninefficient especially in preproduction runs where the error rate is high. In\nthis paper we propose a system that automates fault diagnosis by accurately\nestimating the volume of glue deposits before and even after die attachment. To\nthis end a modular scanning system is deployed that produces high resolution\npoint clouds whereas the actual estimation of glue volume is performed by\n(R)egression-Net (RNet), a 3D Convolutional Neural Network (3DCNN). RNet\noutperforms other deep architectures and is able to estimate the volume either\ndirectly from the point cloud of a glue deposit or more interestingly after die\nattachment when only a small part of glue is visible around each die. The\nentire methodology is evaluated under operational conditions where the proposed\nsystem achieves accurate results without delaying the manufacturing process.", "title": "Fault Diagnosis in Microelectronics Attachment via Deep Learning Analysis of 3D Laser Scans", "author": "Nikolaos Dimitriou, Lampros Leontaris, Thanasis Vafeiadis, Dimosthenis Ioannidis, Tracy Wotherspoon, Gregory Tinker, Dimitrios Tzovaras", "published": "2020-02-25", "arxiv_url": "http://arxiv.org/abs/2002.10974v1", "arxiv_id": "2002.10974v1"}
{"abstraction": "A Bill of Materials (BoM) is a list of all components on a printed circuit\nboard (PCB). Since BoMs are useful for hardware assurance, automatic BoM\nextraction (AutoBoM) is of great interest to the government and electronics\nindustry. To achieve a high-accuracy AutoBoM process, domain knowledge of PCB\ntext and logos must be utilized. In this study, we discuss the challenges\nassociated with automatic PCB marking extraction and propose 1) a plan for\ncollecting salient PCB marking data, and 2) a framework for incorporating this\ndata for automatic PCB assurance. Given the proposed dataset plan and\nframework, subsequent future work, implications, and open research\npossibilities are detailed.", "title": "Framework for Automatic PCB Marking Detection and Recognition for Hardware Assurance", "author": "Olivia P. Dizon-Paradis, Daniel E. Capecci, Nathan T. Jessurun, Damon L. Woodard, Mark M. Tehranipoor, Navid Asadizanjani", "published": "2023-07-24", "arxiv_url": "http://arxiv.org/abs/2307.13105v1", "arxiv_id": "2307.13105v1"}
{"abstraction": "Printed Circuit Boards are the foundation for the functioning of any\nelectronic device, and therefore are an essential component for various\nindustries such as automobile, communication, computation, etc. However, one of\nthe challenges faced by the PCB manufacturers in the process of manufacturing\nof the PCBs is the faulty placement of its components including missing\ncomponents. In the present scenario the infrastructure required to ensure\nadequate quality of the PCB requires a lot of time and effort. The authors\npresent a novel solution for detecting missing components and classifying them\nin a resourceful manner. The presented algorithm focuses on pixel theory and\nobject detection, which has been used in combination to optimize the results\nfrom the given dataset.", "title": "PCB-Fire: Automated Classification and Fault Detection in PCB", "author": "Tejas Khare, Vaibhav Bahel, Anuradha C. Phadke", "published": "2021-02-22", "arxiv_url": "http://arxiv.org/abs/2102.10777v1", "arxiv_id": "2102.10777v1"}
{"abstraction": "Electrocardiogram (ECG) is a valuable tool for medical diagnosis used\nworldwide. Its use has contributed significantly to the prevention of\ncardiovascular diseases including infarctions. Although physicians need to see\nthe printed curves for a diagnosis, nowadays there exist automated tools based\non machine learning that can help diagnosis of arrhythmias and other\npathologies, these tools operate on digitalized ECG data that are merely\none-dimensional discrete signals (a kind of information that is much similar to\ndigitized audio). Thus, it is interesting to have both the graphical\ninformation and the digitized data. This is possible with modern, digital\nequipment. Nevertheless, there still exist many analog electrocardiogram\nmachines that plot results on paper with a printed gris measured in\nmillimeters. This paper presents a novel image analysis method that is capable\nof reading a printed ECG and converting it into a sampled digital signal.", "title": "Automated Optical Reading of Scanned ECGs", "author": "Manuel Pazos-Santomé, Fernando Martín-Rodríguez, Mónica Fernández-Barciela", "published": "2024-08-21", "arxiv_url": "http://arxiv.org/abs/2408.11425v1", "arxiv_id": "2408.11425v1"}
{"abstraction": "Quality control is of vital importance during electronics production. As the\nmethods of producing electronic circuits improve, there is an increasing chance\nof solder defects during assembling the printed circuit board (PCB). Many\ntechnologies have been incorporated for inspecting failed soldering, such as\nX-ray imaging, optical imaging, and thermal imaging. With some advanced\nalgorithms, the new technologies are expected to control the production quality\nbased on the digital images. However, current algorithms sometimes are not\naccurate enough to meet the quality control. Specialists are needed to do a\nfollow-up checking. For automated X-ray inspection, joint of interest on the\nX-ray image is located by region of interest (ROI) and inspected by some\nalgorithms. Some incorrect ROIs deteriorate the inspection algorithm. The high\ndimension of X-ray images and the varying sizes of image dimensions also\nchallenge the inspection algorithms. On the other hand, recent advances on deep\nlearning shed light on image-based tasks and are competitive to human levels.\nIn this paper, deep learning is incorporated in X-ray imaging based quality\ncontrol during PCB quality inspection. Two artificial intelligence (AI) based\nmodels are proposed and compared for joint defect detection. The noised ROI\nproblem and the varying sizes of imaging dimension problem are addressed. The\nefficacy of the proposed methods are verified through experimenting on a\nreal-world 3D X-ray dataset. By incorporating the proposed methods, specialist\ninspection workload is largely saved.", "title": "Deep Learning Based Defect Detection for Solder Joints on Industrial X-Ray Circuit Board Images", "author": "Qianru Zhang, Meng Zhang, Chinthaka Gamanayake, Chau Yuen, Zehao Geng, Hirunima Jayasekara, Xuewen Zhang, Chia-wei Woo, Jenny Low, Xiang Liu", "published": "2020-08-06", "arxiv_url": "http://arxiv.org/abs/2008.02604v2", "arxiv_id": "2008.02604v2"}
{"abstraction": "Printed Circuit boards (PCBs) are one of the most important stages in making\nelectronic products. A small defect in PCBs can cause significant flaws in the\nfinal product. Hence, detecting all defects in PCBs and locating them is\nessential. In this paper, we propose an approach based on denoising\nconvolutional autoencoders for detecting defective PCBs and to locate the\ndefects. Denoising autoencoders take a corrupted image and try to recover the\nintact image. We trained our model with defective PCBs and forced it to repair\nthe defective parts. Our model not only detects all kinds of defects and\nlocates them, but it can also repair them as well. By subtracting the repaired\noutput from the input, the defective parts are located. The experimental\nresults indicate that our model detects the defective PCBs with high accuracy\n(97.5%) compare to state of the art works.", "title": "PCB Defect Detection Using Denoising Convolutional Autoencoders", "author": "Saeed Khalilian, Yeganeh Hallaj, Arian Balouchestani, Hossein Karshenas, Amir Mohammadi", "published": "2020-08-28", "arxiv_url": "http://arxiv.org/abs/2008.12589v1", "arxiv_id": "2008.12589v1"}
{"abstraction": "Outsourced printed circuit board (PCB) fabrication necessitates increased\nhardware assurance capabilities. Several assurance techniques based on\nautomated optical inspection (AOI) have been proposed that leverage PCB images\nacquired using digital cameras. We review state-of-the-art AOI techniques and\nobserve a strong, rapid trend toward machine learning (ML) solutions. These\nrequire significant amounts of labeled ground truth data, which is lacking in\nthe publicly available PCB data space. We contribute the FICS PCB Image\nCollection (FPIC) dataset to address this need. Additionally, we outline new\nhardware security methodologies enabled by our data set.", "title": "FPIC: A Novel Semantic Dataset for Optical PCB Assurance", "author": "Nathan Jessurun, Olivia P. Dizon-Paradis, Jacob Harrison, Shajib Ghosh, Mark M. Tehranipoor, Damon L. Woodard, Navid Asadizanjani", "published": "2022-02-17", "arxiv_url": "http://arxiv.org/abs/2202.08414v2", "arxiv_id": "2202.08414v2"}
{"abstraction": "In the context of hardware trust and assurance, reverse engineering has been\noften considered as an illegal action. Generally speaking, reverse engineering\naims to retrieve information from a product, i.e., integrated circuits (ICs)\nand printed circuit boards (PCBs) in hardware security-related scenarios, in\nthe hope of understanding the functionality of the device and determining its\nconstituent components. Hence, it can raise serious issues concerning\nIntellectual Property (IP) infringement, the (in)effectiveness of\nsecurity-related measures, and even new opportunities for injecting hardware\nTrojans. Ironically, reverse engineering can enable IP owners to verify and\nvalidate the design. Nevertheless, this cannot be achieved without overcoming\nnumerous obstacles that limit successful outcomes of the reverse engineering\nprocess. This paper surveys these challenges from two complementary\nperspectives: image processing and machine learning. These two fields of study\nform a firm basis for the enhancement of efficiency and accuracy of reverse\nengineering processes for both PCBs and ICs. In summary, therefore, this paper\npresents a roadmap indicating clearly the actions to be taken to fulfill\nhardware trust and assurance objectives.", "title": "Hardware Trust and Assurance through Reverse Engineering: A Survey and Outlook from Image Analysis and Machine Learning Perspectives", "author": "Ulbert J. Botero, Ronald Wilson, Hangwei Lu, Mir Tanjidur Rahman, Mukhil A. Mallaiyan, Fatemeh Ganji, Navid Asadizanjani, Mark M. Tehranipoor, Damon L. Woodard, Domenic Forte", "published": "2020-02-11", "arxiv_url": "http://arxiv.org/abs/2002.04210v2", "arxiv_id": "2002.04210v2"}
{"abstraction": "Addressing the critical theme of recycling electronic waste (E-waste), this\ncontribution is dedicated to developing advanced automated data processing\npipelines as a basis for decision-making and process control. Aligning with the\nbroader goals of the circular economy and the United Nations (UN) Sustainable\nDevelopment Goals (SDG), our work leverages non-invasive analysis methods\nutilizing RGB and hyperspectral imaging data to provide both quantitative and\nqualitative insights into the E-waste stream composition for optimizing\nrecycling efficiency. In this paper, we introduce 'PCB-Vision'; a pioneering\nRGB-hyperspectral printed circuit board (PCB) benchmark dataset, comprising 53\nRGB images of high spatial resolution paired with their corresponding high\nspectral resolution hyperspectral data cubes in the visible and near-infrared\n(VNIR) range. Grounded in open science principles, our dataset provides a\ncomprehensive resource for researchers through high-quality ground truths,\nfocusing on three primary PCB components: integrated circuits (IC), capacitors,\nand connectors. We provide extensive statistical investigations on the proposed\ndataset together with the performance of several state-of-the-art (SOTA)\nmodels, including U-Net, Attention U-Net, Residual U-Net, LinkNet, and\nDeepLabv3+. By openly sharing this multi-scene benchmark dataset along with the\nbaseline codes, we hope to foster transparent, traceable, and comparable\ndevelopments of advanced data processing across various scientific communities,\nincluding, but not limited to, computer vision and remote sensing. Emphasizing\nour commitment to supporting a collaborative and inclusive scientific\ncommunity, all materials, including code, data, ground truth, and masks, will\nbe accessible at https://github.com/hifexplo/PCBVision.", "title": "PCB-Vision: A Multiscene RGB-Hyperspectral Benchmark Dataset of Printed Circuit Boards", "author": "Elias Arbash, Margret Fuchs, Behnood Rasti, Sandra Lorenz, Pedram Ghamisi, Richard Gloaguen", "published": "2024-01-12", "arxiv_url": "http://arxiv.org/abs/2401.06528v1", "arxiv_id": "2401.06528v1"}
{"abstraction": "We present the design, fabrication, and experimental characterization of a\ntwo-dimensional, dynamically tuned, metasurface aperture, emphasizing its\npotential performance in computational imaging applications. The dynamic\nmetasurface aperture (DMA) consists of an irregular, planar cavity that feeds a\nmultitude of tunable metamaterial elements, all fabricated in a compact,\nmultilayer printed circuit board process. The design considerations for the\nmetamaterial element as a tunable radiator, the associated biasing circuitry,\nas well as cavity parameters are examined and discussed. A sensing matrix can\nbe constructed from the measured transmit patterns, the singular value spectrum\nof which provides insight into the information capacity of the apertures. We\ninvestigate the singular value spectra of the sensing matrix over a variety of\noperating parameters, such as the number of metamaterial elements, number of\nmasks, and number of radiating elements. After optimizing over these key\nparameters, we demonstrate computational microwave imaging of simple test\nobjects.", "title": "Implementation and Characterization of a Two-Dimensional Printed Circuit Dynamic Metasurface Aperture for Computational Microwave Imaging", "author": "Timothy Sleasman, Mohammadreza F. Imani, Aaron V. Diebold, Michael Boyarsky, Kenneth P. Trofatter, David R. Smith", "published": "2019-10-20", "arxiv_url": "http://arxiv.org/abs/1911.08952v1", "arxiv_id": "1911.08952v1"}
{"abstraction": "Surface mount technology (SMT) is a process for producing printed circuit\nboards. Solder paste printer (SPP), package mounter, and solder reflow oven are\nused for SMT. The board on which the solder paste is deposited from the SPP is\nmonitored by solder paste inspector (SPI). If SPP malfunctions due to the\nprinter defects, the SPP produces defective products, and then abnormal\npatterns are detected by SPI. In this paper, we propose a convolutional\nrecurrent reconstructive network (CRRN), which decomposes the anomaly patterns\ngenerated by the printer defects, from SPI data. CRRN learns only normal data\nand detects anomaly pattern through reconstruction error. CRRN consists of a\nspatial encoder (S-Encoder), a spatiotemporal encoder and decoder\n(ST-Encoder-Decoder), and a spatial decoder (S-Decoder). The ST-Encoder-Decoder\nconsists of multiple convolutional spatiotemporal memories (CSTMs) with\nST-Attention mechanism. CSTM is developed to extract spatiotemporal patterns\nefficiently. Additionally, a spatiotemporal attention (ST-Attention) mechanism\nis designed to facilitate transmitting information from the ST-Encoder to the\nST-Decoder, which can solve the long-term dependency problem. We demonstrate\nthe proposed CRRN outperforms the other conventional models in anomaly\ndetection. Moreover, we show the discriminative power of the anomaly map\ndecomposed by the proposed CRRN through the printer defect classification.", "title": "Convolutional Recurrent Reconstructive Network for Spatiotemporal Anomaly Detection in Solder Paste Inspection", "author": "Yong-Ho Yoo, Ue-Hwan Kim, Jong-Hwan Kim", "published": "2019-08-22", "arxiv_url": "http://arxiv.org/abs/1908.08204v1", "arxiv_id": "1908.08204v1"}
{"abstraction": "To extend the antenna design on printed circuit boards (PCBs) for more\nengineers of interest, we propose a simple method that models PCB antennas with\na few basic components. By taking two separate steps to decide their geometric\ndimensions and positions, antenna prototypes can be facilitated with no\nexperience required. Random sampling statistics relate to the quality of\ndimensions are used in selecting among dimension candidates. A novel\nimage-based classifier using a convolutional neural network (CNN) is introduced\nto further determine the positions of these fixed-dimension components. Two\nexamples from wearable products have been chosen to examine the entire\nworkflow. Their final designs are realistic and their performance metrics are\nnot inferior to the ones designed by experienced engineers.", "title": "Image Classifier Based Generative Method for Planar Antenna Design", "author": "Yang Zhong, Weiping Dou, Andrew Cohen, Dia'a Bisharat, Yuandong Tian, Jiang Zhu, Qing Huo Liu", "published": "2023-12-16", "arxiv_url": "http://arxiv.org/abs/2401.06149v1", "arxiv_id": "2401.06149v1"}
{"abstraction": "Ultrasound image diagnosis of breast tumors has been widely used in recent\nyears. However, there are some problems of it, for instance, poor quality,\nintense noise and uneven echo distribution, which has created a huge obstacle\nto diagnosis. To overcome these problems, we propose a novel method, a breast\ncancer classification with ultrasound images based on SLIC (BCCUI). We first\nutilize the Region of Interest (ROI) extraction based on Simple Linear\nIterative Clustering (SLIC) algorithm and region growing algorithm to extract\nthe ROI at the super-pixel level. Next, the features of ROI are extracted.\nFurthermore, the Support Vector Machine (SVM) classifier is applied. The\ncalculation states that the accuracy of this segment algorithm is up to 88.00%\nand the sensitivity of the algorithm is up to 92.05%, which proves that the\nclassifier presents in this paper has certain research meaning and applied\nworthiness.", "title": "Breast Cancer Classification with Ultrasound Images Based on SLIC", "author": "Zhihao Fang, Wanyi Zhang, He Ma", "published": "2019-04-25", "arxiv_url": "http://arxiv.org/abs/1904.11322v2", "arxiv_id": "1904.11322v2"}
{"abstraction": "To this date, studies focusing on the prodromal diagnosis of Lewy body\ndiseases (LBDs) based on quantitative analysis of graphomotor and handwriting\ndifficulties are missing. In this work, we enrolled 18 subjects diagnosed with\npossible or probable mild cognitive impairment with Lewy bodies (MCI-LB), 7\nsubjects having more than 50% probability of developing Parkinson's disease\n(PD), 21 subjects with both possible/probable MCI-LB and probability of PD >\n50%, and 37 age- and gender-matched healthy controls (HC). Each participant\nperformed three tasks: Archimedean spiral drawing (to quantify graphomotor\ndifficulties), sentence writing task (to quantify handwriting difficulties),\nand pentagon copying test (to quantify cognitive decline). Next, we\nparameterized the acquired data by various temporal, kinematic, dynamic,\nspatial, and task-specific features. And finally, we trained classification\nmodels for each task separately as well as a model for their combination to\nestimate the predictive power of the features for the identification of LBDs.\nUsing this approach we were able to identify prodromal LBDs with 74% accuracy\nand showed the promising potential of computerized objective and non-invasive\ndiagnosis of LBDs based on the assessment of graphomotor and handwriting\ndifficulties.", "title": "Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of Graphomotor and Handwriting Difficulties", "author": "Zoltan Galaz, Jiri Mekyska, Jan Mucha, Vojtech Zvoncak, Zdenek Smekal, Marcos Faundez-Zanuy, Lubos Brabenec, Ivona Moravkova, Irena Rektorova", "published": "2023-01-20", "arxiv_url": "http://arxiv.org/abs/2301.08534v1", "arxiv_id": "2301.08534v1"}
{"abstraction": "The outbreak of COVID-19 exposed the inadequacy of our technical tools for\nhome health surveillance, and recent studies have shown the potential of\nsmartphones as a universal optical microscopic imaging platform for such\napplications. However, most of them use laboratory-grade optomechanical\ncomponents and transmitted illuminations to ensure focus tuning capability and\nimaging quality, which keeps the cost of the equipment high. Here we propose an\nultra-low-cost solution for smartphone microscopy. To realize focus tunability,\nwe designed a seesaw-like structure capable of converting large displacements\non one side into small displacements on the other (reduced to ~9.1%), which\nleverages the intrinsic flexibility of 3D printing materials. We achieved a\nfocus-tuning accuracy of ~5 micron, which is 40 times higher than the machining\naccuracy of the 3D-printed lens holder itself. For microscopic imaging, we use\nan off-the-shelf smartphone camera lens as the objective and the built-in\nflashlight as the illumination. To compensate for the resulting image quality\ndegradation, we developed a learning-based image enhancement method. We use the\nCycleGAN architecture to establish the mapping from smartphone microscope\nimages to benchtop microscope images without pairing. We verified the imaging\nperformance on different biomedical samples. Except for the smartphone, we kept\nthe full costs of the device under 4 USD. We think these efforts to lower the\ncosts of smartphone microscopes will benefit their applications in various\nscenarios, such as point-of-care testing, on-site diagnosis, and home health\nsurveillance.", "title": "Towards ultra-low-cost smartphone microscopy", "author": "Haoran Zhang, Weiyi Zhang, Zirui Zuo, Jianlong Yang", "published": "2023-11-28", "arxiv_url": "http://arxiv.org/abs/2312.11479v1", "arxiv_id": "2312.11479v1"}
{"abstraction": "In this paper, the design, realization, and demonstration of a broadband\nmillimeter-wave imaging system based on the synthetic aperture radar technique\n(SAR) are discussed. The proposed system, operating within the frequency range\nof 25.3-30.8 GHz, consists of a tapered slot antenna as the transmitter and two\nhalf-circle antennas as the receivers. The size of the antenna is 19.5 * 8 mm\nwith a maximum gain of 8.5 dB. The transmitter and the receiver antennas are\nprinted on the same board. This feature leads to a highly compact and flexible\nconfiguration, enabling the applicability of the proposed imaging system in the\nhandheld devices. Furthermore, it significantly reduces the fabrication cost of\nthe system. The proposed broadband imaging system, being capable of performing\n3D real-time imaging with high resolutions, can be easily calibrated for each\nfrequency within the desired range. By performing 3D imaging from metallic\nobjects with different shapes, we experimentally demonstrate the high\nperformance of the proposed system, which offers great potentialities for a\nbroad range of applications such as security, medical diagnostic, concealed\nobject detection, to name a few.", "title": "A Broadband and Compact Millimeter-Wave Imaging System based on Synthetic Aperture Radar", "author": "Fahimeh Sepehripour, Ahmad Shafiei Alavijeh, Mohammad Fakharzadeh, Amin Khavasi", "published": "2022-05-29", "arxiv_url": "http://arxiv.org/abs/2205.14707v1", "arxiv_id": "2205.14707v1"}
{"abstraction": "The digital revolution has replaced the use of printed documents with their\ndigital counterparts. However, many applications require the use of both due to\nseveral factors, including challenges of digital security, installation costs,\nease of use, and lack of digital expertise. Technological developments in the\ndigital domain have also resulted in the easy availability of high-quality\nscanners, printers, and image editing software at lower prices. Miscreants\nleverage such technology to develop forged documents that may go undetected in\nvast volumes of printed documents. These developments mandate the research on\ncreating fast and accurate digital systems for source printer identification of\nprinted documents. We extensively analyze and propose a printer-specific\npooling that improves the performance of printer-specific local texture\ndescriptor on two datasets. The proposed pooling performs well using a simple\ncorrelation-based prediction instead of a complex machine learning-based\nclassifier achieving improved performance under cross-font scenarios. The\nproposed system achieves an average classification accuracy of 93.5%, 94.3%,\nand 60.3% on documents printed in Arial, Times New Roman, and Comic Sans font\ntypes respectively, when documents printed in only Cambria font are available\nfor training.", "title": "Source Printer Identification using Printer Specific Pooling of Letter Descriptors", "author": "Sharad Joshi, Yogesh Kumar Gupta, Nitin Khanna", "published": "2021-09-23", "arxiv_url": "http://arxiv.org/abs/2109.11139v1", "arxiv_id": "2109.11139v1"}
{"abstraction": "Additive manufacturing (AM), particularly 3D printing, has revolutionized the\nproduction of complex structures across various industries. However, ensuring\nquality and detecting defects in 3D-printed objects remain significant\nchallenges. This study focuses on improving defect detection in 3D-printed\ncylinders by integrating novel pre-processing techniques such as Region of\nInterest (ROI) selection, Histogram Equalization (HE), and Details Enhancer\n(DE) with Convolutional Neural Networks (CNNs), specifically the modified VGG16\nmodel. The approaches, ROIN, ROIHEN, and ROIHEDEN, demonstrated promising\nresults, with the best model achieving an accuracy of 1.00 and an F1-score of\n1.00 on the test set. The study also explored the models' interpretability\nthrough Local Interpretable Model-Agnostic Explanations and Gradient-weighted\nClass Activation Mapping, enhancing the understanding of the decision-making\nprocess. Furthermore, the modified VGG16 model showed superior computational\nefficiency with 30713M FLOPs and 15M parameters, the lowest among the compared\nmodels. These findings underscore the significance of tailored pre-processing\nand CNNs in enhancing defect detection in AM, offering a pathway to improve\nmanufacturing precision and efficiency. This research not only contributes to\nthe advancement of 3D printing technology but also highlights the potential of\nintegrating machine learning with AM for superior quality control.", "title": "Defect Localization Using Region of Interest and Histogram-Based Enhancement Approaches in 3D-Printing", "author": "Md Manjurul Ahsan, Shivakumar Raman, Zahed Siddique", "published": "2024-04-25", "arxiv_url": "http://arxiv.org/abs/2404.17015v1", "arxiv_id": "2404.17015v1"}
{"abstraction": "Printed Electronics (PE) provide a mechanically flexible and cost-effective\nsolution for machine learning (ML) circuits, compared to silicon-based\ntechnologies. However, due to large feature sizes, printed classifiers are\nlimited by high power, area, and energy overheads, which restricts the\nrealization of battery-powered systems. In this work, we design sequential\nprinted bespoke Support Vector Machine (SVM) circuits that adhere to the power\nconstraints of existing printed batteries while minimizing energy consumption,\nthereby boosting battery life. Our results show 6.5x energy savings while\nmaintaining higher accuracy compared to the state of the art.", "title": "Late Breaking Results: Energy-Efficient Printed Machine Learning Classifiers with Sequential SVMs", "author": "Spyridon Besias, Ilias Sertaridis, Florentia Afentaki, Konstantinos Balaskas, Georgios Zervakis", "published": "2025-01-28", "arxiv_url": "http://arxiv.org/abs/2501.16828v1", "arxiv_id": "2501.16828v1"}
{"abstraction": "Graininess noise is a common artifact in inkjet printing. While current\ninkjet printing technologies attempt to control graininess in single color\nimages, the results are often less than optimal for multi-color images. This is\ndue to fluidic interactions between inks of different colors. This paper will\ndescribe a color decomposition methodology that can be used to study ink flow\npatterns in multi-color inkjet printed images at a microscopic scale. This\ntechnique is used to decompose multi-color images into several independent\ncolor components. The ink patterns in these components is analyzed to relate\nthem to visually perceptible graininess noise.", "title": "Ink Flow Patterns In Multi Color Inkjet Images And Their Impact On Graininess Noise", "author": "Qiulin Chen, Palghat Ramesh, Chu-heng Liu, Jan P. Allebach", "published": "2019-12-18", "arxiv_url": "http://arxiv.org/abs/1912.08780v1", "arxiv_id": "1912.08780v1"}
{"abstraction": "All color-difference formulas are developed to evaluate color differences for\npairs of stimuli with hair-line separation. In printing applications, however,\ncolor differences are frequently judged between a pair of samples with\nno-separation because they are printed adjacent on the same piece of paper. A\nnew formula, dENS has been developed for pairs of stimuli with no-separation\n(NS). An experiment was conducted to investigate the effect of different\ncolor-difference magnitudes using sample pairs with NS. 1,012 printed pairs\nwith NS were prepared around 11 CIE recommended color centers. The pairs,\nrepresenting four color-difference magnitudes of 1, 2, 4 and 8 CIELAB units\nwere visually evaluated by a panel of 19 observers using the gray-scale method.\nComparison of the present data based on pairs with NS, and previously generated\ndata using pairs with hair-line separation, showed a clear separation effect. A\nnew color-difference equation for the NS viewing condition (dENS) is proposed\nby modifying the CIEDE2000 formula. The separation effect can be well described\nby the new formula. For a sample pair with NS, when the CIEDE2000 color\ndifference is less than 9.1, a larger color difference leads to a larger\nlightness difference, and thus the total color difference increases. When the\nCIEDE2000 color difference is greater than 9.1, the effect is opposite, i.e.\nthe lightness difference decreases, and thus the total color difference also\ndecreases. The new formula is recommended for future research to evaluate its\nperformance in appropriate applications.", "title": "A color-difference formula for evaluating color pairs with no separation -- $ΔE_{NS}$", "author": "Fereshteh Mirjalili, Ming Ronnier Luo, Guihua Cui, Jan Morovic", "published": "2019-04-25", "arxiv_url": "http://arxiv.org/abs/1904.11293v1", "arxiv_id": "1904.11293v1"}
{"abstraction": "Low level image restoration is an integral component of modern artificial\nintelligence (AI) driven camera pipelines. Most of these frameworks are based\non deep neural networks which present a massive computational overhead on\nresource constrained platform like a mobile phone. In this paper, we propose\nseveral lightweight low-level modules which can be used to create a\ncomputationally low cost variant of a given baseline model. Recent works for\nefficient neural networks design have mainly focused on classification.\nHowever, low-level image processing falls under the image-to-image' translation\ngenre which requires some additional computational modules not present in\nclassification. This paper seeks to bridge this gap by designing generic\nefficient modules which can replace essential components used in contemporary\ndeep learning based image restoration networks. We also present and analyse our\nresults highlighting the drawbacks of applying depthwise separable\nconvolutional kernel (a popular method for efficient classification network)\nfor sub-pixel convolution based upsampling (a popular upsampling strategy for\nlow-level vision applications). This shows that concepts from domain of\nclassification cannot always be seamlessly integrated into image-to-image\ntranslation tasks. We extensively validate our findings on three popular tasks\nof image inpainting, denoising and super-resolution. Our results show that\nproposed networks consistently output visually similar reconstructions compared\nto full capacity baselines with significant reduction of parameters, memory\nfootprint and execution speeds on contemporary mobile devices.", "title": "Lightweight Modules for Efficient Deep Learning based Image Restoration", "author": "Avisek Lahiri, Sourav Bairagya, Sutanu Bera, Siddhant Haldar, Prabir Kumar Biswas", "published": "2020-07-11", "arxiv_url": "http://arxiv.org/abs/2007.05835v1", "arxiv_id": "2007.05835v1"}
{"abstraction": "Chest radiography is one of the most common types of diagnostic radiology\nexams, which is critical for screening and diagnosis of many different thoracic\ndiseases. Specialized algorithms have been developed to detect several specific\npathologies such as lung nodule or lung cancer. However, accurately detecting\nthe presence of multiple diseases from chest X-rays (CXRs) is still a\nchallenging task. This paper presents a supervised multi-label classification\nframework based on deep convolutional neural networks (CNNs) for predicting the\nrisk of 14 common thoracic diseases. We tackle this problem by training\nstate-of-the-art CNNs that exploit dependencies among abnormality labels. We\nalso propose to use the label smoothing technique for a better handling of\nuncertain samples, which occupy a significant portion of almost every CXR\ndataset. Our model is trained on over 200,000 CXRs of the recently released\nCheXpert dataset and achieves a mean area under the curve (AUC) of 0.940 in\npredicting 5 selected pathologies from the validation set. This is the highest\nAUC score yet reported to date. The proposed method is also evaluated on the\nindependent test set of the CheXpert competition, which is composed of 500 CXR\nstudies annotated by a panel of 5 experienced radiologists. The performance is\non average better than 2.6 out of 3 other individual radiologists with a mean\nAUC of 0.930, which ranks first on the CheXpert leaderboard at the time of\nwriting this paper.", "title": "Interpreting chest X-rays via CNNs that exploit hierarchical disease dependencies and uncertainty labels", "author": "Hieu H. Pham, Tung T. Le, Dat Q. Tran, Dat T. Ngo, Ha Q. Nguyen", "published": "2019-11-15", "arxiv_url": "http://arxiv.org/abs/1911.06475v3", "arxiv_id": "1911.06475v3"}
{"abstraction": "Diabetic Retinopathy (DR), an ocular complication of diabetes, is a leading\ncause of blindness worldwide. Traditionally, DR is monitored using Color Fundus\nPhotography (CFP), a widespread 2-D imaging modality. However, DR\nclassifications based on CFP have poor predictive power, resulting in\nsuboptimal DR management. Optical Coherence Tomography Angiography (OCTA) is a\nrecent 3-D imaging modality offering enhanced structural and functional\ninformation (blood flow) with a wider field of view. This paper investigates\nautomatic DR severity assessment using 3-D OCTA. A straightforward solution to\nthis task is a 3-D neural network classifier. However, 3-D architectures have\nnumerous parameters and typically require many training samples. A lighter\nsolution consists in using 2-D neural network classifiers processing 2-D\nen-face (or frontal) projections and/or 2-D cross-sectional slices. Such an\napproach mimics the way ophthalmologists analyze OCTA acquisitions: 1) en-face\nflow maps are often used to detect avascular zones and neovascularization, and\n2) cross-sectional slices are commonly analyzed to detect macular edemas, for\ninstance. However, arbitrary data reduction or selection might result in\ninformation loss. Two complementary strategies are thus proposed to optimally\nsummarize OCTA volumes with 2-D images: 1) a parametric en-face projection\noptimized through deep learning and 2) a cross-sectional slice selection\nprocess controlled through gradient-based attribution. The full summarization\nand DR classification pipeline is trained from end to end. The automatic 2-D\nsummary can be displayed in a viewer or printed in a report to support the\ndecision. We show that the proposed 2-D summarization and classification\npipeline outperforms direct 3-D classification with the advantage of improved\ninterpretability.", "title": "DISCOVER: 2-D Multiview Summarization of Optical Coherence Tomography Angiography for Automatic Diabetic Retinopathy Diagnosis", "author": "Mostafa El Habib Daho, Yihao Li, Rachid Zeghlache, Hugo Le Boité, Pierre Deman, Laurent Borderie, Hugang Ren, Niranchana Mannivanan, Capucine Lepicard, Béatrice Cochener, Aude Couturier, Ramin Tadayoni, Pierre-Henri Conze, Mathieu Lamard, Gwenolé Quellec", "published": "2024-01-10", "arxiv_url": "http://arxiv.org/abs/2401.05137v1", "arxiv_id": "2401.05137v1"}
{"abstraction": "Is it possible to detect a feature in an image without ever looking at it?\nImages are known to have sparser representation in Wavelets and other similar\ntransforms. Compressed Sensing is a technique which proposes simultaneous\nacquisition and compression of any signal by taking very few random linear\nmeasurements (M). The quality of reconstruction directly relates with M, which\nshould be above a certain threshold for a reliable recovery. Since these\nmeasurements can non-adaptively reconstruct the signal to a faithful extent\nusing purely analytical methods like Basis Pursuit, Matching Pursuit, Iterative\nthresholding, etc., we can be assured that these compressed samples contain\nenough information about any relevant macro-level feature contained in the\n(image) signal. Thus if we choose to deliberately acquire an even lower number\nof measurements - in order to thwart the possibility of a comprehensible\nreconstruction, but high enough to infer whether a relevant feature exists in\nan image - we can achieve accurate image classification while preserving its\nprivacy. Through the print error detection problem, it is demonstrated that\nsuch a novel system can be implemented in practise.", "title": "Minimizing Acquisition Maximizing Inference -- A demonstration on print error detection", "author": "Suyash Shandilya", "published": "2020-06-06", "arxiv_url": "http://arxiv.org/abs/2006.03839v1", "arxiv_id": "2006.03839v1"}
{"abstraction": "A shape filter is presented to repair segmentation results obtained in\ncalcium imaging of neurons in vivo. This post-segmentation algorithm can\nautomatically smooth the shapes obtained from a preliminary segmentation, while\nprecluding the cases where two neurons are counted as one combined component.\nThe shape filter is realized using a square-root velocity to project the shapes\non a shape manifold in which distances between shapes are based on elastic\nchanges. Two data-driven weighting methods are proposed to achieve a trade-off\nbetween shape smoothness and consistency with the data. Intuitive comparisons\nof proposed methods via projection onto Cartesian maps demonstrate the\nsmoothing ability of the shape filter. Quantitative measures also prove the\nsuperiority of our methods over models that do not employ any weighting\ncriterion.", "title": "Nonlinear Shape Regression For Filtering Segmentation Results From Calcium Imaging", "author": "Jie Wang, Zhongxiao Fu, Nasrin Sadeghzadehyazdi, Jonathan Kipnis, Scott T. Acton", "published": "2018-02-14", "arxiv_url": "http://arxiv.org/abs/1802.05318v2", "arxiv_id": "1802.05318v2"}
{"abstraction": "We develop and evaluate a neural network-based method for Gibbs artifact and\nnoise removal. A convolutional neural network (CNN) was designed for artifact\nremoval in diffusion-weighted imaging data. Two implementations were\nconsidered: one for magnitude images and one for complex images. Both models\nwere based on the same encoder-decoder structure and were trained by simulating\nMRI acquisitions on synthetic non-MRI images. Both machine learning methods\nwere able to mitigate artifacts in diffusion-weighted images and diffusion\nparameter maps. The CNN for complex images was also able to reduce artifacts in\npartial Fourier acquisitions. The proposed CNNs extend the ability of artifact\ncorrection in diffusion MRI. The machine learning method described here can be\napplied on each imaging slice independently, allowing it to be used flexibly in\nclinical applications.", "title": "Training a Neural Network for Gibbs and Noise Removal in Diffusion MRI", "author": "Matthew J. Muckley, Benjamin Ades-Aron, Antonios Papaioannou, Gregory Lemberskiy, Eddy Solomon, Yvonne W. Lui, Daniel K. Sodickson, Els Fieremans, Dmitry S. Novikov, Florian Knoll", "published": "2019-05-10", "arxiv_url": "http://arxiv.org/abs/1905.04176v2", "arxiv_id": "1905.04176v2"}
{"abstraction": "This paper presents detailed results of neutron imaging of argon bubble flows\nin a rectangular liquid gallium vessel with and without the application of\nexternal horizontal magnetic field. The developed image processing algorithm is\npresented and its capability to extract physical information from images of low\nsignal-to-noise ratio is demonstrated. Bubble parameters, velocity components,\ntrajectories and relevant statistics were computed and analysed. A simpler\nversion of the code was applied to the output of computational fluid dynamics\nsimulations that reproduced the experiment. This work serves to further\nvalidate the neutron radiography as a suitable method for monitoring gas bubble\nflow in liquid metals, as well as to outline procedures that might help others\nto extract data from neutron radiography images with a low signal-to-noise\nratio resulting from high frame rate acquisitions required to resolve rapid\nbubble motion.", "title": "Dynamic neutron imaging of argon bubble flow in liquid gallium in external magnetic field", "author": "Mihails Birjukovs, Valters Dzelme, Andris Jakovics, Knud Thomsen, Pavel Trtik", "published": "2020-02-04", "arxiv_url": "http://arxiv.org/abs/2002.10970v2", "arxiv_id": "2002.10970v2"}
{"abstraction": "In the film industry, the same movie is expected to be watched on displays of\nvastly different sizes, from cinema screens to mobile phones. But visual\ninduction, the perceptual phenomenon by which the appearance of a scene region\nis affected by its surroundings, will be different for the same image shown on\ntwo displays of different dimensions. This presents a practical challenge for\nthe preservation of the artistic intentions of filmmakers, as it can lead to\nshifts in image appearance between viewing destinations. In this work we show\nthat a neural field model based on the efficient representation principle is\nable to predict induction effects, and how by regularizing its associated\nenergy functional the model is still able to represent induction but is now\ninvertible. From this we propose a method to pre-process an image in a\nscreen-size dependent way so that its perception, in terms of visual induction,\nmay remain constant across displays of different size. The potential of the\nmethod is demonstrated through psychophysical experiments on synthetic images\nand qualitative examples on natural images.", "title": "Matching visual induction effects on screens of different size", "author": "Trevor D. Canham, Javier Vazquez-Corral, Elise Mathieu, Marcelo Bertalmío", "published": "2020-05-06", "arxiv_url": "http://arxiv.org/abs/2005.02694v2", "arxiv_id": "2005.02694v2"}
{"abstraction": "Atmospheric optical turbulence can be a significant source of image\ndegradation, particularly in long range imaging applications. Many turbulence\nmitigation algorithms rely on an optical transfer function (OTF) model that\nincludes the Fried parameter. We present anisoplanatic tilt statistics for\nspherical wave propagation. We transform these into 2D autocorrelation\nfunctions that can inform turbulence modeling and mitigation algorithms. Using\nthese, we construct an OTF model that accounts for image registration. We also\npropose a spectral-ratio Fried parameter estimation algorithm that is robust to\ncamera motion and requires no specialized scene content or sources. We employ\nthe Fried parameter estimation and OTF model for turbulence mitigation. A\nnumerical wave-propagation turbulence simulator is used to generate data to\nquantitatively validate the proposed methods. Results with real camera data are\nalso presented.", "title": "Application of Tilt Correlation Statistics to Anisoplanatic Optical Turbulence Modeling and Mitigation", "author": "Russell C. Hardie, Michael A. Rucci, Santasri Bose-Pillai, Richard Van Hook", "published": "2021-08-01", "arxiv_url": "http://arxiv.org/abs/2108.00528v1", "arxiv_id": "2108.00528v1"}
{"abstraction": "Open spina bifida (SB) is one of the most common congenital defects and can\nlead to impaired brain development. Emerging fetal surgery methods have shown\nconsiderable success in the treatment of patients with this severe anomaly.\nAfterwards, alterations in the brain development of these fetuses have been\nobserved. Currently no longitudinal studies exist to show the effect of fetal\nsurgery on brain development. In this work, we present a fetal MRI\nneuro-imaging analysis pipeline for fetuses with SB, including automated fetal\nventricle segmentation and deformation-based morphometry, and demonstrate its\napplicability with an analysis of ventricle enlargement in fetuses with SB.\nUsing a robust super-resolution algorithm, we reconstructed fetal brains at\nboth pre-operative and post-operative time points and trained a U-Net CNN in\norder to automatically segment the ventricles. We investigated the change of\nventricle shape post-operatively, and the impacts of lesion size, type, and GA\nat operation on the change in ventricle shape. No impact was found, except for\nmoderately larger ventriculomegaly progression in myeloschisis patients.\nPrenatal ventricle volume growth was also investigated. Our method allows for\nthe quantification of longitudinal morphological changes to fully quantify the\nimpact of prenatal SB repair and could be applied to predict postnatal\noutcomes.", "title": "Longitudinal analysis of fetal MRI in patients with prenatal spina bifida repair", "author": "Kelly Payette, Ueli Moehrlen, Luca Mazzone, Nicole Ochsenbein-Koelble, Ruth Tuura, Raimund Kottke, Martin Meuli, Andras Jakab", "published": "2019-11-15", "arxiv_url": "http://arxiv.org/abs/1911.06542v1", "arxiv_id": "1911.06542v1"}
{"abstraction": "In this paper, a novel method to enhance Frequency Modulated Continuous Wave\n(FMCW) THz imaging resolution beyond its diffraction limit is proposed. Our\nmethod comprises two stages. Firstly, we reconstruct the signal in\ndepth-direction using a sinc-envelope, yielding a significant improvement in\ndepth estimation and signal parameter extraction. The resulting high precision\ndepth estimate is used to deduce an accurate reflection intensity THz image.\nThis image is fed in the second stage of our method to a 2D blind deconvolution\nprocedure, adopted to enhance the lateral THz image resolution beyond the\ndiffraction limit. Experimental data acquired with a FMCW system operating at\n577 GHz with a bandwidth of 126 GHz shows that the proposed method enhances the\nlateral resolution by a factor of 2.29 to 346.2um with respect to the\ndiffraction limit. The depth accuracy is 91um. Interestingly, the lateral\nresolution enhancement achieved with this blind deconvolution concept leads to\nbetter results in comparison to conventional gaussian deconvolution.\nExperimental data on a PCB resolution target is presented, in order to quantify\nthe resolution enhancement and to compare the performance with established\nimage enhancement approaches. The presented technique allows exposure of the\ninterwoven fibre reinforced embedded structures of the PCB test sample.", "title": "Computational Image Enhancement for Frequency Modulated Continuous Wave (FMCW) THz Image", "author": "Tak Ming Wong, Matthias Kahl, Peter Haring Bolívar, Andreas Kolb", "published": "2018-02-15", "arxiv_url": "http://arxiv.org/abs/1802.05457v3", "arxiv_id": "1802.05457v3"}
{"abstraction": "Grayscale images are fundamental to many image processing applications like\ndata compression, feature extraction, printing and tone mapping. However, some\nimage information is lost when converting from color to grayscale. In this\npaper, we propose a light-weight and high-speed image decolorization method\nbased on human perception of color temperatures. Chromatic aberration results\nfrom differential refraction of light depending on its wavelength. It causes\nsome rays corresponding to cooler colors (like blue, green) to converge before\nthe warmer colors (like red, orange). This phenomena creates a perception of\nwarm colors \"advancing\" toward the eye, while the cool colors to be \"receding\"\naway. In this proposed color to gray conversion model, we implement a weighted\nblending function to combine red (perceived warm) and blue (perceived cool)\nchannel. Our main contribution is threefold: First, we implement a high-speed\ncolor processing method using exact pixel by pixel processing, and we report a\n$5.7\\times$ speed up when compared to other new algorithms. Second, our optimal\ncolor conversion method produces luminance in images that are comparable to\nother state of the art methods which we quantified using the objective metrics\n(E-score and C2G-SSIM) and a subjective user study. Third, we demonstrate that\nan effective luminance distribution can be achieved using our algorithm by\nusing global and local tone mapping applications.", "title": "A color temperature-based high-speed decolorization: an empirical approach for tone mapping applications", "author": "Prasoon Ambalathankandy, Yafei Ou, Masayuki Ikebe", "published": "2021-08-31", "arxiv_url": "http://arxiv.org/abs/2108.13656v1", "arxiv_id": "2108.13656v1"}
{"abstraction": "Current commercial matrix transducers for 3D DCE-US do not display\nside-by-side B-mode and contrast-mode images when capturing volumetric data,\nthus leaving the operator with no position feedback during lengthy\nacquisitions. The purpose of this study was to investigate the use of\ntransducer tracking to provide positioning feedback and to re-align images to\nimprove quantification. An interventional tracking system was developed in\nhouse using an infrared camera and a 3D-printed tracking target attached to a\nX6-1 matrix transducer. The system displays a virtual probe on a separate\nscreen and allows to capture a reference position to provide operator feedback\nwhen no B-mode image is available. To test this set-up, five experienced\noperators were asked to locate an image landmark within a volunteer liver in\nB-mode images using the X6-1 connected to an EPIQ7 system. Operators were then\nasked to maintain the transducer position for 4 minutes under three feedback\nmethods: i) B-mode, ii) display of real-time virtual transducer, iii) blind.\nThe magnitude of displacement over the cine was computed as an estimate of the\nimaging position error. We also investigated whether transducer coordinates can\nbe used to re-align images due to motion and improve contrast ultrasound\nperfusion repeatability. A total of 8 patient data were obtained under an IRB\nfor this. Results suggest that tracking can assist operators maintain a steady\nposition during a lengthy acquisition. With blinded acquisition, an average\ndisplacement of 4.58 mm (S.D. 2.65 mm) was noted. In contrast, the average\ndisplacement for tracking-feedback was comparable to B-mode at 3.48 mm (S.D.\n0.8 mm). We also observed that perfusion parameters had better repeatability\nafter re-alignment.", "title": "Clinical Evaluation of Real-Time Optical-Tracking Navigation and Live Time-Intensity Curves to Provide Feedback During Blinded 4D Contrast-Enhanced Ultrasound Imaging", "author": "Ahmed El Kaffas, Renhui Gong, Samvardhini Sridharan, Rosa Maria Silveira Sigrist, Isabelle Durot, Jürgen K. Willmann, Dimitre Hristov", "published": "2020-11-02", "arxiv_url": "http://arxiv.org/abs/2011.00744v1", "arxiv_id": "2011.00744v1"}
{"abstraction": "Oil reheating has a significant impact on global health due to its extensive\nconsumption, especially in South Asia, and severe health risks. Nevertheless,\nfood image analysis using multispectral imaging systems(MISs) has not been\napplied to oil reheating analysis despite their vast application in rapid food\nquality screening. To that end, the paper discusses the application of a\nlow-cost MSI to estimate the 'reheat cycle count classes' (number of times an\noil sample is recursively heated) and identify 'critical classes' at which\nsubstantial changes in the oil sample have materialized. Firstly, the reheat\ncycle count class is estimated with Bhattacharyya distance between the reheated\nand a pure oil sample as the input. The classification was performed using a\nsupport vector machine classifier that resulted in an accuracy of 83.34 % for\nreheat cycle count identification. Subsequently, an unsupervised clustering\nprocedure was introduced using a modified spectral clustering (SC) algorithm to\ndistinguish critical classes under reheating. In addition, laboratory\nexperiments were performed to ascertain the ramifications of the reheating\nprocess with a chemical analysis. The chemical analysis of the coconut oil\nsamples used in the experiment coincided with the chemical analysis results and\nwas statistically significant (p < 0.05). Accordingly, the proposed work closes\nthe gap for using multispectral imaging for oil reheating and proposes a novel\nalgorithm for unsupervised detection of critical property changes in the oil.\nHence, the proposed research work is significant in its practical implications,\ncontribution to food image analysis, and unsupervised classification\nmechanisms.", "title": "Transmittance Multispectral Imaging for Reheated Coconut Oil Differentiation", "author": "D. Y. L. Ranasinghe, H. M. H. K. Weerasooriya, S. Herath, M. P. B. Ekanayake, H. M. V. R. Herath, G. M. R. I. Godaliyadda, T. Madhujith", "published": "2021-08-28", "arxiv_url": "http://arxiv.org/abs/2108.12665v2", "arxiv_id": "2108.12665v2"}
{"abstraction": "This study focuses on the application of a specific subfield of artificial\nintelligence referred to as computer vision in the analysis of 2-dimensional\nlung x-ray images for the assisted medical diagnosis of ordinary pneumonia.\n  A convolutional neural network algorithm was implemented in a Python-coded,\nFlask-based web application that can analyze x-ray images for the detection of\nordinary pneumonia. Since convolutional neural network algorithms rely on\nmachine learning for the identification and detection of patterns, a technique\nreferred to as transfer learning was implemented to train the neural network in\nthe identification and detection of patterns within the dataset. Open-source\nlung x-ray images were used as training data to create a knowledge base that\nserved as the core element of the web application and the experimental design\nemployed a 5-Trial Confirmatory Test for the validation of the web application.\n  The results of the 5-Trial Confirmatory Test show the calculation of\nDiagnostic Precision Percentage per Trial, General Diagnostic Precision\nPercentage, and General Diagnostic Error Percentage while the Confusion Matrix\nfurther shows the relationship between the label and the corresponding\ndiagnosis result of the web application on each test images.\n  The developed web application can be used by medical practitioners in\nA.I.-assisted diagnosis of ordinary pneumonia, and by researchers in the fields\nof computer science and bioinformatics.", "title": "Applied Computer Vision on 2-Dimensional Lung X-Ray Images for Assisted Medical Diagnosis of Pneumonia", "author": "Ralph Joseph S. D. Ligueran, Manuel Luis C. Delos Santos, Ronaldo S. Tinio, Emmanuel H. Valencia", "published": "2022-07-27", "arxiv_url": "http://arxiv.org/abs/2207.13295v1", "arxiv_id": "2207.13295v1"}
{"abstraction": "Background: People with severe speech and motor impairment (SSMI) often uses\na technique called eye pointing to communicate with outside world. One of their\nparents, caretakers or teachers hold a printed board in front of them and by\nanalyzing their eye gaze manually, their intentions are interpreted. This\ntechnique is often error prone and time consuming and depends on a single\ncaretaker.\n  Objective: We aimed to automate the eye tracking process electronically by\nusing commercially available tablet, computer or laptop and without requiring\nany dedicated hardware for eye gaze tracking. The eye gaze tracker is used to\ndevelop a video see through based AR (augmented reality) display that controls\na robotic device with eye gaze and deployed for a fabric printing task.\n  Methodology: We undertook a user centred design process and separately\nevaluated the web cam based gaze tracker and the video see through based human\nrobot interaction involving users with SSMI. We also reported a user study on\nmanipulating a robotic arm with webcam based eye gaze tracker.\n  Results: Using our bespoke eye gaze controlled interface, able bodied users\ncan select one of nine regions of screen at a median of less than 2 secs and\nusers with SSMI can do so at a median of 4 secs. Using the eye gaze controlled\nhuman-robot AR display, users with SSMI could undertake representative pick and\ndrop task at an average duration less than 15 secs and reach a randomly\ndesignated target within 60 secs using a COTS eye tracker and at an average\ntime of 2 mins using the webcam based eye gaze tracker.", "title": "Eye Gaze Controlled Robotic Arm for Persons with SSMI", "author": "Vinay Krishna Sharma, L. R. D. Murthy, KamalPreet Singh Saluja, Vimal Mollyn, Gourav Sharma, Pradipta Biswas", "published": "2020-05-25", "arxiv_url": "http://arxiv.org/abs/2005.11994v1", "arxiv_id": "2005.11994v1"}
{"abstraction": "Diffuse gliomas are highly infiltrative tumors whose early diagnosis and\nfollow-up usually rely on magnetic resonance imaging (MRI). However, the\nlimited sensitivity of this technique makes it impossible to directly assess\nthe extent of the glioma cell invasion, leading to sub-optimal treatment\nplaning. Reaction-diffusion growth models have been proposed for decades to\nextrapolate glioma cell infiltration beyond margins visible on MRI and predict\nits spatial-temporal evolution. These models nevertheless require an initial\ncondition, that is the tumor cell density values at every location of the brain\nat diagnosis time. Several works have proposed to relate the tumor cell density\nfunction to abnormality outlines visible on MRI but the underlying assumptions\nhave never been verified so far. In this work we propose to verify these\nassumptions by stereotactic histological analysis of a non-operated brain with\nglioblastoma using a tailored 3D-printed slicer. Cell density maps are computed\nfrom histological slides using a deep learning approach. The density maps are\nthen registered to a postmortem MR image and related to an MR-derived geodesic\ndistance map to the tumor core. The relation between the edema outlines visible\non T2 FLAIR MRI and the distance to the core is also investigated. Our results\nsuggest that (i) the previously suggested exponential decrease of the tumor\ncell density with the distance to the tumor core is not unreasonable but (ii)\nthe edema outlines may in general not correspond to a cell density iso-contour\nand (iii) the commonly adopted tumor cell density value at these outlines is\nlikely overestimated. These findings highlight the limitations of using\nconventional MRI to derive glioma cell density maps and point out the need of\nvalidating other methods to initialize reaction-diffusion growth models and\nmake them usable in clinical practice.", "title": "Initial condition assessment for reaction-diffusion glioma growth models: A translational MRI/histology (in)validation study", "author": "Corentin Martens, Laetitia Lebrun, Christine Decaestecker, Thomas Vandamme, Yves-Rémi Van Eycke, Antonin Rovai, Thierry Metens, Olivier Debeir, Serge Goldman, Isabelle Salmon, Gaetan Van Simaeys", "published": "2021-02-02", "arxiv_url": "http://arxiv.org/abs/2102.01719v1", "arxiv_id": "2102.01719v1"}
{"abstraction": "The 3D printing process flow requires several inputs for the best printing\nquality. These settings may vary from sample to sample, printer to printer, and\ndepend upon users' previous experience. The involved operational parameters for\n3D Printing are varied to test the optimality. Thirty-eight samples are printed\nusing four commercially available 3D printers, namely: (a) Ultimaker 2\nExtended+, (b) Delta Wasp, (c) Raise E2, and (d) ProJet MJP. The sample\nprofiles contain uniform and non-uniform distribution of the assorted size of\ncubes and spheres with a known amount of porosity. These samples are scanned\nusing X-Ray Computed Tomography system. Functional Imaging analysis is\nperformed using AI-based segmentation codes to (a) characterize these 3D\nprinters and (b) find Three-dimensional surface roughness of three teeth and\none sandstone pebble (from riverbed) with naturally deposited layers is also\ncompared with printed sample values. Teeth has best quality. It is found that\nProJet MJP gives the best quality of printed samples with the least amount of\nsurface roughness and almost near to the actual porosity value. As expected,\n100% infill density value, best spatial resolution for printing or Layer\nheight, and minimum nozzle speed give the best quality of 3D printing.", "title": "Characterization of 3D Printers and X-Ray Computerized Tomography", "author": "Sunita Khod, Akshay Dvivedi, Mayank Goswami", "published": "2022-05-27", "arxiv_url": "http://arxiv.org/abs/2206.00041v1", "arxiv_id": "2206.00041v1"}
{"abstraction": "Given the substantial growth in the use of additive manufacturing in\nconstruction (AMC), it is necessary to ensure the quality of printed specimens\nwhich can be much more complex than conventionally manufactured parts. This\nstudy explores the various aspects of geometry and surface quality control for\n3D concrete printing (3DCP), with a particular emphasis on deposition-based\nmethods, namely extrusion and shotcrete 3D printing (SC3DP). A comprehensive\noverview of existing quality control (QC) methods and strategies is provided\nand preceded by an in-depth discussion. Four categories of data capture\ntechnologies are investigated and their advantages and limitations in the\ncontext of AMC are discussed. Additionally, the effects of environmental\nconditions and objects' properties on data capture are also analyzed. The study\nextends to automated data capture planning methods for different sensors.\nFurthermore, various quality control strategies are explored across different\nstages of the fabrication cycle of the printed object including: (i) During\nprinting, (ii) Layer-wise, (iii) Preassembly, and (iv) Assembly. In addition to\nreviewing the methods already applied in AMC, we also address various research\ngaps and future trends and highlight potential methodologies from adjacent\ndomains that could be transferred to AMC.", "title": "A Review on Geometry and Surface Inspection in 3D Concrete Printing", "author": "K. Mawas, M. Maboudi, M. Gerke", "published": "2025-03-10", "arxiv_url": "http://arxiv.org/abs/2503.07472v1", "arxiv_id": "2503.07472v1"}
{"abstraction": "Medical imaging phantoms are widely used for validation and verification of\nimaging systems and algorithms in surgical guidance and radiation oncology\nprocedures. Especially, for the performance evaluation of new algorithms in the\nfield of medical imaging, manufactured phantoms need to replicate specific\nproperties of the human body, e.g., tissue morphology and radiological\nproperties. Additive manufacturing (AM) technology provides an inexpensive\nopportunity for accurate anatomical replication with customization\ncapabilities. In this study, we proposed a simple and cheap protocol to\nmanufacture realistic tumor phantoms based on the filament 3D printing\ntechnology. Tumor phantoms with both homogenous and heterogenous radiodensity\nwere fabricated. The radiodensity similarity between the printed tumor models\nand real tumor data from CT images of lung cancer patients was evaluated.\nAdditionally, it was investigated whether a heterogeneity in the 3D printed\ntumor phantoms as observed in the tumor patient data had an influence on the\nvalidation of image registration algorithms. A density range between -217 to\n226 HUs was achieved for 3D printed phantoms; this range of radiation\nattenuation is also observed in the human lung tumor tissue. The resulted HU\nrange could serve as a lookup-table for researchers and phantom manufactures to\ncreate realistic CT tumor phantoms with the desired range of radiodensities.\nThe 3D printed tumor phantoms also precisely replicated real lung tumor patient\ndata regarding morphology and could also include life-like heterogeneity of the\nradiodensity inside the tumor models. An influence of the heterogeneity on\naccuracy and robustness of the image registration algorithms was not found.", "title": "Realistic 3D printed imaging tumor phantoms for validation of image processing algorithms", "author": "Sepideh Hatamikia, Ingo Gulyas, Wolfgang Birkfellner, Gernot Kronreif, Alexander Unger, Gunpreet Oberoi, Andrea Lorenz, Ewald Unger, Joachim Kettenbach, Michael Figl, Janina Patsch, Andreas Strassl, Dietmar Georg, Andreas Renner", "published": "2022-11-27", "arxiv_url": "http://arxiv.org/abs/2211.14861v1", "arxiv_id": "2211.14861v1"}
{"abstraction": "We present a novel framework to advance generative artificial intelligence\n(AI) applications in the realm of printed art products, specifically addressing\nlarge-format products that require high-resolution artworks. The framework\nconsists of a pipeline that addresses two major challenges in the domain: the\nhigh complexity of generating effective prompts, and the low native resolution\nof images produced by diffusion models. By integrating AI-enhanced prompt\ngenerations with AI-powered upscaling techniques, our framework can efficiently\nproduce high-quality, diverse artistic images suitable for many new commercial\nuse cases. Our work represents a significant step towards democratizing\nhigh-quality AI art, opening new avenues for consumers, artists, designers, and\nbusinesses.", "title": "Generating Print-Ready Personalized AI Art Products from Minimal User Inputs", "author": "Noah Pursell, Anindya Maiti", "published": "2024-03-28", "arxiv_url": "http://arxiv.org/abs/2405.18247v1", "arxiv_id": "2405.18247v1"}
{"abstraction": "3D printing has been widely adopted for clinical decision making and\ninterventional planning of Congenital heart disease (CHD), while whole heart\nand great vessel segmentation is the most significant but time-consuming step\nin the model generation for 3D printing. While various automatic whole heart\nand great vessel segmentation frameworks have been developed in the literature,\nthey are ineffective when applied to medical images in CHD, which have\nsignificant variations in heart structure and great vessel connections. To\naddress the challenge, we leverage the power of deep learning in processing\nregular structures and that of graph algorithms in dealing with large\nvariations and propose a framework that combines both for whole heart and great\nvessel segmentation in CHD. Particularly, we first use deep learning to segment\nthe four chambers and myocardium followed by the blood pool, where variations\nare usually small. We then extract the connection information and apply graph\nmatching to determine the categories of all the vessels. Experimental results\nusing 683D CT images covering 14 types of CHD show that our method can increase\nDice score by 11.9% on average compared with the state-of-the-art whole heart\nand great vessel segmentation method in normal anatomy. The segmentation\nresults are also printed out using 3D printers for validation.", "title": "Accurate Congenital Heart Disease Model Generation for 3D Printing", "author": "Xiaowei Xu, Tianchen Wang, Dewen Zeng, Yiyu Shi, Qianjun Jia, Haiyun Yuan, Meiping Huang, Jian Zhuang", "published": "2019-07-06", "arxiv_url": "http://arxiv.org/abs/1907.05273v2", "arxiv_id": "1907.05273v2"}
{"abstraction": "Vast volumes of printed documents continue to be used for various important\nas well as trivial applications. Such applications often rely on the\ninformation provided in the form of printed text documents whose integrity\nverification poses a challenge due to time constraints and lack of resources.\nSource printer identification provides essential information about the origin\nand integrity of a printed document in a fast and cost-effective manner. Even\nwhen fraudulent documents are identified, information about their origin can\nhelp stop future frauds. If a smartphone camera replaces scanner for the\ndocument acquisition process, document forensics would be more economical,\nuser-friendly, and even faster in many applications where remote and\ndistributed analysis is beneficial. Building on existing methods, we propose to\nlearn a single CNN model from the fusion of letter images and their\nprinter-specific noise residuals. In the absence of any publicly available\ndataset, we created a new dataset consisting of 2250 document images of text\ndocuments printed by eighteen printers and acquired by a smartphone camera at\nfive acquisition settings. The proposed method achieves 98.42% document\nclassification accuracy using images of letter 'e' under a 5x2 cross-validation\napproach. Further, when tested using about half a million letters of all types,\nit achieves 90.33% and 98.01% letter and document classification accuracies,\nrespectively, thus highlighting the ability to learn a discriminative model\nwithout dependence on a single letter type. Also, classification accuracies are\nencouraging under various acquisition settings, including low illumination and\nchange in angle between the document and camera planes.", "title": "Source Printer Identification from Document Images Acquired using Smartphone", "author": "Sharad Joshi, Suraj Saxena, Nitin Khanna", "published": "2020-03-27", "arxiv_url": "http://arxiv.org/abs/2003.12602v1", "arxiv_id": "2003.12602v1"}
{"abstraction": "Copy detection patterns (CDP) present an efficient technique for product\nprotection against counterfeiting. However, the complexity of studying CDP\nproduction variability often results in time-consuming and costly procedures,\nlimiting CDP scalability. Recent advancements in computer modelling, notably\nthe concept of a \"digital twin\" for printing-imaging channels, allow for\nenhanced scalability and the optimization of authentication systems. Yet, the\ndevelopment of an accurate digital twin is far from trivial.\n  This paper extends previous research which modelled a printing-imaging\nchannel using a machine learning-based digital twin for CDP. This model, built\nupon an information-theoretic framework known as \"Turbo\", demonstrated superior\nperformance over traditional generative models such as CycleGAN and pix2pix.\nHowever, the emerging field of Denoising Diffusion Probabilistic Models (DDPM)\npresents a potential advancement in generative models due to its ability to\nstochastically model the inherent randomness of the printing-imaging process,\nand its impressive performance in image-to-image translation tasks.\n  This study aims at comparing the capabilities of the Turbo framework and DDPM\non the same CDP datasets, with the goal of establishing the real-world benefits\nof DDPM models for digital twin applications in CDP security. Furthermore, the\npaper seeks to evaluate the generative potential of the studied models in the\ncontext of mobile phone data acquisition. Despite the increased complexity of\nDDPM methods when compared to traditional approaches, our study highlights\ntheir advantages and explores their potential for future applications.", "title": "Stochastic Digital Twin for Copy Detection Patterns", "author": "Yury Belousov, Olga Taran, Vitaliy Kinakh, Slava Voloshynovskiy", "published": "2023-09-28", "arxiv_url": "http://arxiv.org/abs/2309.16866v1", "arxiv_id": "2309.16866v1"}
{"abstraction": "In this paper, we present the TacShade a newly designed 3D-printed soft\noptical tactile sensor. The sensor is developed for shape reconstruction under\nthe inspiration of sketch drawing that uses the density of sketch lines to draw\nlight and shadow, resulting in the creation of a 3D-view effect. TacShade,\nbuilding upon the strengths of the TacTip, a single-camera tactile sensor of\nlarge in-depth deformation and being sensitive to edge and surface following,\nimproves the structure in that the markers are distributed within the gap of\npapillae pins. Variations in light, dark, and grey effects can be generated\ninside the sensor through external contact interactions. The contours of the\ncontacting objects are outlined by white markers, while the contact depth\ncharacteristics can be indirectly obtained from the distribution of black pins\nand white markers, creating a 2.5D visualization. Based on the imaging effect,\nwe improve the Shape from Shading (SFS) algorithm to process tactile images,\nenabling a coarse but fast reconstruction for the contact objects. Two\nexperiments are performed. The first verifies TacShade s ability to reconstruct\nthe shape of the contact objects through one image for object distinction. The\nsecond experiment shows the shape reconstruction capability of TacShade for a\nlarge panel with ridged patterns based on the location of robots and image\nsplicing technology.", "title": "TacShade A New 3D-printed Soft Optical Tactile Sensor Based on Light, Shadow and Greyscale for Shape Reconstruction", "author": "Zhenyu Lu, Jialong Yang, Haoran Li, Yifan Li, Weiyong Si, Nathan Lepora, Chenguang Yang", "published": "2024-06-01", "arxiv_url": "http://arxiv.org/abs/2406.00485v1", "arxiv_id": "2406.00485v1"}
{"abstraction": "Additive manufacturing techniques are revolutionizing product development by\nenabling fast turnaround from design to fabrication. However, the throughput of\nthe rapid prototyping pipeline remains constrained by print optimization,\nrequiring multiple iterations of fabrication and ex-situ metrology. Despite the\nneed for a suitable technology, robust in-situ shape measurement of an entire\nprint is not currently available with any additive manufacturing modality.\nHere, we address this shortcoming by demonstrating fully simultaneous 3D\nmetrology and printing. We exploit the dramatic increase in light scattering by\na photoresin during gelation for real-time 3D imaging of prints during\ntomographic volumetric additive manufacturing. Tomographic imaging of the light\nscattering density in the build volume yields quantitative, artifact-free 3D +\ntime models of cured objects that are accurate to below 1% of the size of the\nprint. By integrating shape measurement into the printing process, our work\npaves the way for next-generation rapid prototyping with real-time defect\ndetection and correction.", "title": "On-the-fly 3D metrology of volumetric additive manufacturing", "author": "Antony Orth, Kathleen L. Sampson, Yujie Zhang, Kayley Ting, Derek Aranguren van Egmond, Kurtis Laqua, Thomas Lacelle, Daniel Webber, Dorothy Fathi, Jonathan Boisvert, Chantal Paquet", "published": "2022-02-07", "arxiv_url": "http://arxiv.org/abs/2202.04644v1", "arxiv_id": "2202.04644v1"}
{"abstraction": "Purpose. Ability to locate and track ultrasound images in the 3D operating\nspace is of great benefit for multiple clinical applications. This is often\naccomplished by tracking the probe using a precise but expensive optical or\nelectromagnetic tracking system. Our goal is to develop a simple and low cost\naugmented reality echography framework using a standard RGB-D Camera.\n  Methods. A prototype system consisting of an Occipital Structure Core RGB-D\ncamera, a specifically-designed 3D marker, and a fast point cloud registration\nalgorithm FaVoR was developed and evaluated on an Ultrasonix ultrasound system.\nThe probe was calibrated on a 3D-printed N-wire phantom using the software PLUS\ntoolkit. The proposed calibration method is simplified, requiring no additional\nmarkers or sensors attached to the phantom. Also, a visualization software\nbased on OpenGL was developed for the augmented reality application.\n  Results. The calibrated probe was used to augment a real-world video in a\nsimulated needle insertion scenario. The ultrasound images were rendered on the\nvideo, and visually-coherent results were observed. We evaluated the end-to-end\naccuracy of our AR US framework on localizing a cube of 5 cm size. From our two\nexperiments, the target pose localization error ranges from 5.6 to 5.9 mm and\nfrom -3.9 to 4.2 degrees.\n  Conclusion. We believe that with the potential democratization of RGB-D\ncameras integrated in mobile devices and AR glasses in the future, our\nprototype solution may facilitate the use of 3D freehand ultrasound in clinical\nroutine. Future work should include a more rigorous and thorough evaluation, by\ncomparing the calibration accuracy with those obtained by commercial tracking\nsolutions in both simulated and real medical scenarios.", "title": "A Novel Augmented Reality Ultrasound Framework Using an RGB-D Camera and a 3D-printed Marker", "author": "Yitian Zhou, Gaétan Lelu, Boris Labbé, Guillaume Pasquier, Pierre Le Gargasson, Albert Murienne, Laurent Launay", "published": "2022-05-09", "arxiv_url": "http://arxiv.org/abs/2205.04350v1", "arxiv_id": "2205.04350v1"}
{"abstraction": "Recent advances in media generation techniques have made it easier for\nattackers to create forged images and videos. State-of-the-art methods enable\nthe real-time creation of a forged version of a single video obtained from a\nsocial network. Although numerous methods have been developed for detecting\nforged images and videos, they are generally targeted at certain domains and\nquickly become obsolete as new kinds of attacks appear. The method introduced\nin this paper uses a capsule network to detect various kinds of spoofs, from\nreplay attacks using printed images or recorded videos to computer-generated\nvideos using deep convolutional neural networks. It extends the application of\ncapsule networks beyond their original intention to the solving of inverse\ngraphics problems.", "title": "Capsule-Forensics: Using Capsule Networks to Detect Forged Images and Videos", "author": "Huy H. Nguyen, Junichi Yamagishi, Isao Echizen", "published": "2018-10-26", "arxiv_url": "http://arxiv.org/abs/1810.11215v1", "arxiv_id": "1810.11215v1"}
{"abstraction": "Osteoporosis induced fractures occur worldwide about every 3 seconds.\nVertebral compression fractures are early signs of the disease and considered\nrisk predictors for secondary osteoporotic fractures. We present a detection\nmethod to opportunistically screen spine-containing CT images for the presence\nof these vertebral fractures. Inspired by radiology practice, existing methods\nare based on 2D and 2.5D features but we present, to the best of our knowledge,\nthe first method for detecting vertebral fractures in CT using automatically\nlearned 3D feature maps. The presented method explicitly localizes these\nfractures allowing radiologists to interpret its results. We train a\nvoxel-classification 3D Convolutional Neural Network (CNN) with a training\ndatabase of 90 cases that has been semi-automatically generated using\nradiologist readings that are readily available in clinical practice. Our 3D\nmethod produces an Area Under the Curve (AUC) of 95% for patient-level fracture\ndetection and an AUC of 93% for vertebra-level fracture detection in a\nfive-fold cross-validation experiment.", "title": "Detection of vertebral fractures in CT using 3D Convolutional Neural Networks", "author": "Joeri Nicolaes, Steven Raeymaeckers, David Robben, Guido Wilms, Dirk Vandermeulen, Cesar Libanati, Marc Debois", "published": "2019-11-05", "arxiv_url": "http://arxiv.org/abs/1911.01816v1", "arxiv_id": "1911.01816v1"}
{"abstraction": "In this work we present a method of automatic segmentation of defective\nskulls for custom cranial implant design and 3D printing purposes. Since such\ntissue models are usually required in patient cases with complex anatomical\ndefects and variety of external objects present in the acquired data, most deep\nlearning-based approaches fall short because it is not possible to create a\nsufficient training dataset that would encompass the spectrum of all possible\nstructures. Because CNN segmentation experiments in this application domain\nhave been so far limited to simple patch-based CNN architectures, we first show\nhow the usage of the encoder-decoder architecture can substantially improve the\nsegmentation accuracy. Then, we show how the number of segmentation artifacts,\nwhich usually require manual corrections, can be further reduced by adding a\nboundary term to CNN training and by globally optimizing the segmentation with\ngraph-cut. Finally, we show that using the proposed method, 3D segmentation\naccurate enough for clinical application can be achieved with 2D CNN\narchitectures as well as their 3D counterparts.", "title": "Segmentation of Defective Skulls from CT Data for Tissue Modelling", "author": "Oldřich Kodym, Michal Španěl, Adam Herout", "published": "2019-11-20", "arxiv_url": "http://arxiv.org/abs/1911.08805v2", "arxiv_id": "1911.08805v2"}
{"abstraction": "Registration of partial-view 3D US volumes with MRI data is influenced by\ninitialization. The standard of practice is using extrinsic or intrinsic\nlandmarks, which can be very tedious to obtain. To overcome the limitations of\nregistration initialization, we present a novel approach that is based on\nEuclidean distance maps derived from easily obtainable coarse segmentations. We\nevaluate our approach quantitatively on the publicly available RESECT dataset\nand show that it is robust regarding overlap of target area and initial\nposition. Furthermore, our method provides initializations that are suitable\nfor state-of-the-art nonlinear, deformable image registration algorithm's\ncapture ranges.", "title": "Initialize globally before acting locally: Enabling Landmark-free 3D US to MRI Registration", "author": "Julia Rackerseder, Maximilian Baust, Rüdiger Göbl, Nassir Navab, Christoph Hennersperger", "published": "2018-06-12", "arxiv_url": "http://arxiv.org/abs/1806.04368v1", "arxiv_id": "1806.04368v1"}
{"abstraction": "Human endeavor has involved making choices about color and looking for ways\nto color objects since the dawn of civilization. While it has been the\nexclusive domain of artists and craftspeople for millennia, the last century\nhas seen the introduction of a scientific basis to color communication. The\nultimate goal of this development is for color communication to happen\nseamlessly and in a transparent way. There are however two categories of\nchallenges here: first, understanding and quantifying color needs and\nexpectation and second, developing control mechanisms that deliver the desired\ncolor. In this paper a review will be presented of the color needs in\nend-to-end color journeys, from initial concept to final colored object and an\noverview of recent developments in color printing will follow. Topics like\nimaging pipelines (including the recently-introduced HP Pixel Control), the\nease of use of color workflows (including HP Smart Color Tools), the handling\nof brand or corporate identity colors (via HP Professional PANTONE Emulation)\nand the measurement of color difference under specific viewing arrangements\n(i.e., the dENS metric for viewing samples without separation) will be\naddressed. Finally, a series of challenges for the future will be set out, so\nthat their solution can be approached by both academic and industrial\ncommunities.", "title": "Color continuity along the journey from ideas to objects", "author": "Jan Morovic", "published": "2019-09-27", "arxiv_url": "http://arxiv.org/abs/1909.12583v1", "arxiv_id": "1909.12583v1"}
{"abstraction": "Face verification is a fast-growing authentication tool for everyday systems,\nsuch as smartphones. While current 2D face recognition methods are very\naccurate, it has been suggested recently that one may wish to add a 3D sensor\nto such solutions to make them more reliable and robust to spoofing, e.g.,\nusing a 2D print of a person's face. Yet, this requires an additional\nrelatively expensive depth sensor. To mitigate this, we propose a novel\nauthentication system, based on slim grayscale coded light field imaging. We\nprovide a reconstruction free fast anti-spoofing mechanism, working directly on\nthe coded image. It is followed by a multi-view, multi-modal face verification\nnetwork that given grayscale data together with a low-res depth map achieves\ncompetitive results to the RGB case. We demonstrate the effectiveness of our\nsolution on a simulated 3D (RGBD) version of LFW, which will be made public,\nand a set of real faces acquired by a light field computational camera.", "title": "Face Authentication from Grayscale Coded Light Field", "author": "Dana Weitzner, David Mendlovic, Raja Giryes", "published": "2020-05-31", "arxiv_url": "http://arxiv.org/abs/2006.00473v1", "arxiv_id": "2006.00473v1"}
{"abstraction": "The recent surge in the adoption of machine learning techniques for materials\ndesign, discovery, and characterization has resulted in an increased interest\nand application of Image Driven Machine Learning (IDML) approaches. In this\nwork, we review the application of IDML to the field of materials\ncharacterization. A hierarchy of six action steps is defined which\ncompartmentalizes a problem statement into well-defined modules. The studies\nreviewed in this work are analyzed through the decisions adopted by them at\neach of these steps. Such a review permits a granular assessment of the field,\nfor example the impact of IDML on materials characterization at the nanoscale,\nthe number of images in a typical dataset required to train a semantic\nsegmentation model on electron microscopy images, the prevalence of transfer\nlearning in the domain, etc. Finally, we discuss the importance of\ninterpretability and explainability, and provide an overview of two emerging\ntechniques in the field: semantic segmentation and generative adversarial\nnetworks.", "title": "The Adoption of Image-Driven Machine Learning for Microstructure Characterization and Materials Design: A Perspective", "author": "Arun Baskaran, Elizabeth J. Kautz, Aritra Chowdhary, Wufei Ma, Bulent Yener, Daniel J. Lewis", "published": "2021-05-20", "arxiv_url": "http://arxiv.org/abs/2105.09729v1", "arxiv_id": "2105.09729v1"}
{"abstraction": "Lensless imaging protects visual privacy by capturing heavily blurred images\nthat are imperceptible for humans to recognize the subject but contain enough\ninformation for machines to infer information. Unfortunately, protecting visual\nprivacy comes with a reduction in recognition accuracy and vice versa. We\npropose a learnable lensless imaging framework that protects visual privacy\nwhile maintaining recognition accuracy. To make captured images imperceptible\nto humans, we designed several loss functions based on total variation,\ninvertibility, and the restricted isometry property. We studied the effect of\nprivacy protection with blurriness on the identification of personal identity\nvia a quantitative method based on a subjective evaluation. Moreover, we\nvalidate our simulation by implementing a hardware realization of lensless\nimaging with photo-lithographically printed masks.", "title": "Human-Imperceptible Identification with Learnable Lensless Imaging", "author": "Thuong Nguyen Canh, Trung Thanh Ngo, Hajime Nagahara", "published": "2023-02-04", "arxiv_url": "http://arxiv.org/abs/2302.02255v1", "arxiv_id": "2302.02255v1"}
{"abstraction": "Advances in 3D printing of biocompatible materials make patient-specific\nimplants increasingly popular. The design of these implants is, however, still\na tedious and largely manual process. Existing approaches to automate implant\ngeneration are mainly based on 3D U-Net architectures on downsampled or\npatch-wise data, which can result in a loss of detail or contextual\ninformation. Following the recent success of Diffusion Probabilistic Models, we\npropose a novel approach for implant generation based on a combination of 3D\npoint cloud diffusion models and voxelization networks. Due to the stochastic\nsampling process in our diffusion model, we can propose an ensemble of\ndifferent implants per defect, from which the physicians can choose the most\nsuitable one. We evaluate our method on the SkullBreak and SkullFix datasets,\ngenerating high-quality implants and achieving competitive evaluation scores.", "title": "Point Cloud Diffusion Models for Automatic Implant Generation", "author": "Paul Friedrich, Julia Wolleb, Florentin Bieder, Florian M. Thieringer, Philippe C. Cattin", "published": "2023-03-14", "arxiv_url": "http://arxiv.org/abs/2303.08061v2", "arxiv_id": "2303.08061v2"}
{"abstraction": "Particle size measurement is crucial in various applications, be it sizing\ndroplets in inkjet printing or respiratory events, tracking particulate\nejection in hypersonic impacts, or detecting floating target markers in free\nsurface flows. Such systems are characterised by extracting quantitative\ninformation like size, position, velocity and number density of the dispersed\nparticles, which is typically non-trivial. The existing methods like phase\nDoppler or digital holography offer precise estimates at the expense of\ncomplicated systems, demanding significant expertise. We present a novel\nvolumetric measurement approach for estimating the size and position of\ndispersed spherical particles that utilises a unique 'Depth from Defocus' (DFD)\ntechnique with a single camera. The calibration free sizing enables in-situ\nexamination of hard to measure systems, including naturally occurring phenomena\nlike pathogenic aerosols, pollen dispersion or raindrops. The efficacy of the\ntechnique is demonstrated for diverse sparse dispersions, including dots, glass\nbeads, spray droplets, and pollen grains. The simple optical configuration and\nsemi-autonomous calibration procedure make the method readily deployable and\naccessible, with a scope of applicability across vast research horizons.", "title": "Depth from Defocus Technique: A Simple Calibration-Free Approach for Dispersion Size Measurement", "author": "Saini Jatin Rao, Shubham Sharma, Saptarshi Basu, Cameron Tropea", "published": "2023-07-20", "arxiv_url": "http://arxiv.org/abs/2307.10678v2", "arxiv_id": "2307.10678v2"}
{"abstraction": "Directional information measurement has many applications in domains such as\nrobotics, virtual and augmented reality, and industrial computer vision.\nConventional methods either require pre-calibration or necessitate controlled\nenvironments. The state-of-the-art MoireTag approach exploits the Moire effect\nand QR-design to continuously track the angular shift precisely. However, it is\nstill not a fully QR code design. To overcome the above challenges, we propose\na novel snapshot method for discrete angular measurement and tracking with\nscannable QR-design patterns that are generated by binary structures printed on\nboth sides of a glass plate. The QR codes, resulting from the parallax effect\ndue to the geometry alignment between two layers, can be readily measured as\nangular information using a phone camera. The simulation results show that the\nproposed non-contact object tracking framework is computationally efficient\nwith high accuracy.", "title": "QR-Tag: Angular Measurement and Tracking with a QR-Design Marker", "author": "Simeng Qiu, Hadi Amata, Wolfgang Heidrich", "published": "2023-10-09", "arxiv_url": "http://arxiv.org/abs/2310.06109v1", "arxiv_id": "2310.06109v1"}
{"abstraction": "Neonatal echocardiography is vital for early detection of heart anomalies in\nnewborns, enabling timely, non-invasive interventions where 4D ultrasound, adds\nthe dimension of time to 3D imaging, enhances diagnostic capabilities by\nvisualizing real-time heart dynamics. However, training for 4D neonatal\nechocardiography is limited by the lack of simulators that support 4D\nUltrasound volume visualization within gamified environments. This paper\nintroduces EchoSim4D, an XR-based simulator leveraging novel pipeline for\nvisualizing 4D volume data in Unity, incorporating real-time volume\nreconstruction, and a preloaded version optimized for low-end systems.\nEchoSim4D integrates a sensor-equipped manikin and a custom 3D-printed\ntransducer with a 6-DOF sensor, replicating the precise probe maneuvers\nnecessary for neonatal echocardiography. In a validation study with\npostgraduate medical students (0-5 years of experience), supervised by a domain\nexpert, EchoSim4D demonstrated high visual fidelity and training efficacy.\nFindings suggest that 4D visualization techniques hold significant potential\nfor advancing medical training in neonatal echocardiography.", "title": "EchoSim4D: A Proof-of-Concept Gamified XR Echocardiography Training Simulator for Neonates using 4D Ultrasound Volume", "author": "Deepthy Rose Jose, Venkataseshan Sundaram, M Manivannan", "published": "2024-12-09", "arxiv_url": "http://arxiv.org/abs/2412.06271v1", "arxiv_id": "2412.06271v1"}
{"abstraction": "Ultrasound images are one of the most widely used techniques in clinical\nsettings to analyze and detect different organs for study or diagnoses of\ndiseases. The dependence on subjective opinions of experts such as radiologists\ncalls for an automatic recognition and detection system that can provide an\nobjective analysis. Previous work done on this topic is limited and can be\nclassified by the organ of interest. Hybrid neural networks, linear and\nlogistic regression models, 3D reconstructed models, and various machine\nlearning techniques have been used to solve complex problems such as detection\nof lesions and cancer. Our project aims to use Dual Path Networks (DPN) to\nsegment and detect shapes in ultrasound images taken from 3D printed models of\nthe liver. Further the DPN deep architectures could be coupled with Fully\nConvolutional Network (FCN) to refine the results. Data denoised with various\nfilters would be used to gauge how they fare against each other and provide the\nbest results. Small amount of dataset works with DPNs, and hence, that should\nbe appropriate for us as our dataset shall be limited in size. Moreover, the\nultrasound scans shall need to be taken from different orientations of the\nscanner with respect to the organ, such that the training dataset can\naccurately perform segmentation and shape detection.", "title": "Shape Detection In 2D Ultrasound Images", "author": "Ruturaj Gole, Haixia Wu, Subho Ghose", "published": "2019-11-22", "arxiv_url": "http://arxiv.org/abs/1911.09863v1", "arxiv_id": "1911.09863v1"}
{"abstraction": "Among the different kinds of fire accidents that can occur during industrial\nactivities that involve hazardous materials, jet fires are one of the\nlesser-known types. This is because they are often involved in a process that\ngenerates a sequence of other accidents of greater magnitude, known as domino\neffect. Flame impingement usually causes domino effects, and jet fires present\nspecific features that can significantly increase the probability of this\nhappening. These features become relevant from a risk analysis perspective,\nmaking their proper characterization a crucial task. Deep Learning approaches\nhave become extensively used for tasks such as jet fire characterization;\nhowever, these methods are heavily dependent on the amount of data and the\nquality of the labels. Data acquisition of jet fires involve expensive\nexperiments, especially so if infrared imagery is used. Therefore, this paper\nproposes the use of Generative Adversarial Networks to produce plausible\ninfrared images from visible ones, making experiments less expensive and\nallowing for other potential applications. The results suggest that it is\npossible to realistically replicate the results for experiments carried out\nusing both visible and infrared cameras. The obtained results are compared with\nsome previous experiments, and it is shown that similar results were obtained.", "title": "Computer Vision-based Characterization of Large-scale Jet Flames using a Synthetic Infrared Image Generation Approach", "author": "Carmina Pérez-Guerrero, Jorge Francisco Ciprián-Sánchez, Adriana Palacios, Gilberto Ochoa-Ruiz, Miguel Gonzalez-Mendoza, Vahid Foroughi, Elsa Pastor, Gerardo Rodriguez-Hernandez", "published": "2022-06-05", "arxiv_url": "http://arxiv.org/abs/2206.02110v1", "arxiv_id": "2206.02110v1"}
{"abstraction": "This paper proposes a novel transformer-based model architecture for medical\nimaging problems involving analysis of vertebrae. It considers two applications\nof such models in MR images: (a) detection of spinal metastases and the related\nconditions of vertebral fractures and metastatic cord compression, (b)\nradiological grading of common degenerative changes in intervertebral discs.\nOur contributions are as follows: (i) We propose a Spinal Context Transformer\n(SCT), a deep-learning architecture suited for the analysis of repeated\nanatomical structures in medical imaging such as vertebral bodies (VBs). Unlike\nprevious related methods, SCT considers all VBs as viewed in all available\nimage modalities together, making predictions for each based on context from\nthe rest of the spinal column and all available imaging modalities. (ii) We\napply the architecture to a novel and important task: detecting spinal\nmetastases and the related conditions of cord compression and vertebral\nfractures/collapse from multi-series spinal MR scans. This is done using\nannotations extracted from free-text radiological reports as opposed to bespoke\nannotation. However, the resulting model shows strong agreement with\nvertebral-level bespoke radiologist annotations on the test set. (iii) We also\napply SCT to an existing problem: radiological grading of inter-vertebral discs\n(IVDs) in lumbar MR scans for common degenerative changes.We show that by\nconsidering the context of vertebral bodies in the image, SCT improves the\naccuracy for several gradings compared to previously published model.", "title": "Context-Aware Transformers For Spinal Cancer Detection and Radiological Grading", "author": "Rhydian Windsor, Amir Jamaludin, Timor Kadir, Andrew Zisserman", "published": "2022-06-27", "arxiv_url": "http://arxiv.org/abs/2206.13173v1", "arxiv_id": "2206.13173v1"}
{"abstraction": "JPEG is one of the popular image compression algorithms that provide\nefficient storage and transmission capabilities in consumer electronics, and\nhence it is the most preferred image format over the internet world. In the\npresent digital and Big-data era, a huge volume of JPEG compressed document\nimages are being archived and communicated through consumer electronics on\ndaily basis. Though it is advantageous to have data in the compressed form on\none side, however, on the other side processing with off-the-shelf methods\nbecomes computationally expensive because it requires decompression and\nrecompression operations. Therefore, it would be novel and efficient, if the\ncompressed data are processed directly in their respective compressed domains\nof consumer electronics. In the present research paper, we propose to\ndemonstrate this idea taking the case study of printed text line segmentation.\nSince, JPEG achieves compression by dividing the image into non overlapping 8x8\nblocks in the pixel domain and using Discrete Cosine Transform (DCT); it is\nvery likely that the partitioned 8x8 DCT blocks overlap the contents of two\nadjacent text-lines without leaving any clue for the line separator, thus\nmaking text-line segmentation a challenging problem. Two approaches of\nsegmentation have been proposed here using the DC projection profile and AC\ncoefficients of each 8x8 DCT block. The first approach is based on the strategy\nof partial decompression of selected DCT blocks, and the second approach is\nwith intelligent analysis of F10 and F11 AC coefficients and without using any\ntype of decompression. The proposed methods have been tested with variable font\nsizes, font style and spacing between lines, and a good performance is\nreported.", "title": "Automatic Text Line Segmentation Directly in JPEG Compressed Document Images", "author": "Bulla Rajesh, Mohammed Javed, P Nagabhushan", "published": "2019-07-29", "arxiv_url": "http://arxiv.org/abs/1907.12219v1", "arxiv_id": "1907.12219v1"}
{"abstraction": "This work demonstrates an application of near field indirect microwave\nholography for the detection of malignant tissues in the human breast in an\neffective way. The holograms are recorded by two directive antennas aligned\nalong each other's boresight while performing a raster scan over a 2D plane\nutilizing XY-linear motorized translation stage and a uniform reference wave.\nThe whole information i.e. amplitude and phase of an object has been provided\nby indirect holography at microwave frequencies. The extracted phase values are\nused to determine the dielectric permittivity values which are further utilized\nfor the identification and validating the positions of malignant tissues in the\nbreast phantom. The experimental evaluations performed on the in-house designed\nand developed tissue mimicking 3D printed breast phantoms. The experimental\nresults demonstrate the ability of microwave holography using directive\nantennas in locating and identifying the tumors up to the minimum size of 4mm\nand a maximum depth of 25mm in fabricated phantom. The preliminary results\npresent the potential of the Near Field Indirect Holographic Imaging (NFIHI) in\norder to develop an efficient and economical tool for breast cancer detection.", "title": "Early Detection of Cancerous Tissues in Human Breast utilizing Near field Microwave Holography", "author": "Vineeta Kumari, Aijaz Ahmed, Tirupathiraju Kanumuri, Chandra Shakher, Gyanendra Sheoran", "published": "2019-04-22", "arxiv_url": "http://arxiv.org/abs/1904.09870v1", "arxiv_id": "1904.09870v1"}
{"abstraction": "Segmentation of medical images is critical for making several processes of\nanalysis and classification more reliable. With the growing number of people\npresenting back pain and related problems, the semi-automatic segmentation and\n3D reconstruction of vertebral bodies became even more important to support\ndecision making. A 3D reconstruction allows a fast and objective analysis of\neach vertebrae condition, which may play a major role in surgical planning and\nevaluation of suitable treatments. In this paper, we propose 3DBGrowth, which\ndevelops a 3D reconstruction over the efficient Balanced Growth method for 2D\nimages. We also take advantage of the slope coefficient from the annotation\ntime to reduce the total number of annotated slices, reducing the time spent on\nmanual annotation. We show experimental results on a representative dataset\nwith 17 MRI exams demonstrating that our approach significantly outperforms the\ncompetitors and, on average, only 37% of the total slices with vertebral body\ncontent must be annotated without losing performance/accuracy. Compared to the\nstate-of-the-art methods, we have achieved a Dice Score gain of over 5% with\ncomparable processing time. Moreover, 3DBGrowth works well with imprecise seed\npoints, which reduces the time spent on manual annotation by the specialist.", "title": "3DBGrowth: volumetric vertebrae segmentation and reconstruction in magnetic resonance imaging", "author": "Jonathan S. Ramos, Mirela T. Cazzolato, Bruno S. Faiçal, Marcello H. Nogueira-Barbosa, Caetano Traina Jr., Agma J. M. Traina", "published": "2019-06-25", "arxiv_url": "http://arxiv.org/abs/1906.10288v2", "arxiv_id": "1906.10288v2"}
{"abstraction": "The aim of this study was to develop a new method for generating reproducible\n3D measurements for the quantification of the distal radioulnar joint\nmorphology. We hypothesized that automated 3D measurement of the ulnar variance\nand the sigmoid notch angle are comparable to those of the gold standard, while\novercoming some of the drawbacks of conventional 2D measurements. Radiological\ndata of healthy forearm bones of 53 adult subjects were included in the study.\nAutomated measurements for the assessment of the sigmoid-notch morphology based\non 3D landmarks were developed incorporating the subject-specific estimation of\nthe cartilage surface orientation. A common anatomical reference was defined\namong the different imaging modalities and a comparison between the sigmoid\nnotch angle and UV measurements was performed in radiographs, CT scans and 3D\nmodels. Finally, the developed UV measurements in 3D were compared to the\nmethod by radiographs in an experimental setup with 3D printed bone models. The\nproposed automated 3D analysis of notch subtype showed a significantly larger\nnotch radius for negative notch angle compared to positive sigmoid notch angle\nsubjects. Similar UV measurements were obtained in healthy joint morphologies\nwith a high correlation between the radiographs and 3D measurements, for\nsigmoid notch angle and UV . In the experimental setup with a modified radial\ninclination, the UV was on average 1.13 mm larger in the radiographs compared\nto the 3D measurements, and 1.30 mm larger in the cases with a modified palmar\ntilt. The developed 3D measurements allowed to reliably quantify differences in\nthe sigmoid notch subtypes.", "title": "Three-Dimensional Automated Assessment of the Distal Radioulnar Joint Morphology according to Sigmoid Notch Surface Orientation", "author": "Simon Roner, Philipp Fürnstahl, Anne-Gita Scheibler, Ladislav Nagy, Fabio Carrillo", "published": "2020-03-27", "arxiv_url": "http://arxiv.org/abs/2003.12333v1", "arxiv_id": "2003.12333v1"}
{"abstraction": "Using printed photograph and replaying videos of biometric modalities, such\nas iris, fingerprint and face, are common attacks to fool the recognition\nsystems for granting access as the genuine user. With the growing online\nperson-to-person shopping (e.g., Ebay and Craigslist), such attacks also\nthreaten those services, where the online photo illustration might not be\ncaptured from real items but from paper or digital screen. Thus, the study of\nanti-spoofing should be extended from modality-specific solutions to\ngeneric-object-based ones. In this work, we define and tackle the problem of\nGeneric Object Anti-Spoofing (GOAS) for the first time. One significant cue to\ndetect these attacks is the noise patterns introduced by the capture sensors\nand spoof mediums. Different sensor/medium combinations can result in diverse\nnoise patterns. We propose a GAN-based architecture to synthesize and identify\nthe noise patterns from seen and unseen medium/sensor combinations. We show\nthat the procedure of synthesis and identification are mutually beneficial. We\nfurther demonstrate the learned GOAS models can directly contribute to\nmodality-specific anti-spoofing without domain transfer. The code and GOSet\ndataset are available at cvlab.cse.msu.edu/project-goas.html.", "title": "Noise Modeling, Synthesis and Classification for Generic Object Anti-Spoofing", "author": "Joel Stehouwer, Amin Jourabloo, Yaojie Liu, Xiaoming Liu", "published": "2020-03-29", "arxiv_url": "http://arxiv.org/abs/2003.13043v2", "arxiv_id": "2003.13043v2"}
{"abstraction": "The training of deep learning models typically requires extensive data, which\nare not readily available as large well-curated medical-image datasets for\ndevelopment of artificial intelligence (AI) models applied in Radiology.\nRecognizing the potential for transfer learning (TL) to allow a fully trained\nmodel from one institution to be fine-tuned by another institution using a much\nsmall local dataset, this report describes the challenges, methodology, and\nbenefits of TL within the context of developing an AI model for a basic\nuse-case, segmentation of Left Ventricular Myocardium (LVM) on images from\n4-dimensional coronary computed tomography angiography. Ultimately, our results\nfrom comparisons of LVM segmentation predicted by a model locally trained using\nrandom initialization, versus one training-enhanced by TL, showed that a\nuse-case model initiated by TL can be developed with sparse labels with\nacceptable performance. This process reduces the time required to build a new\nmodel in the clinical environment at a different institution.", "title": "Democratizing Artificial Intelligence in Healthcare: A Study of Model Development Across Two Institutions Incorporating Transfer Learning", "author": "Vikash Gupta1, Holger Roth, Varun Buch3, Marcio A. B. C. Rockenbach, Richard D White, Dong Yang, Olga Laur, Brian Ghoshhajra, Ittai Dayan, Daguang Xu, Mona G. Flores, Barbaros Selnur Erdal", "published": "2020-09-25", "arxiv_url": "http://arxiv.org/abs/2009.12437v1", "arxiv_id": "2009.12437v1"}
{"abstraction": "Radiofrequency field inhomogeneity is a significant issue in imaging large\nfields of view in high- and ultrahigh-field MRI. Passive shimming with coupled\ncoils or dielectric pads is the most common approach at 3 T. We introduce and\ntest light and compact metasurface, providing the same homogeneity improvement\nin clinical abdominal imaging at 3 T as a conventional dielectric pad. The\nmetasurface comprising a periodic structure of copper strips and parallel-plate\ncapacitive elements printed on a flexible polyimide substrate supports\npropagation of slow electromagnetic waves similar to a high-permittivity slab.\nWe compare the metasurface operating inside a transmit body birdcage coil to\nthe state-of-the-art pad by numerical simulations and in vivo study on healthy\nvolunteers. Numerical simulations with different body models show that the\nlocal minimum of B1+ causing a dark void in the abdominal domain is removed by\nthe metasurface with comparable resulting homogeneity as for the pad without\nnoticeable SAR change. In vivo results confirm similar homogeneity improvement\nand demonstrate the stability to body mass index. The light, flexible, and\ncheap metasurface can replace a relatively heavy and expensive pad based on the\naqueous suspension of barium titanate in abdominal imaging at 3 T.", "title": "Improving B1 homogeneity in abdominal imaging at 3 T with light and compact metasurface", "author": "Vsevolod Vorobyev, Alena Shchelokova, Alexander Efimtcev, Juan D. Baena, Redha Abdeddaim, Pavel Belov, Irina Melchakova, Stanislav Glybovski", "published": "2021-02-02", "arxiv_url": "http://arxiv.org/abs/2102.01384v1", "arxiv_id": "2102.01384v1"}
{"abstraction": "Magnetic Resonance Imaging (MRI) is a widely used imaging technique to assess\nbrain tumor. Accurately segmenting brain tumor from MR images is the key to\nclinical diagnostics and treatment planning. In addition, multi-modal MR images\ncan provide complementary information for accurate brain tumor segmentation.\nHowever, it's common to miss some imaging modalities in clinical practice. In\nthis paper, we present a novel brain tumor segmentation algorithm with missing\nmodalities. Since it exists a strong correlation between multi-modalities, a\ncorrelation model is proposed to specially represent the latent multi-source\ncorrelation. Thanks to the obtained correlation representation, the\nsegmentation becomes more robust in the case of missing modality. First, the\nindividual representation produced by each encoder is used to estimate the\nmodality independent parameter. Then, the correlation model transforms all the\nindividual representations to the latent multi-source correlation\nrepresentations. Finally, the correlation representations across modalities are\nfused via attention mechanism into a shared representation to emphasize the\nmost important features for segmentation. We evaluate our model on BraTS 2018\nand BraTS 2019 dataset, it outperforms the current state-of-the-art methods and\nproduces robust results when one or more modalities are missing.", "title": "Latent Correlation Representation Learning for Brain Tumor Segmentation with Missing MRI Modalities", "author": "Tongxue Zhou, Stéphane Canu, Pierre Vera, Su Ruan", "published": "2021-04-13", "arxiv_url": "http://arxiv.org/abs/2104.06231v2", "arxiv_id": "2104.06231v2"}
{"abstraction": "Assessing the Mitotic Count has a known high degree of intra- and inter-rater\nvariability. Computer-aided systems have proven to decrease this variability\nand reduce labeling time. These systems, however, are generally highly\ndependent on their training domain and show poor applicability to unseen\ndomains. In histopathology, these domain shifts can result from various\nsources, including different slide scanning systems used to digitize histologic\nsamples. The MItosis DOmain Generalization challenge focused on this specific\ndomain shift for the task of mitotic figure detection. This work presents a\nmitotic figure detection algorithm developed as a baseline for the challenge,\nbased on domain adversarial training. On the challenge's test set, the\nalgorithm scored an F$_1$ score of 0.7183. The corresponding network weights\nand code for implementing the network are made publicly available.", "title": "Domain Adversarial RetinaNet as a Reference Algorithm for the MItosis DOmain Generalization Challenge", "author": "Frauke Wilm, Christian Marzahl, Katharina Breininger, Marc Aubreville", "published": "2021-08-25", "arxiv_url": "http://arxiv.org/abs/2108.11269v3", "arxiv_id": "2108.11269v3"}
{"abstraction": "While medical images such as computed tomography (CT) are stored in DICOM\nformat in hospital PACS, it is still quite routine in many countries to print a\nfilm as a transferable medium for the purposes of self-storage and secondary\nconsultation. Also, with the ubiquitousness of mobile phone cameras, it is\nquite common to take pictures of CT films, which unfortunately suffer from\ngeometric deformation and illumination variation. In this work, we study the\nproblem of recovering a CT film, which marks \\textbf{the first attempt} in the\nliterature, to the best of our knowledge. We start with building a large-scale\nhead CT film database CTFilm20K, consisting of approximately 20,000 pictures,\nusing the widely used computer graphics software Blender. We also record all\naccompanying information related to the geometric deformation (such as 3D\ncoordinate, depth, normal, and UV maps) and illumination variation (such as\nalbedo map). Then we propose a deep framework called \\textbf{F}ilm\n\\textbf{I}mage \\textbf{Re}covery \\textbf{Net}work (\\textbf{FIReNet}) to tackle\ngeometric deformation and illumination variation using the multiple maps\nextracted from the CT films to collaboratively guide the recovery process.\nFinally, we convert the dewarped images to DICOM files with our cascade model\nfor further analysis such as radiomics feature extraction. Extensive\nexperiments demonstrate the superiority of our approach over the previous\napproaches. We plan to open source the simulated images and deep models for\npromoting the research on CT film image analysis.", "title": "Recovering medical images from CT film photos", "author": "Quan Quan, Qiyuan Wang, Yuanqi Du, Liu Li, S. Kevin Zhou", "published": "2022-03-10", "arxiv_url": "http://arxiv.org/abs/2203.05567v1", "arxiv_id": "2203.05567v1"}
{"abstraction": "Vector-Borne Disease (VBD) is an infectious disease transmitted through the\npathogenic female Aedes mosquito to humans and animals. It is important to\ncontrol dengue disease by reducing the spread of Aedes mosquito vectors.\nCommunity awareness plays acrucial role to ensure Aedes control programmes and\nencourages the communities to involve active participation. Identifying the\nspecies of mosquito will help to recognize the mosquito density in the locality\nand intensifying mosquito control efforts in particular areas. This willhelp in\navoiding Aedes breeding sites around residential areas and reduce adult\nmosquitoes. To serve this purpose, an android application are developed to\nidentify Aedes species that help the community to contribute in mosquito\ncontrol events. Several Android applications have been developed to identify\nspecies like birds, plant species, and Anopheles mosquito species. In this\nwork, a user-friendly mobile application mAedesID is developed for identifying\nthe Aedes mosquito species using a deep learning Convolutional Neural Network\n(CNN) algorithm which is best suited for species image classification and\nachieves better accuracy for voluminous images. The mobile application can be\ndownloaded from the URLhttps://tinyurl.com/mAedesID.", "title": "mAedesID: Android Application for Aedes Mosquito Species Identification using Convolutional Neural Network", "author": "G. Jeyakodi, Trisha Agarwal, P. Shanthi Bala", "published": "2023-05-02", "arxiv_url": "http://arxiv.org/abs/2305.07664v2", "arxiv_id": "2305.07664v2"}
{"abstraction": "In this paper, we introduce a completion framework to reconstruct the\ngeometric shapes of various anatomies, including organs, vessels and muscles.\nOur work targets a scenario where one or multiple anatomies are missing in the\nimaging data due to surgical, pathological or traumatic factors, or simply\nbecause these anatomies are not covered by image acquisition. Automatic\nreconstruction of the missing anatomies benefits many applications, such as\norgan 3D bio-printing, whole-body segmentation, animation realism,\npaleoradiology and forensic imaging. We propose two paradigms based on a 3D\ndenoising auto-encoder (DAE) to solve the anatomy reconstruction problem: (i)\nthe DAE learns a many-to-one mapping between incomplete and complete instances;\n(ii) the DAE learns directly a one-to-one residual mapping between the\nincomplete instances and the target anatomies. We apply a loss aggregation\nscheme that enables the DAE to learn the many-to-one mapping more effectively\nand further enhances the learning of the residual mapping. On top of this, we\nextend the DAE to a multiclass completor by assigning a unique label to each\nanatomy involved. We evaluate our method using a CT dataset with whole-body\nsegmentations. Results show that our method produces reasonable anatomy\nreconstructions given instances with different levels of incompleteness (i.e.,\none or multiple random anatomies are missing). Codes and pretrained models are\npublicly available at https://github.com/Jianningli/medshapenet-feedback/\ntree/main/anatomy-completor", "title": "Anatomy Completor: A Multi-class Completion Framework for 3D Anatomy Reconstruction", "author": "Jianning Li, Antonio Pepe, Gijs Luijten, Christina Schwarz-Gsaxner, Jens Kleesiek, Jan Egger", "published": "2023-09-10", "arxiv_url": "http://arxiv.org/abs/2309.04956v1", "arxiv_id": "2309.04956v1"}
{"abstraction": "This paper proposes a novel approach to generating omni-directional images\nfrom a single snapshot picture. The previous method has relied on the\ngenerative adversarial networks based on convolutional neural networks (CNN).\nAlthough this method has successfully generated omni-directional images, CNN\nhas two drawbacks for this task. First, since a convolutional layer only\nprocesses a local area, it is difficult to propagate the information of an\ninput snapshot picture embedded in the center of the omni-directional image to\nthe edges of the image. Thus, the omni-directional images created by the\nCNN-based generator tend to have less diversity at the edges of the generated\nimages, creating similar scene images. Second, the CNN-based model requires\nlarge video memory in graphics processing units due to the nature of the deep\nstructure in CNN since shallow-layer networks only receives signals from a\nlimited range of the receptive field. To solve these problems, MLPMixer-based\nmethod was proposed in this paper. The MLPMixer has been proposed as an\nalternative to the self-attention in the transformer, which captures long-range\ndependencies and contextual information. This enables to propagate information\nefficiently in the omni-directional image generation task. As a result,\ncompetitive performance has been achieved with reduced memory consumption and\ncomputational cost, in addition to increasing diversity of the generated\nomni-directional images.", "title": "Increasing diversity of omni-directional images generated from single image using cGAN based on MLPMixer", "author": "Atsuya Nakata, Ryuto Miyazaki, Takao Yamanaka", "published": "2023-09-15", "arxiv_url": "http://arxiv.org/abs/2309.08129v1", "arxiv_id": "2309.08129v1"}
{"abstraction": "Compressive focal plane arrays (FPA) enable cost-effective high-resolution\n(HR) imaging by acquisition of several multiplexed measurements on a\nlow-resolution (LR) sensor. Multiplexed encoding of the visual scene is\ntypically performed via electronically controllable spatial light modulators\n(SLM). An HR image is then reconstructed from the encoded measurements by\nsolving an inverse problem that involves the forward model of the imaging\nsystem. To capture system non-idealities such as optical aberrations, a\nmainstream approach is to conduct an offline calibration scan to measure the\nsystem response for a point source at each spatial location on the imaging\ngrid. However, it is challenging to run calibration scans when using structured\nSLMs as they cannot encode individual grid locations. In this study, we propose\na novel compressive FPA system based on online deep-learning calibration of\nmultiplexed LR measurements (CalibFPA). We introduce a piezo-stage that\nlocomotes a pre-printed fixed coded aperture. A deep neural network is then\nleveraged to correct for the influences of system non-idealities in multiplexed\nmeasurements without the need for offline calibration scans. Finally, a deep\nplug-and-play algorithm is used to reconstruct images from corrected\nmeasurements. On simulated and experimental datasets, we demonstrate that\nCalibFPA outperforms state-of-the-art compressive FPA methods. We also report\nanalyses to validate the design elements in CalibFPA and assess computational\ncomplexity.", "title": "CalibFPA: A Focal Plane Array Imaging System based on Online Deep-Learning Calibration", "author": "Alper Güngör, M. Umut Bahceci, Yasin Ergen, Ahmet Sözak, O. Oner Ekiz, Tolga Yelboga, Tolga Çukur", "published": "2023-09-20", "arxiv_url": "http://arxiv.org/abs/2309.11421v1", "arxiv_id": "2309.11421v1"}
{"abstraction": "A significant volume of analog information, i.e., documents and images, have\nbeen digitized in the form of scanned copies for storing, sharing, and/or\nanalyzing in the digital world. However, the quality of such contents is\nseverely degraded by various distortions caused by printing, storing, and\nscanning processes in the physical world. Although restoring high-quality\ncontent from scanned copies has become an indispensable task for many products,\nit has not been systematically explored, and to the best of our knowledge, no\npublic datasets are available. In this paper, we define this problem as\nDescanning and introduce a new high-quality and large-scale dataset named\nDESCAN-18K. It contains 18K pairs of original and scanned images collected in\nthe wild containing multiple complex degradations. In order to eliminate such\ncomplex degradations, we propose a new image restoration model called\nDescanDiffusion consisting of a color encoder that corrects the global color\ndegradation and a conditional denoising diffusion probabilistic model (DDPM)\nthat removes local degradations. To further improve the generalization ability\nof DescanDiffusion, we also design a synthetic data generation scheme by\nreproducing prominent degradations in scanned images. We demonstrate that our\nDescanDiffusion outperforms other baselines including commercial restoration\nproducts, objectively and subjectively, via comprehensive experiments and\nanalyses.", "title": "Descanning: From Scanned to the Original Images with a Color Correction Diffusion Model", "author": "Junghun Cha, Ali Haider, Seoyun Yang, Hoeyeong Jin, Subin Yang, A. F. M. Shahab Uddin, Jaehyoung Kim, Soo Ye Kim, Sung-Ho Bae", "published": "2024-02-08", "arxiv_url": "http://arxiv.org/abs/2402.05350v1", "arxiv_id": "2402.05350v1"}
{"abstraction": "In this work, we present a comparison between color spaces namely YUV, LAB,\nRGB and their effect on learned image compression. For this we use the\nstructure and color based learned image codec (SLIC) from our prior work, which\nconsists of two branches - one for the luminance component (Y or L) and another\nfor chrominance components (UV or AB). However, for the RGB variant we input\nall 3 channels in a single branch, similar to most learned image codecs\noperating in RGB. The models are trained for multiple bitrate configurations in\neach color space. We report the findings from our experiments by evaluating\nthem on various datasets and compare the results to state-of-the-art image\ncodecs. The YUV model performs better than the LAB variant in terms of MS-SSIM\nwith a Bj{\\o}ntegaard delta bitrate (BD-BR) gain of 7.5\\% using VTM\nintra-coding mode as the baseline. Whereas the LAB variant has a better\nperformance than YUV model in terms of CIEDE2000 having a BD-BR gain of 8\\%.\nOverall, the RGB variant of SLIC achieves the best performance with a BD-BR\ngain of 13.14\\% in terms of MS-SSIM and a gain of 17.96\\% in CIEDE2000 at the\ncost of a higher model complexity.", "title": "A Study on the Effect of Color Spaces in Learned Image Compression", "author": "Srivatsa Prativadibhayankaram, Mahadev Prasad Panda, Jürgen Seiler, Thomas Richter, Heiko Sparenberg, Siegfried Fößel, André Kaup", "published": "2024-06-19", "arxiv_url": "http://arxiv.org/abs/2406.13709v1", "arxiv_id": "2406.13709v1"}
{"abstraction": "We propose a machine learning pipeline for forensic shoeprint pattern\nmatching that improves on the accuracy and generalisability of existing\nmethods. We extract 2D coordinates from shoeprint scans using edge detection\nand align the two shoeprints with iterative closest point (ICP). We then\nextract similarity metrics to quantify how well the two prints match and use\nthese metrics to train a random forest that generates a probabilistic\nmeasurement of how likely two prints are to have originated from the same\noutsole. We assess the generalisability of machine learning methods trained on\nlab shoeprint scans to more realistic crime scene shoeprint data by evaluating\nthe accuracy of our methods on several shoeprint scenarios: partial prints,\nprints with varying levels of blurriness, prints with different amounts of\nwear, and prints from different shoe models. We find that models trained on one\ntype of shoeprint yield extremely high levels of accuracy when tested on\nshoeprint pairs of the same scenario but fail to generalise to other scenarios.\nWe also discover that models trained on a variety of scenarios predict almost\nas accurately as models trained on specific scenarios.", "title": "Improving and Evaluating Machine Learning Methods for Forensic Shoeprint Matching", "author": "Divij Jain, Saatvik Kher, Lena Liang, Yufeng Wu, Ashley Zheng, Xizhen Cai, Anna Plantinga, Elizabeth Upton", "published": "2024-04-02", "arxiv_url": "http://arxiv.org/abs/2405.14878v1", "arxiv_id": "2405.14878v1"}
{"abstraction": "Purpose: To assess the feasibility of deep learning-based high resolution\nsynthetic CT generation from MRI scans of the lower arm for orthopedic\napplications.\n  Methods: A conditional Generative Adversarial Network was trained to\nsynthesize CT images from multi-echo MR images. A training set of MRI and CT\nscans of 9 ex vivo lower arms was acquired and the CT images were registered to\nthe MRI images. Three-fold cross-validation was applied to generate independent\nresults for the entire dataset. The synthetic CT images were quantitatively\nevaluated with the mean absolute error metric, and Dice similarity and surface\nto surface distance on cortical bone segmentations.\n  Results: The mean absolute error was 63.5 HU on the overall tissue volume and\n144.2 HU on the cortical bone. The mean Dice similarity of the cortical bone\nsegmentations was 0.86. The average surface to surface distance between bone on\nreal and synthetic CT was 0.48 mm. Qualitatively, the synthetic CT images\ncorresponded well with the real CT scans and partially maintained high\nresolution structures in the trabecular bone. The bone segmentations on\nsynthetic CT images showed some false positives on tendons, but the general\nshape of the bone was accurately reconstructed.\n  Conclusions: This study demonstrates that high quality synthetic CT can be\ngenerated from MRI scans of the lower arm. The good correspondence of the bone\nsegmentations demonstrates that synthetic CT could be competitive with real CT\nin applications that depend on such segmentations, such as planning of\northopedic surgery and 3D printing.", "title": "CT synthesis from MR images for orthopedic applications in the lower arm using a conditional generative adversarial network", "author": "Frank Zijlstra, Koen Willemsen, Mateusz C. Florkow, Ralph J. B. Sakkers, Harrie H. Weinans, Bart C. H. van der Wal, Marijn van Stralen, Peter R. Seevinck", "published": "2019-01-24", "arxiv_url": "http://arxiv.org/abs/1901.08449v1", "arxiv_id": "1901.08449v1"}
{"abstraction": "High fidelity reproductions of paintings provide new opportunities to museums\nin preserving and providing access to cultural heritage. This paper presents an\nintegrated system which is able to capture and fabricate color, topography and\ngloss of a painting, of which gloss capturing forms the most important\ncontribution. A 3D imaging system, using fringe-encoded stereo imaging, is\nextended to capture spatially-varying gloss, utilizing specular reflectance\npolarization. The gloss is measured by sampling the specular reflection around\nBrewster's angle, where these reflections are effectively polarized, and can be\nseparated from the unpolarized, diffuse reflectance. Off-center gloss\nmeasurements are calibrated relative to the center measurement. Off-specular\ngloss measurements, following from local variation of the surface normal, are\nmasked based on the height map and corrected. Shadowed regions, caused by the\n3D relief, are treated similarly. The area of a single capture is approximately\n180x90mm at a resolution of 25x25micron. Aligned color, height, and gloss tiles\nare stitched together, registering overlapping color areas. These maps are\ninputs for a 3D printer. Two paintings were reproduced to verify the\neffectiveness and efficiency of the proposed system. One painting was scanned\nfour times, consecutively rotated by 90 degrees, to evaluate the influence of\nthe scanning system geometric configuration on the gloss measurement.\nExperimental results show that the method is sufficiently fast for practical\napplication. The results can well be used for the purpose of physical\nreproduction and other applications needing first-order estimates of the\nappearance. Our method to extend appearance scanning with gloss measurements is\na valuable addition in the quest for realistic reproductions, in terms of its\npractical applicability and its perceptual added value, when added to color and\ntopography.", "title": "Gloss, Color and Topography Scanning for Reproducing a Painting's Appearance using 3D printing", "author": "Willemijn Elkhuizen, Tessa Essers, Yu Song, Jo Geraedts, Clemens Weijkamp, Joris Dik, Sylvia Pont", "published": "2019-10-23", "arxiv_url": "http://arxiv.org/abs/1910.10836v1", "arxiv_id": "1910.10836v1"}
{"abstraction": "We propose Artificial Intelligence Prospective Randomized Observer Blinding\nEvaluation (AI-PROBE) for quantitative clinical performance evaluation of\nradiology AI systems within prospective randomized clinical trials. AI-PROBE\nencompasses a study design and a matching radiology IT infrastructure that\nrandomly blinds radiologists for results provided by AI-based image analysis.\nTo demonstrate the applicability of our evaluation framework, we present a\nfirst prospective randomized clinical trial on the effect of Intra-Cranial\nHemorrhage (ICH) detection in emergent care head CT on radiology study\nTurn-Around Time (TAT). Here, we acquired 620 non-contrast head CT scans from\ninpatient and emergency room patients at a large academic hospital. Following\nacquisition, scans were automatically analyzed for the presence of ICH using\ncommercially available software (Aidoc, Tel Aviv, Israel). Cases identified\npositive for ICH by AI (ICH-AI+) were flagged in radiologists' reading\nworklists, where flagging was randomly switched off with probability 50%. TAT\nwas measured as time difference between study completion and first clinically\ncommunicated reporting, with time stamps automatically retrieved from various\nIT systems. TATs for flagged cases (73+/-143 min) were significantly lower than\nTATs for non-flagged (132+/-193 min) cases (p<0.05, one-sided t-test), where\n105 of 122 ICH-AI+ cases were true positive. Total sensitivity, specificity,\nand accuracy over all analyzed cases were 95.0%, 96.7%, and 96.4%,\nrespectively. We conclude that automatic identification of ICH reduces TAT for\nICH in emergent care head CT, which carries the potential for improving timely\nclinical management of ICH. Our results suggest that AI-PROBE can contribute to\nsystematic quantitative evaluation of AI systems in clinical practice using\nclinically meaningful quantities, such as TAT or diagnostic accuracy.", "title": "A Prospective Randomized Clinical Trial for Measuring Radiology Study Reporting Time on Artificial Intelligence-Based Detection of Intracranial Hemorrhage in Emergent Care Head CT", "author": "Axel Wismüller, Larry Stockmaster", "published": "2020-02-28", "arxiv_url": "http://arxiv.org/abs/2002.12515v1", "arxiv_id": "2002.12515v1"}
{"abstraction": "As field surveys used for manual lithological mapping are costly and\ntime-consuming, digital lithological mapping (DLM) that utilizes remotely\nsensed spectral imaging provides a viable and economical alternative.\nGenerally, DLM has been performed using spectral imaging with the use of\nlaboratory-generated generic endmember signatures. To that end, this paper\nproposes generating a single-target abundance mineral map for DLM, where the\ngenerated map can further be used as a guide for the selection or avoidance of\na field survey. For that, a stochastic cancellation-based methodology was used\nto generate a site-specific endemic signature for the mineral in concern to\nreduce the inclusive nature otherwise present in DLM. Furthermore, a soil pixel\nalignment strategy to visualize the relative purity level of the target mineral\nhas been introduced in the proposed work. Then, for the method validation,\nmapping of limestone deposits in the Jaffna peninsula of Sri Lanka was\nconducted as the case study using satellite-based spectral imaging as the\ninput. It was observed that despite the low signal-to-noise ratio of the input\nhyperspectral data the proposed methodology was able to robustly extract the\nrich information contained in the input data. Further, a field survey was\nconducted to collect soil samples of four sites chosen by the proposed DLM from\nthe Jaffna peninsula as an algorithm validation and to demonstrate the\napplication of the proposed solution. The proposed abundance threshold of 0.1\ncoincided with the industrial standard X-ray diffraction (XRD) threshold of 5%\nfor the mineral presence. The results of the XRD test validated the use of the\nalgorithm in the selection of sites to be surveyed, hence could avoid\nconducting a costly field survey on the assumption of the existence of a\nmineral.", "title": "Single-target mineral detection with site-specific endmember extraction for survey points identification: A case study of Jaffna, Sri Lanka", "author": "D. Y. L. Ranasinghe, H. M. H. K. Weerasooriya, S. Herath, H. M. V. R. Herath, G. M. R. I. Godaliyadda, M. P. B. Ekanayake, A. Senaratne, S. L. P. Yasakethu", "published": "2021-09-06", "arxiv_url": "http://arxiv.org/abs/2109.08143v1", "arxiv_id": "2109.08143v1"}
{"abstraction": "Fluorescence microscopy is essential to study biological structures and\ndynamics. However, existing systems suffer from a tradeoff between\nfield-of-view (FOV), resolution, and complexity, and thus cannot fulfill the\nemerging need of miniaturized platforms providing micron-scale resolution\nacross centimeter-scale FOVs. To overcome this challenge, we developed\nComputational Miniature Mesoscope (CM$^2$) that exploits a computational\nimaging strategy to enable single-shot 3D high-resolution imaging across a wide\nFOV in a miniaturized platform. Here, we present CM$^2$ V2 that significantly\nadvances both the hardware and computation. We complement the 3$\\times$3\nmicrolens array with a new hybrid emission filter that improves the imaging\ncontrast by 5$\\times$, and design a 3D-printed freeform collimator for the LED\nilluminator that improves the excitation efficiency by 3$\\times$. To enable\nhigh-resolution reconstruction across the large imaging volume, we develop an\naccurate and efficient 3D linear shift-variant (LSV) model that characterizes\nthe spatially varying aberrations. We then train a multi-module deep learning\nmodel, CM$^2$Net, using only the 3D-LSV simulator. We show that CM$^2$Net\ngeneralizes well to experiments and achieves accurate 3D reconstruction across\na $\\sim$7-mm FOV and 800-$\\mu$m depth, and provides $\\sim$6-$\\mu$m lateral and\n$\\sim$25-$\\mu$m axial resolution. This provides $\\sim$8$\\times$ better axial\nlocalization and $\\sim$1400$\\times$ faster speed as compared to the previous\nmodel-based algorithm. We anticipate this simple and low-cost computational\nminiature imaging system will be impactful to many large-scale 3D fluorescence\nimaging applications.", "title": "Deep-learning-augmented Computational Miniature Mesoscope", "author": "Yujia Xue, Qianwan Yang, Guorong Hu, Kehan Guo, Lei Tian", "published": "2022-04-30", "arxiv_url": "http://arxiv.org/abs/2205.00123v5", "arxiv_id": "2205.00123v5"}
{"abstraction": "The Auto-ICell system, a novel, and cost-effective integrated droplet\nmicrofluidic system, is introduced for real-time analysis of single-cell\nmorphology and apoptosis. This system integrates a 3D-printed microfluidic chip\nwith image analysis algorithms, enabling the generation of uniform droplet\nreactors and immediate image analysis. The system employs a color-based image\nanalysis algorithm in the bright field for droplet content analysis. Meanwhile,\nin the fluorescence field, cell apoptosis is quantitatively measured through a\ncombination of deep-learning-enabled multiple fluorescent channel analysis and\na live/dead cell stain kit. Breast cancer cells are encapsulated within uniform\ndroplets, with diameters ranging from 70 {\\mu}m to 240 {\\mu}m, generated at a\nhigh throughput of 1,500 droplets per minute. Real-time image analysis results\nare displayed within 2 seconds on a custom graphical user interface (GUI). The\nsystem provides an automatic calculation of the distribution and ratio of\nencapsulated dyes in the bright field, and in the fluorescent field, cell\nblebbing and cell circularity are observed and quantified respectively. The\nAuto-ICell system is non-invasive and provides online detection, offering a\nrobust, time-efficient, user-friendly, and cost-effective solution for\nsingle-cell analysis. It significantly enhances the detection throughput of\ndroplet single-cell analysis by reducing setup costs and improving operational\nperformance. This study highlights the potential of the Auto-ICell system in\nadvancing biological research and personalized disease treatment, with\npromising applications in cell culture, biochemical microreactors, drug\ncarriers, cell-based assays, synthetic biology, and point-of-care diagnostics.", "title": "Auto-ICell: An Accessible and Cost-Effective Integrative Droplet Microfluidic System for Real-Time Single-Cell Morphological and Apoptotic Analysis", "author": "Yuanyuan Wei, Meiai Lin, Shanhang Luo, Syed Muhammad Tariq Abbasi, Liwei Tan, Guangyao Cheng, Bijie Bai, Yi-Ping Ho, Scott Wu Yuan, Ho-Pui Ho", "published": "2023-11-06", "arxiv_url": "http://arxiv.org/abs/2311.02927v1", "arxiv_id": "2311.02927v1"}
{"abstraction": "Examining the authenticity of images has become increasingly important as\nmanipulation tools become more accessible and advanced. Recent work has shown\nthat while CNN-based image manipulation detectors can successfully identify\nmanipulations, they are also vulnerable to adversarial attacks, ranging from\nsimple double JPEG compression to advanced pixel-based perturbation. In this\npaper we explore another method of highly plausible attack: printing and\nscanning. We demonstrate the vulnerability of two state-of-the-art models to\nthis type of attack. We also propose a new machine learning model that performs\ncomparably to these state-of-the-art models when trained and validated on\nprinted and scanned images. Of the three models, our proposed model outperforms\nthe others when trained and validated on images from a single printer. To\nfacilitate this exploration, we create a dataset of over 6,000 printed and\nscanned image blocks. Further analysis suggests that variation between images\nproduced from different printers is significant, large enough that good\nvalidation accuracy on images from one printer does not imply similar\nvalidation accuracy on identical images from a different printer.", "title": "Printing and Scanning Attack for Image Counter Forensics", "author": "Hailey Joren, Otkrist Gupta, Dan Raviv", "published": "2020-04-27", "arxiv_url": "http://arxiv.org/abs/2005.02160v2", "arxiv_id": "2005.02160v2"}
{"abstraction": "Researches have been done on Ethiopic scripts. However studies excluded the\nGeez numbers from the studies because of different reasons. This paper presents\noffline handwritten and machine printed Geez number recognition using feed\nforward back propagation artificial neural network. On this study, different\nGeez image characters were collected from google image search and three persons\nare instructed to write the numbers using pencil. In total we have collected\n560 numbers of characters. We have used 460 of the characters for training and\n100 are used for testing. Accordingly we have achieved overall all\nclassification ~89:88%", "title": "Handwritten and Machine printed OCR for Geez Numbers Using Artificial Neural Network", "author": "Eyob Gebretinsae Beyene", "published": "2019-11-15", "arxiv_url": "http://arxiv.org/abs/1911.06845v1", "arxiv_id": "1911.06845v1"}
{"abstraction": "The goal of this work is to propose a robust, fast, and fully automatic\nmethod for personalized cranial defect reconstruction and implant modeling.\n  We propose a two-step deep learning-based method using a modified U-Net\narchitecture to perform the defect reconstruction, and a dedicated iterative\nprocedure to improve the implant geometry, followed by automatic generation of\nmodels ready for 3-D printing. We propose a cross-case augmentation based on\nimperfect image registration combining cases from different datasets. We\nperform ablation studies regarding different augmentation strategies and\ncompare them to other state-of-the-art methods.\n  We evaluate the method on three datasets introduced during the AutoImplant\n2021 challenge, organized jointly with the MICCAI conference. We perform the\nquantitative evaluation using the Dice and boundary Dice coefficients, and the\nHausdorff distance. The average Dice coefficient, boundary Dice coefficient,\nand the 95th percentile of Hausdorff distance are 0.91, 0.94, and 1.53 mm\nrespectively. We perform an additional qualitative evaluation by 3-D printing\nand visualization in mixed reality to confirm the implant's usefulness.\n  We propose a complete pipeline that enables one to create the cranial implant\nmodel ready for 3-D printing. The described method is a greatly extended\nversion of the method that scored 1st place in all AutoImplant 2021 challenge\ntasks. We freely release the source code, that together with the open datasets,\nmakes the results fully reproducible. The automatic reconstruction of cranial\ndefects may enable manufacturing personalized implants in a significantly\nshorter time, possibly allowing one to perform the 3-D printing process\ndirectly during a given intervention. Moreover, we show the usability of the\ndefect reconstruction in mixed reality that may further reduce the surgery\ntime.", "title": "Deep Learning-based Framework for Automatic Cranial Defect Reconstruction and Implant Modeling", "author": "Marek Wodzinski, Mateusz Daniol, Miroslaw Socha, Daria Hemmerling, Maciej Stanuch, Andrzej Skalski", "published": "2022-04-13", "arxiv_url": "http://arxiv.org/abs/2204.06310v1", "arxiv_id": "2204.06310v1"}
{"abstraction": "Automatic text recognition from ancient handwritten record images is an\nimportant problem in the genealogy domain. However, critical challenges such as\nvarying noise conditions, vanishing texts, and variations in handwriting make\nthe recognition task difficult. We tackle this problem by developing a\nhandwritten-to-machine-print conditional Generative Adversarial network\n(HW2MP-GAN) model that formulates handwritten recognition as a\ntext-Image-to-text-Image translation problem where a given image, typically in\nan illegible form, is converted into another image, close to its machine-print\nform. The proposed model consists of three-components including a generator,\nand word-level and character-level discriminators. The model incorporates\nSliced Wasserstein distance (SWD) and U-Net architectures in HW2MP-GAN for\nbetter quality image-to-image transformation. Our experiments reveal that\nHW2MP-GAN outperforms state-of-the-art baseline cGAN models by almost 30 in\nFrechet Handwritten Distance (FHD), 0.6 on average Levenshtein distance and 39%\nin word accuracy for image-to-image translation on IAM database. Further,\nHW2MP-GAN improves handwritten recognition word accuracy by 1.3% compared to\nbaseline handwritten recognition models on the IAM database.", "title": "Illegible Text to Readable Text: An Image-to-Image Transformation using Conditional Sliced Wasserstein Adversarial Networks", "author": "Mostafa Karimi, Gopalkrishna Veni, Yen-Yun Yu", "published": "2019-10-11", "arxiv_url": "http://arxiv.org/abs/1910.05425v1", "arxiv_id": "1910.05425v1"}
{"abstraction": "Fingerprint recognition systems are widely deployed in various real-life\napplications as they have achieved high accuracy. The widely used applications\ninclude border control, automated teller machine (ATM), and attendance\nmonitoring systems. However, these critical systems are prone to spoofing\nattacks (a.k.a presentation attacks (PA)). PA for fingerprint can be performed\nby presenting gummy fingers made from different materials such as silicone,\ngelatine, play-doh, ecoflex, 2D printed paper, 3D printed material, or latex.\nBiometrics Researchers have developed Presentation Attack Detection (PAD)\nmethods as a countermeasure to PA. PAD is usually done by training a machine\nlearning classifier for known attacks for a given dataset, and they achieve\nhigh accuracy in this task. However, generalizing to unknown attacks is an\nessential problem from applicability to real-world systems, mainly because\nattacks cannot be exhaustively listed in advance. In this survey paper, we\npresent a comprehensive survey on existing PAD algorithms for fingerprint\nrecognition systems, specifically from the standpoint of detecting unknown PAD.\nWe categorize PAD algorithms, point out their advantages/disadvantages, and\nfuture directions for this area.", "title": "A Survey on Unknown Presentation Attack Detection for Fingerprint", "author": "Jag Mohan Singh, Ahmed Madhun, Guoqiang Li, Raghavendra Ramachandra", "published": "2020-05-17", "arxiv_url": "http://arxiv.org/abs/2005.08337v1", "arxiv_id": "2005.08337v1"}
{"abstraction": "Knowing the printer model used to print a given document may provide a\ncrucial lead towards identifying counterfeits or conversely verifying the\nvalidity of a real document. Inkjet printers produce probabilistic droplet\npatterns that appear to be distinct for each printer model and as such we\ninvestigate the utilization of droplet characteristics including frequency\ndomain features extracted from printed document scans for the classification of\nthe underlying printer model. We collect and publish a dataset of high\nresolution document scans and show that our extracted features are informative\nenough to enable a neural network to distinguish not only the printer\nmanufacturer, but also individual printer models.", "title": "Classification of Inkjet Printers based on Droplet Statistics", "author": "Patrick Takenaka, Manuel Eberhardinger, Daniel Grießhaber, Johannes Maucher", "published": "2024-06-26", "arxiv_url": "http://arxiv.org/abs/2407.09539v1", "arxiv_id": "2407.09539v1"}
{"abstraction": "The field of 3D medical vision self-supervised learning lacks consistency and\nstandardization. While many methods have been developed it is impossible to\nidentify the current state-of-the-art, due to i) varying and small pre-training\ndatasets, ii) varying architectures, and iii) being evaluated on differing\ndownstream datasets. In this paper we bring clarity to this field and lay the\nfoundation for further method advancements: We a) publish the largest publicly\navailable pre-training dataset comprising 114k 3D brain MRI volumes and b)\nbenchmark existing SSL methods under common architectures and c) provide the\ncode of our framework publicly to facilitate rapid adoption and reproduction.\nThis pre-print \\textit{only describes} the dataset contribution (a); Data,\nbenchmark, and codebase will be made available shortly.", "title": "An OpenMind for 3D medical vision self-supervised learning", "author": "Tassilo Wald, Constantin Ulrich, Jonathan Suprijadi, Michal Nohel, Robin Peretzke, Klaus H. Maier-Hein", "published": "2024-12-22", "arxiv_url": "http://arxiv.org/abs/2412.17041v1", "arxiv_id": "2412.17041v1"}
{"abstraction": "This paper presents our recent development on a portable and refreshable text\nreading and sensory substitution system for the blind or visually impaired\n(BVI), called Finger-eye. The system mainly consists of an opto-text processing\nunit and a compact electro-tactile based display that can deliver text-related\nelectrical signals to the fingertip skin through a wearable and Braille-dot\npatterned electrode array and thus delivers the electro-stimulation based\nBraille touch sensations to the fingertip. To achieve the goal of aiding BVI to\nread any text not written in Braille through this portable system, in this\nwork, a Rapid Optical Character Recognition (R-OCR) method is firstly developed\nfor real-time processing text information based on a Fisheye imaging device\nmounted at the finger-wearable electro-tactile display. This allows real-time\ntranslation of printed text to electro-Braille along with natural movement of\nuser's fingertip as if reading any Braille display or book. More importantly,\nan electro-tactile neuro-stimulation feedback mechanism is proposed and\nincorporated with the R-OCR method, which facilitates a new opto-electrotactile\nfeedback based text line tracking control approach that enables text line\nfollowing by user fingertip during reading. Multiple experiments were designed\nand conducted to test the ability of blindfolded participants to read through\nand follow the text line based on the opto-electrotactile-feedback method. The\nexperiments show that as the result of the opto-electrotactile-feedback, the\nusers were able to maintain their fingertip within a $2mm$ distance of the text\nwhile scanning a text line. This research is a significant step to aid the BVI\nusers with a portable means to translate and follow to read any printed text to\nBraille, whether in the digital realm or physically, on any surface.", "title": "Printed Texts Tracking and Following for a Finger-Wearable Electro-Braille System Through Opto-electrotactile Feedback", "author": "Mehdi Rahimi, Yantao Shen, Zhiming Liu, Fang Jiang", "published": "2021-08-06", "arxiv_url": "http://arxiv.org/abs/2109.02385v1", "arxiv_id": "2109.02385v1"}
{"abstraction": "In the domain of Biometrics, recognition systems based on iris, fingerprint\nor palm print scans etc. are often considered more dependable due to extremely\nlow variance in the properties of these entities with respect to time. However,\nover the last decade data processing capability of computers has increased\nmanifold, which has made real-time video content analysis possible. This shows\nthat the need of the hour is a robust and highly automated Face Detection and\nRecognition algorithm with credible accuracy rate. The proposed Face Detection\nand Recognition system using Discrete Wavelet Transform (DWT) accepts face\nframes as input from a database containing images from low cost devices such as\nVGA cameras, webcams or even CCTV's, where image quality is inferior. Face\nregion is then detected using properties of L*a*b* color space and only Frontal\nFace is extracted such that all additional background is eliminated. Further,\nthis extracted image is converted to grayscale and its dimensions are resized\nto 128 x 128 pixels. DWT is then applied to entire image to obtain the\ncoefficients. Recognition is carried out by comparison of the DWT coefficients\nbelonging to the test image with those of the registered reference image. On\ncomparison, Euclidean distance classifier is deployed to validate the test\nimage from the database. Accuracy for various levels of DWT Decomposition is\nobtained and hence, compared.", "title": "A robust, low-cost approach to Face Detection and Face Recognition", "author": "Divya Jyoti, Aman Chadha, Pallavi Vaidya, M. Mani Roja", "published": "2011-11-04", "arxiv_url": "http://arxiv.org/abs/1111.1090v1", "arxiv_id": "1111.1090v1"}
{"abstraction": "The use of computed tomography (CT) imaging has become of increasing interest\nto academic areas outside of the field of medical imaging and industrial\ninspection, e.g., to biology and cultural heritage research. The pecularities\nof these fields, however, sometimes require that objects need to be imaged\non-site, e.g., in field-work conditions or in museum collections. Under these\ncircumstances, it is often not possible to use a commercial device and a custom\nsolution is the only viable option. In order to achieve high image quality\nunder adverse conditions, reliable calibration and trajectory reproduction are\nusually key requirements for any custom CT scanning system. Here, we introduce\nthe construction of a low-cost disassemblable CT scanner that allows\ncalibration even when trajectory reproduction is not possible due to the\nlimitations imposed by the project conditions. Using 3D-printed in-image\ncalibration phantoms, we compute a projection matrix directly from each\ncaptured X-ray projection. We describe our method in detail and show successful\ntomographic reconstructions of several specimen as proof of concept.", "title": "Disassemblable Fieldwork CT Scanner Using a 3D-printed Calibration Phantom", "author": "Florian Schiffers, Thomas Bochynek, Andre Aichert, Tobias Würfl, Michael Rubenstein, Oliver Cossairt", "published": "2020-11-12", "arxiv_url": "http://arxiv.org/abs/2011.06671v1", "arxiv_id": "2011.06671v1"}
{"abstraction": "When predicting PM2.5 concentrations, it is necessary to consider complex\ninformation sources since the concentrations are influenced by various factors\nwithin a long period. In this paper, we identify a set of critical domain\nknowledge for PM2.5 forecasting and develop a novel graph based model,\nPM2.5-GNN, being capable of capturing long-term dependencies. On a real-world\ndataset, we validate the effectiveness of the proposed model and examine its\nabilities of capturing both fine-grained and long-term influences in PM2.5\nprocess. The proposed PM2.5-GNN has also been deployed online to provide free\nforecasting service.", "title": "PM2.5-GNN: A Domain Knowledge Enhanced Graph Neural Network For PM2.5 Forecasting", "author": "Shuo Wang, Yanran Li, Jiang Zhang, Qingye Meng, Lingwei Meng, Fei Gao", "published": "2020-02-10", "arxiv_url": "http://arxiv.org/abs/2002.12898v2", "arxiv_id": "2002.12898v2"}
{"abstraction": "Thermal infrared cameras are increasingly being used in various applications\nsuch as robot vision, industrial inspection and medical imaging, thanks to\ntheir improved resolution and portability. However, the performance of\ntraditional computer vision techniques developed for electro-optical imagery\ndoes not directly translate to the thermal domain due to two major reasons:\nthese algorithms require photometric assumptions to hold, and methods for\nphotometric calibration of RGB cameras cannot be applied to thermal-infrared\ncameras due to difference in data acquisition and sensor phenomenology. In this\npaper, we take a step in this direction, and introduce a novel algorithm for\nonline photometric calibration of thermal-infrared cameras. Our proposed method\ndoes not require any specific driver/hardware support and hence can be applied\nto any commercial off-the-shelf thermal IR camera. We present this in the\ncontext of visual odometry and SLAM algorithms, and demonstrate the efficacy of\nour proposed system through extensive experiments for both standard benchmark\ndatasets, and real-world field tests with a thermal-infrared camera in natural\noutdoor environments.", "title": "Online Photometric Calibration of Automatic Gain Thermal Infrared Cameras", "author": "Manash Pratim Das, Larry Matthies, Shreyansh Daftry", "published": "2020-12-07", "arxiv_url": "http://arxiv.org/abs/2012.14292v2", "arxiv_id": "2012.14292v2"}
{"abstraction": "Face anti-spoofing is the key to preventing security breaches in biometric\nrecognition applications. Existing software-based and hardware-based face\nliveness detection methods are effective in constrained environments or\ndesignated datasets only. Deep learning method using RGB and infrared images\ndemands a large amount of training data for new attacks. In this paper, we\npresent a face anti-spoofing method in a real-world scenario by automatic\nlearning the physical characteristics in polarization images of a real face\ncompared to a deceptive attack. A computational framework is developed to\nextract and classify the unique face features using convolutional neural\nnetworks and SVM together. Our real-time polarized face anti-spoofing (PAAS)\ndetection method uses a on-chip integrated polarization imaging sensor with\noptimized processing algorithms. Extensive experiments demonstrate the\nadvantages of the PAAS technique to counter diverse face spoofing attacks\n(print, replay, mask) in uncontrolled indoor and outdoor conditions by learning\npolarized face images of 33 people. A four-directional polarized face image\ndataset is released to inspire future applications within biometric\nanti-spoofing field.", "title": "Face Anti-Spoofing by Learning Polarization Cues in a Real-World Scenario", "author": "Yu Tian, Kunbo Zhang, Leyuan Wang, Zhenan Sun", "published": "2020-03-18", "arxiv_url": "http://arxiv.org/abs/2003.08024v3", "arxiv_id": "2003.08024v3"}
{"abstraction": "Current advances in deep learning is leading to human-level accuracy in\ncomputer vision tasks such as object classification, localization, semantic\nsegmentation, and instance segmentation. In this paper, we describe a new deep\nconvolutional neural network architecture called Vec2Instance for instance\nsegmentation. Vec2Instance provides a framework for parametrization of\ninstances, allowing convolutional neural networks to efficiently estimate the\ncomplex shapes of instances around their centroids. We demonstrate the\nfeasibility of the proposed architecture with respect to instance segmentation\ntasks on satellite images, which have a wide range of applications. Moreover,\nwe demonstrate the usefulness of the new method for extracting building\nfoot-prints from satellite images. Total pixel-wise accuracy of our approach is\n89\\%, near the accuracy of the state-of-the-art Mask RCNN (91\\%). Vec2Instance\nis an alternative approach to complex instance segmentation pipelines, offering\nsimplicity and intuitiveness. The code developed under this study is available\nin the Vec2Instance GitHub repository, https://github.com/lakmalnd/Vec2Instance", "title": "Vec2Instance: Parameterization for Deep Instance Segmentation", "author": "N. Lakmal Deshapriya, Matthew N. Dailey, Manzul Kumar Hazarika, Hiroyuki Miyazaki", "published": "2020-10-06", "arxiv_url": "http://arxiv.org/abs/2010.02725v1", "arxiv_id": "2010.02725v1"}
{"abstraction": "Thanks to the excellent learning capability of deep convolutional neural\nnetworks (CNN), monocular depth estimation using CNNs has achieved great\nsuccess in recent years. However, depth estimation from a monocular image alone\nis essentially an ill-posed problem, and thus, it seems that this approach\nwould have inherent vulnerabilities. To reveal this limitation, we propose a\nmethod of adversarial patch attack on monocular depth estimation. More\nspecifically, we generate artificial patterns (adversarial patches) that can\nfool the target methods into estimating an incorrect depth for the regions\nwhere the patterns are placed. Our method can be implemented in the real world\nby physically placing the printed patterns in real scenes. We also analyze the\nbehavior of monocular depth estimation under attacks by visualizing the\nactivation levels of the intermediate layers and the regions potentially\naffected by the adversarial attack.", "title": "Adversarial Patch Attacks on Monocular Depth Estimation Networks", "author": "Koichiro Yamanaka, Ryutaroh Matsumoto, Keita Takahashi, Toshiaki Fujii", "published": "2020-10-06", "arxiv_url": "http://arxiv.org/abs/2010.03072v1", "arxiv_id": "2010.03072v1"}
{"abstraction": "This paper proposes to use Fast Fourier Transformation-based U-Net (a refined\nfully convolutional networks) and perform image convolution in neural networks.\nLeveraging the Fast Fourier Transformation, it reduces the image convolution\ncosts involved in the Convolutional Neural Networks (CNNs) and thus reduces the\noverall computational costs. The proposed model identifies the object\ninformation from the images. We apply the Fast Fourier transform algorithm on\nan image data set to obtain more accessible information about the image data,\nbefore segmenting them through the U-Net architecture. More specifically, we\nimplement the FFT-based convolutional neural network to improve the training\ntime of the network. The proposed approach was applied to publicly available\nBroad Bioimage Benchmark Collection (BBBC) dataset. Our model demonstrated\nimprovement in training time during convolution from $600-700$ ms/step to\n$400-500$ ms/step. We evaluated the accuracy of our model using Intersection\nover Union (IoU) metric showing significant improvements.", "title": "Fast Fourier Transformation for Optimizing Convolutional Neural Networks in Object Recognition", "author": "Varsha Nair, Moitrayee Chatterjee, Neda Tavakoli, Akbar Siami Namin, Craig Snoeyink", "published": "2020-10-08", "arxiv_url": "http://arxiv.org/abs/2010.04257v1", "arxiv_id": "2010.04257v1"}
{"abstraction": "Road network and building footprint extraction is essential for many\napplications such as updating maps, traffic regulations, city planning,\nride-hailing, disaster response \\textit{etc}. Mapping road networks is\ncurrently both expensive and labor-intensive. Recently, improvements in image\nsegmentation through the application of deep neural networks has shown\npromising results in extracting road segments from large scale, high resolution\nsatellite imagery. However, significant challenges remain due to lack of enough\nlabeled training data needed to build models for industry grade applications.\nIn this paper, we propose a two-stage transfer learning technique to improve\nrobustness of semantic segmentation for satellite images that leverages noisy\npseudo ground truth masks obtained automatically (without human labor) from\ncrowd-sourced OpenStreetMap (OSM) data. We further propose Pyramid\nPooling-LinkNet (PP-LinkNet), an improved deep neural network for segmentation\nthat uses focal loss, poly learning rate, and context module. We demonstrate\nthe strengths of our approach through evaluations done on three popular\ndatasets over two tasks, namely, road extraction and building foot-print\ndetection. Specifically, we obtain 78.19\\% meanIoU on SpaceNet building\nfootprint dataset, 67.03\\% and 77.11\\% on the road topology metric on SpaceNet\nand DeepGlobe road extraction dataset, respectively.", "title": "PP-LinkNet: Improving Semantic Segmentation of High Resolution Satellite Imagery with Multi-stage Training", "author": "An Tran, Ali Zonoozi, Jagannadan Varadarajan, Hannes Kruppa", "published": "2020-10-14", "arxiv_url": "http://arxiv.org/abs/2010.06932v1", "arxiv_id": "2010.06932v1"}
{"abstraction": "High-performance semiconductor optoelectronics such as perovskites have\nhigh-dimensional and vast composition spaces that govern the performance\nproperties of the material. To cost-effectively search these composition\nspaces, we utilize a high-throughput experimentation method of rapidly printing\ndiscrete droplets via inkjet deposition, in which each droplet is comprised of\na unique permutation of semiconductor materials. However, inkjet printer\nsystems are not optimized to run high-throughput experimentation on\nsemiconductor materials. Thus, in this work, we develop a computer\nvision-driven Bayesian optimization framework for optimizing the deposited\ndroplet structures from an inkjet printer such that it is tuned to perform\nhigh-throughput experimentation on semiconductor materials. The goal of this\nframework is to tune to the hardware conditions of the inkjet printer in the\nshortest amount of time using the fewest number of droplet samples such that we\nminimize the time and resources spent on setting the system up for material\ndiscovery applications. We demonstrate convergence on optimum inkjet hardware\nconditions in 10 minutes using Bayesian optimization of computer vision-scored\ndroplet structures. We compare our Bayesian optimization results with\nstochastic gradient descent.", "title": "Online Preconditioning of Experimental Inkjet Hardware by Bayesian Optimization in Loop", "author": "Alexander E. Siemenn, Matthew Beveridge, Tonio Buonassisi, Iddo Drori", "published": "2021-05-06", "arxiv_url": "http://arxiv.org/abs/2105.02858v1", "arxiv_id": "2105.02858v1"}
{"abstraction": "We introduce AiD Regen, a novel system that generates 3D wound models\ncombining 2D semantic segmentation with 3D reconstruction so that they can be\nprinted via 3D bio-printers during the surgery to treat diabetic foot ulcers\n(DFUs). AiD Regen seamlessly binds the full pipeline, which includes RGB-D\nimage capturing, semantic segmentation, boundary-guided point-cloud processing,\n3D model reconstruction, and 3D printable G-code generation, into a single\nsystem that can be used out of the box. We developed a multi-stage data\npreprocessing method to handle small and unbalanced DFU image datasets. AiD\nRegen's human-in-the-loop machine learning interface enables clinicians to not\nonly create 3D regenerative patches with just a few touch interactions but also\ncustomize and confirm wound boundaries. As evidenced by our experiments, our\nmodel outperforms prior wound segmentation models and our reconstruction\nalgorithm is capable of generating 3D wound models with compelling accuracy. We\nfurther conducted a case study on a real DFU patient and demonstrated the\neffectiveness of AiD Regen in treating DFU wounds.", "title": "Generating 3D Bio-Printable Patches Using Wound Segmentation and Reconstruction to Treat Diabetic Foot Ulcers", "author": "Han Joo Chae, Seunghwan Lee, Hyewon Son, Seungyeob Han, Taebin Lim", "published": "2022-03-08", "arxiv_url": "http://arxiv.org/abs/2203.03814v1", "arxiv_id": "2203.03814v1"}
{"abstraction": "X-ray coronary angiography (XCA) is used to assess coronary artery disease\nand provides valuable information on lesion morphology and severity. However,\nXCA images are 2D and therefore limit visualisation of the vessel. 3D\nreconstruction of coronary vessels is possible using multiple views, however\nlumen border detection in current software is performed manually resulting in\nlimited reproducibility and slow processing time. In this study we propose\n3DAngioNet, a novel deep learning (DL) system that enables rapid 3D vessel mesh\nreconstruction using 2D XCA images from two views. Our approach learns a coarse\nmesh template using an EfficientB3-UNet segmentation network and projection\ngeometries, and deforms it using a graph convolutional network. 3DAngioNet\noutperforms similar automated reconstruction methods, offers improved\nefficiency, and enables modelling of bifurcated vessels. The approach was\nvalidated using state-of-the-art software verified by skilled cardiologists.", "title": "3D Coronary Vessel Reconstruction from Bi-Plane Angiography using Graph Convolutional Networks", "author": "Kit Mills Bransby, Vincenzo Tufaro, Murat Cap, Greg Slabaugh, Christos Bourantas, Qianni Zhang", "published": "2023-02-28", "arxiv_url": "http://arxiv.org/abs/2302.14795v1", "arxiv_id": "2302.14795v1"}
{"abstraction": "We present a novel methodology that combines graph and dense segmentation\ntechniques by jointly learning both point and pixel contour representations,\nthereby leveraging the benefits of each approach. This addresses deficiencies\nin typical graph segmentation methods where misaligned objectives restrict the\nnetwork from learning discriminative vertex and contour features. Our joint\nlearning strategy allows for rich and diverse semantic features to be encoded,\nwhile alleviating common contour stability issues in dense-based approaches,\nwhere pixel-level objectives can lead to anatomically implausible topologies.\nIn addition, we identify scenarios where correct predictions that fall on the\ncontour boundary are penalised and address this with a novel hybrid contour\ndistance loss. Our approach is validated on several Chest X-ray datasets,\ndemonstrating clear improvements in segmentation stability and accuracy against\na variety of dense- and point-based methods. Our source code is freely\navailable at: www.github.com/kitbransby/Joint_Graph_Segmentation", "title": "Joint Dense-Point Representation for Contour-Aware Graph Segmentation", "author": "Kit Mills Bransby, Greg Slabaugh, Christos Bourantas, Qianni Zhang", "published": "2023-06-21", "arxiv_url": "http://arxiv.org/abs/2306.12155v1", "arxiv_id": "2306.12155v1"}
{"abstraction": "Time of Flight (ToF) is a prevalent depth sensing technology in the fields of\nrobotics, medical imaging, and non-destructive testing. Yet, ToF sensing faces\nchallenges from complex ambient conditions making an inverse modelling from the\nsparse temporal information intractable. This paper highlights the potential of\nmodern super-resolution techniques to learn varying surroundings for a reliable\nand accurate ToF detection. Unlike existing models, we tailor an architecture\nfor sub-sample precise semi-global signal localization by combining\nsuper-resolution with an efficient residual contraction block to balance\nbetween fine signal details and large scale contextual information. We\nconsolidate research on ToF by conducting a benchmark comparison against six\nstate-of-the-art methods for which we employ two publicly available datasets.\nThis includes the release of our SToF-Chirp dataset captured by an airborne\nultrasound transducer. Results showcase the superior performance of our\nproposed StofNet in terms of precision, reliability and model complexity. Our\ncode is available at https://github.com/hahnec/stofnet.", "title": "StofNet: Super-resolution Time of Flight Network", "author": "Christopher Hahne, Michel Hayoz, Raphael Sznitman", "published": "2023-08-23", "arxiv_url": "http://arxiv.org/abs/2308.12009v2", "arxiv_id": "2308.12009v2"}
{"abstraction": "Tuberculosis (TB) is caused by the bacterium Mycobacterium tuberculosis,\nprimarily affecting the lungs. Early detection is crucial for improving\ntreatment effectiveness and reducing transmission risk. Artificial intelligence\n(AI), particularly through image classification of chest X-rays, can assist in\nTB detection. However, class imbalance in TB chest X-ray datasets presents a\nchallenge for accurate classification. In this paper, we propose a few-shot\nlearning (FSL) approach using the Prototypical Network algorithm to address\nthis issue. We compare the performance of ResNet-18, ResNet-50, and VGG16 in\nfeature extraction from the TBX11K Chest X-ray dataset. Experimental results\ndemonstrate classification accuracies of 98.93% for ResNet-18, 98.60% for\nResNet-50, and 33.33% for VGG16. These findings indicate that the proposed\nmethod outperforms others in mitigating data imbalance, which is particularly\nbeneficial for disease classification applications.", "title": "Few-Shot Learning Approach on Tuberculosis Classification Based on Chest X-Ray Images", "author": "A. A. G. Yogi Pramana, Faiz Ihza Permana, Muhammad Fazil Maulana, Dzikri Rahadian Fudholi", "published": "2024-09-18", "arxiv_url": "http://arxiv.org/abs/2409.11644v1", "arxiv_id": "2409.11644v1"}
{"abstraction": "Morphing attacks have diversified significantly over the past years, with new\nmethods based on generative adversarial networks (GANs) and diffusion models\nposing substantial threats to face recognition systems. Recent research has\ndemonstrated the effectiveness of features extracted from large vision models\npretrained on bonafide data only (attack-agnostic features) for detecting deep\ngenerative images. Building on this, we investigate the potential of these\nimage representations for morphing attack detection (MAD). We develop\nsupervised detectors by training a simple binary linear SVM on the extracted\nfeatures and one-class detectors by modeling the distribution of bonafide\nfeatures with a Gaussian Mixture Model (GMM). Our method is evaluated across a\ncomprehensive set of attacks and various scenarios, including generalization to\nunseen attacks, different source datasets, and print-scan data. Our results\nindicate that attack-agnostic features can effectively detect morphing attacks,\noutperforming traditional supervised and one-class detectors from the\nliterature in most scenarios. Additionally, we provide insights into the\nstrengths and limitations of each considered representation and discuss\npotential future research directions to further enhance the robustness and\ngeneralizability of our approach.", "title": "Evaluating the Effectiveness of Attack-Agnostic Features for Morphing Attack Detection", "author": "Laurent Colbois, Sébastien Marcel", "published": "2024-10-22", "arxiv_url": "http://arxiv.org/abs/2410.16802v1", "arxiv_id": "2410.16802v1"}
{"abstraction": "The electrocardiogram (ECG) is an essential non-invasive diagnostic tool for\nassessing cardiac conditions. Existing automatic interpretation methods suffer\nfrom limited generalizability, focusing on a narrow range of cardiac\nconditions, and typically depend on raw physiological signals, which may not be\nreadily available in resource-limited settings where only printed or digital\nECG images are accessible. Recent advancements in multimodal large language\nmodels (MLLMs) present promising opportunities for addressing these challenges.\nHowever, the application of MLLMs to ECG image interpretation remains\nchallenging due to the lack of instruction tuning datasets and well-established\nECG image benchmarks for quantitative evaluation. To address these challenges,\nwe introduce ECGInstruct, a comprehensive ECG image instruction tuning dataset\nof over one million samples, covering a wide range of ECG-related tasks from\ndiverse data sources. Using ECGInstruct, we develop PULSE, an MLLM tailored for\nECG image comprehension. In addition, we curate ECGBench, a new evaluation\nbenchmark covering four key ECG image interpretation tasks across nine\ndifferent datasets. Our experiments show that PULSE sets a new\nstate-of-the-art, outperforming general MLLMs with an average accuracy\nimprovement of 15% to 30%. This work highlights the potential of PULSE to\nenhance ECG interpretation in clinical practice.", "title": "Teach Multimodal LLMs to Comprehend Electrocardiographic Images", "author": "Ruoqi Liu, Yuelin Bai, Xiang Yue, Ping Zhang", "published": "2024-10-21", "arxiv_url": "http://arxiv.org/abs/2410.19008v1", "arxiv_id": "2410.19008v1"}
{"abstraction": "Deep neural nets achieve state-of-the-art performance on the problem of\noptical flow estimation. Since optical flow is used in several safety-critical\napplications like self-driving cars, it is important to gain insights into the\nrobustness of those techniques. Recently, it has been shown that adversarial\nattacks easily fool deep neural networks to misclassify objects. The robustness\nof optical flow networks to adversarial attacks, however, has not been studied\nso far. In this paper, we extend adversarial patch attacks to optical flow\nnetworks and show that such attacks can compromise their performance. We show\nthat corrupting a small patch of less than 1% of the image size can\nsignificantly affect optical flow estimates. Our attacks lead to noisy flow\nestimates that extend significantly beyond the region of the attack, in many\ncases even completely erasing the motion of objects in the scene. While\nnetworks using an encoder-decoder architecture are very sensitive to these\nattacks, we found that networks using a spatial pyramid architecture are less\naffected. We analyse the success and failure of attacking both architectures by\nvisualizing their feature maps and comparing them to classical optical flow\ntechniques which are robust to these attacks. We also demonstrate that such\nattacks are practical by placing a printed pattern into real scenes.", "title": "Attacking Optical Flow", "author": "Anurag Ranjan, Joel Janai, Andreas Geiger, Michael J. Black", "published": "2019-10-22", "arxiv_url": "http://arxiv.org/abs/1910.10053v1", "arxiv_id": "1910.10053v1"}
{"abstraction": "Recovering a high-quality image from noisy indirect measurements is an\nimportant problem with many applications. For such inverse problems, supervised\ndeep convolutional neural network (CNN)-based denoising methods have shown\nstrong results, but the success of these supervised methods critically depends\non the availability of a high-quality training dataset of similar measurements.\nFor image denoising, methods are available that enable training without a\nseparate training dataset by assuming that the noise in two different pixels is\nuncorrelated. However, this assumption does not hold for inverse problems,\nresulting in artifacts in the denoised images produced by existing methods.\nHere, we propose Noise2Inverse, a deep CNN-based denoising method for linear\nimage reconstruction algorithms that does not require any additional clean or\nnoisy data. Training a CNN-based denoiser is enabled by exploiting the noise\nmodel to compute multiple statistically independent reconstructions. We develop\na theoretical framework which shows that such training indeed obtains a\ndenoising CNN, assuming the measured noise is element-wise independent and\nzero-mean. On simulated CT datasets, Noise2Inverse demonstrates an improvement\nin peak signal-to-noise ratio and structural similarity index compared to\nstate-of-the-art image denoising methods and conventional reconstruction\nmethods, such as Total-Variation Minimization. We also demonstrate that the\nmethod is able to significantly reduce noise in challenging real-world\nexperimental datasets.", "title": "Noise2Inverse: Self-supervised deep convolutional denoising for tomography", "author": "Allard A. Hendriksen, Daniel M. Pelt, K. Joost Batenburg", "published": "2020-01-31", "arxiv_url": "http://arxiv.org/abs/2001.11801v3", "arxiv_id": "2001.11801v3"}
{"abstraction": "Bronchiectasis is the permanent dilation of airways. Patients with the\ndisease can suffer recurrent exacerbations, reducing their quality of life. The\ngold standard to diagnose and monitor bronchiectasis is accomplished by\ninspection of chest computed tomography (CT) scans. A clinician examines the\nbroncho-arterial ratio to determine if an airway is brochiectatic. The visual\nanalysis assumes the blood vessel diameter remains constant, although this\nassumption is disputed in the literature. We propose a simple measurement of\ntapering along the airways to diagnose and monitor bronchiectasis. To this end,\nwe constructed a pipeline to measure the cross-sectional area along the airways\nat contiguous intervals, starting from the carina to the most distal point\nobservable. Using a phantom with calibrated 3D printed structures, the\nprecision and accuracy of our algorithm extends to the sub voxel level. The\ntapering measurement is robust to bifurcations along the airway and was applied\nto chest CT images acquired in clinical practice. The result is a statistical\ndifference in tapering rate between airways with bronchiectasis and controls.\nOur code is available at https://github.com/quan14/AirwayTaperingInCT.", "title": "Tapering Analysis of Airways with Bronchiectasis", "author": "Kin Quan, Rebecca J. Shipley, Ryutaro Tanno, Graeme McPhillips, Vasileios Vavourakis, David Edwards, Joseph Jacob, John R. Hurst, David J. Hawkes", "published": "2019-09-14", "arxiv_url": "http://arxiv.org/abs/1909.06604v1", "arxiv_id": "1909.06604v1"}
{"abstraction": "The black-box nature of the deep networks makes the explanation for \"why\"\nthey make certain predictions extremely challenging. Saliency maps are one of\nthe most widely-used local explanation tools to alleviate this problem. One of\nthe primary approaches for generating saliency maps is by optimizing a mask\nover the input dimensions so that the output of the network is influenced the\nmost by the masking. However, prior work only studies such influence by\nremoving evidence from the input. In this paper, we present iGOS++, a framework\nto generate saliency maps that are optimized for altering the output of the\nblack-box system by either removing or preserving only a small fraction of the\ninput. Additionally, we propose to add a bilateral total variation term to the\noptimization that improves the continuity of the saliency map especially under\nhigh resolution and with thin object parts. The evaluation results from\ncomparing iGOS++ against state-of-the-art saliency map methods show significant\nimprovement in locating salient regions that are directly interpretable by\nhumans. We utilized iGOS++ in the task of classifying COVID-19 cases from x-ray\nimages and discovered that sometimes the CNN network is overfitted to the\ncharacters printed on the x-ray images when performing classification. Fixing\nthis issue by data cleansing significantly improved the precision and recall of\nthe classifier.", "title": "iGOS++: Integrated Gradient Optimized Saliency by Bilateral Perturbations", "author": "Saeed Khorram, Tyler Lawson, Fuxin Li", "published": "2020-12-31", "arxiv_url": "http://arxiv.org/abs/2012.15783v2", "arxiv_id": "2012.15783v2"}
{"abstraction": "In this paper, we present a Computer Vision (CV) based tracking and fusion\nalgorithm, dedicated to a 3D printed gimbal system on drones operating in\nnature. The whole gimbal system can stabilize the camera orientation robustly\nin a challenging nature scenario by using skyline and ground plane as\nreferences. Our main contributions are the following: a) a light-weight\nResnet-18 backbone network model was trained from scratch, and deployed onto\nthe Jetson Nano platform to segment the image into binary parts (ground and\nsky); b) our geometry assumption from nature cues delivers the potential for\nrobust visual tracking by using the skyline and ground plane as a reference; c)\na spherical surface-based adaptive particle sampling, can fuse orientation from\nmultiple sensor sources flexibly. The whole algorithm pipeline is tested on our\ncustomized gimbal module including Jetson and other hardware components. The\nexperiments were performed on top of a building in the real landscape.", "title": "Adaptive Sampling-based Particle Filter for Visual-inertial Gimbal in the Wild", "author": "Xueyang Kang, Ariel Herrera, Henry Lema, Esteban Valencia, Patrick Vandewalle", "published": "2022-06-22", "arxiv_url": "http://arxiv.org/abs/2206.10981v5", "arxiv_id": "2206.10981v5"}
{"abstraction": "Terahertz (THz) sensing is a promising imaging technology for a wide variety\nof different applications. Extracting the interpretable and physically\nmeaningful parameters for such applications, however, requires solving an\ninverse problem in which a model function determined by these parameters needs\nto be fitted to the measured data. Since the underlying optimization problem is\nnonconvex and very costly to solve, we propose learning the prediction of\nsuitable parameters from the measured data directly. More precisely, we develop\na model-based autoencoder in which the encoder network predicts suitable\nparameters and the decoder is fixed to a physically meaningful model function,\nsuch that we can train the encoding network in an unsupervised way. We\nillustrate numerically that the resulting network is more than 140 times faster\nthan classical optimization techniques while making predictions with only\nslightly higher objective values. Using such predictions as starting points of\nlocal optimization techniques allows us to converge to better local minima\nabout twice as fast as optimization without the network-based initialization.", "title": "Training Auto-encoder-based Optimizers for Terahertz Image Reconstruction", "author": "Tak Ming Wong, Matthias Kahl, Peter Haring Bolívar, Andreas Kolb, Michael Möller", "published": "2019-07-02", "arxiv_url": "http://arxiv.org/abs/1907.01377v2", "arxiv_id": "1907.01377v2"}
{"abstraction": "Among the most impactful diabetic complications are diabetic retinopathy, the\nleading cause of blindness among working class adults, and cardiovascular\ndisease, the leading cause of death worldwide. This study describes the\ndevelopment of improved machine learning based screening of these conditions.\nFirst, a random forest model was developed by retrospectively analyzing the\ninfluence of various risk factors (obtained quickly and non-invasively) on\ncardiovascular risk. Next, a deep-learning model was developed for prediction\nof diabetic retinopathy from retinal fundus images by a modified and re-trained\nInceptionV3 image classification model. The input was simplified by\nautomatically segmenting the blood vessels in the retinal image. The technique\nof transfer learning enables the model to capitalize on existing infrastructure\non the target device, meaning more versatile deployment, especially helpful in\nlow-resource settings. The models were integrated into a smartphone-based\ndevice, combined with an inexpensive 3D-printed retinal imaging attachment.\nAccuracy scores, as well as the receiver operating characteristic curve, the\nlearning curve, and other gauges, were promising. This test is much cheaper and\nfaster, enabling continuous monitoring for two damaging complications of\ndiabetes. It has the potential to replace the manual methods of diagnosing both\ndiabetic retinopathy and cardiovascular risk, which are time consuming and\ncostly processes only done by medical professionals away from the point of\ncare, and to prevent irreversible blindness and heart-related complications\nthrough faster, cheaper, and safer monitoring of diabetic complications. As\nwell, tracking of cardiovascular and ocular complications of diabetes can\nenable improved detection of other diabetic complications, leading to earlier\nand more efficient treatment on a global scale.", "title": "Smartphone-Based Test and Predictive Models for Rapid, Non-Invasive, and Point-of-Care Monitoring of Ocular and Cardiovascular Complications Related to Diabetes", "author": "Kasyap Chakravadhanula", "published": "2020-10-25", "arxiv_url": "http://arxiv.org/abs/2011.08068v1", "arxiv_id": "2011.08068v1"}
{"abstraction": "Patient-specific 3D printing of congenital heart anatomy demands an accurate\nsegmentation of the thin tissue interfaces which characterise these diagnoses.\nEven when a label set has a high spatial overlap with the ground truth,\ninaccurate delineation of these interfaces can result in topological errors.\nThese compromise the clinical utility of such models due to the anomalous\nappearance of defects. CNNs have achieved state-of-the-art performance in\nsegmentation tasks. Whilst data augmentation has often played an important\nrole, we show that conventional image resampling schemes used therein can\nintroduce topological changes in the ground truth labelling of augmented\nsamples. We present a novel pipeline to correct for these changes, using a\nfast-marching algorithm to enforce the topology of the ground truth labels\nwithin their augmented representations. In so doing, we invoke the idea of\ncardiac contiguous topology to describe an arbitrary combination of congenital\nheart defects and develop an associated, clinically meaningful metric to\nmeasure the topological correctness of segmentations. In a series of five-fold\ncross-validations, we demonstrate the performance gain produced by this\npipeline and the relevance of topological considerations to the segmentation of\ncongenital heart defects. We speculate as to the applicability of this approach\nto any segmentation task involving morphologically complex targets.", "title": "Topology-preserving augmentation for CNN-based segmentation of congenital heart defects from 3D paediatric CMR", "author": "Nick Byrne, James R. Clough, Isra Valverde, Giovanni Montana, Andrew P. King", "published": "2019-08-23", "arxiv_url": "http://arxiv.org/abs/1908.08870v1", "arxiv_id": "1908.08870v1"}
{"abstraction": "Screening mammograms are a routine imaging exam performed to detect breast\ncancer in its early stages to reduce morbidity and mortality attributed to this\ndisease. In order to maximize the efficacy of breast cancer screening programs,\nproper mammographic positioning is paramount. Proper positioning ensures\nadequate visualization of breast tissue and is necessary for effective breast\ncancer detection. Therefore, breast-imaging radiologists must assess each\nmammogram for the adequacy of positioning before providing a final\ninterpretation of the examination; this often necessitates return patient\nvisits for additional imaging. In this paper, we propose a deep\nlearning-algorithm method that mimics and automates this decision-making\nprocess to identify poorly positioned mammograms. Our objective for this\nalgorithm is to assist mammography technologists in recognizing inadequately\npositioned mammograms real-time, improve the quality of mammographic\npositioning and performance, and ultimately reducing repeat visits for patients\nwith initially inadequate imaging. The proposed model showed a true positive\nrate for detecting correct positioning of 91.35% in the mediolateral oblique\nview and 95.11% in the craniocaudal view. In addition to these results, we also\npresent an automatically generated report which can aid the mammography\ntechnologist in taking corrective measures during the patient visit.", "title": "Deep Learning-Based Automatic Detection of Poorly Positioned Mammograms to Minimize Patient Return Visits for Repeat Imaging: A Real-World Application", "author": "Vikash Gupta, Clayton Taylor, Sarah Bonnet, Luciano M. Prevedello, Jeffrey Hawley, Richard D White, Mona G Flores, Barbaros Selnur Erdal", "published": "2020-09-28", "arxiv_url": "http://arxiv.org/abs/2009.13580v1", "arxiv_id": "2009.13580v1"}
{"abstraction": "This doctoral thesis covers some of my advances in electron microscopy with\ndeep learning. Highlights include a comprehensive review of deep learning in\nelectron microscopy; large new electron microscopy datasets for machine\nlearning, dataset search engines based on variational autoencoders, and\nautomatic data clustering by t-distributed stochastic neighbour embedding;\nadaptive learning rate clipping to stabilize learning; generative adversarial\nnetworks for compressed sensing with spiral, uniformly spaced and other fixed\nsparse scan paths; recurrent neural networks trained to piecewise adapt sparse\nscan paths to specimens by reinforcement learning; improving signal-to-noise;\nand conditional generative adversarial networks for exit wavefunction\nreconstruction from single transmission electron micrographs. This thesis adds\nto my publications by presenting their relationships, reflections, and holistic\nconclusions. This version of my thesis is typeset for online dissemination to\nimprove readability, whereas the thesis submitted to the University of Warwick\nin support of my application for the degree of Doctor of Philosophy in Physics\nis typeset for physical printing and binding.", "title": "Advances in Electron Microscopy with Deep Learning", "author": "Jeffrey M. Ede", "published": "2021-01-04", "arxiv_url": "http://arxiv.org/abs/2101.01178v5", "arxiv_id": "2101.01178v5"}
{"abstraction": "Over the past decade, machine learning methods have given us driverless cars,\nvoice recognition, effective web search, and a much better understanding of the\nhuman genome. Machine learning is so common today that it is used dozens of\ntimes a day, possibly unknowingly. Trying to teach a machine some processes or\nsome situations can make them predict some results that are difficult to\npredict by the human brain. These methods also help us do some operations that\nare often impossible or difficult to do with human activities in a short time.\nFor these reasons, machine learning is so important today. In this study, two\ndifferent machine learning methods were combined. In order to solve a\nreal-world problem, the manuscript documents were first transferred to the\ncomputer and then classified. We used three basic methods to realize the whole\nprocess. Handwriting or printed documents have been digitalized by a scanner or\ndigital camera. These documents have been processed with two different Optical\nCharacter Recognition (OCR) operation. After that generated texts are\nclassified by using Naive Bayes algorithm. All project was programmed in\nMicrosoft Visual Studio 12 platform on Windows operating system. C# programming\nlanguage was used for all parts of the study. Also, some prepared codes and\nDLLs were used.", "title": "Classification of Documents Extracted from Images with Optical Character Recognition Methods", "author": "Omer Aydin", "published": "2021-06-15", "arxiv_url": "http://arxiv.org/abs/2106.11125v1", "arxiv_id": "2106.11125v1"}
{"abstraction": "Medical images, especially volumetric images, are of high resolution and\noften exceed the capacity of standard desktop GPUs. As a result, most deep\nlearning-based medical image analysis tasks require the input images to be\ndownsampled, often substantially, before these can be fed to a neural network.\nHowever, downsampling can lead to a loss of image quality, which is undesirable\nespecially in reconstruction tasks, where the fine geometric details need to be\npreserved. In this paper, we propose that high-resolution images can be\nreconstructed in a coarse-to-fine fashion, where a deep learning algorithm is\nonly responsible for generating a coarse representation of the image, which\nconsumes moderate GPU memory. For producing the high-resolution outcome, we\npropose two novel methods: learned voxel rearrangement of the coarse output and\nhierarchical image synthesis. Compared to the coarse output, the\nhigh-resolution counterpart allows for smooth surface triangulation, which can\nbe 3D-printed in the highest possible quality. Experiments of this paper are\ncarried out on the dataset of AutoImplant 2021\n(https://autoimplant2021.grand-challenge.org/), a MICCAI challenge on cranial\nimplant design. The dataset contains high-resolution skulls that can be viewed\nas 2D manifolds embedded in a 3D space. Codes associated with this study can be\naccessed at https://github.com/Jianningli/voxel_rearrangement.", "title": "Learning to Rearrange Voxels in Binary Segmentation Masks for Smooth Manifold Triangulation", "author": "Jianning Li, Antonio Pepe, Christina Gsaxner, Yuan Jin, Jan Egger", "published": "2021-08-11", "arxiv_url": "http://arxiv.org/abs/2108.05269v1", "arxiv_id": "2108.05269v1"}
{"abstraction": "This paper deals with the highly challenging problem of reconstructing the\nshape of a refracting object from a single image of its resulting caustic. Due\nto the ubiquity of transparent refracting objects in everyday life,\nreconstruction of their shape entails a multitude of practical applications.\nThe recent Shape from Caustics (SfC) method casts the problem as the inverse of\na light propagation simulation for synthesis of the caustic image, that can be\nsolved by a differentiable renderer. However, the inherent complexity of light\ntransport through refracting surfaces currently limits the practicability with\nrespect to reconstruction speed and robustness. To address these issues, we\nintroduce Neural-Shape from Caustics (N-SfC), a learning-based extension that\nincorporates two components into the reconstruction pipeline: a denoising\nmodule, which alleviates the computational cost of the light transport\nsimulation, and an optimization process based on learned gradient descent,\nwhich enables better convergence using fewer iterations. Extensive experiments\ndemonstrate the effectiveness of our neural extensions in the scenario of\nquality control in 3D glass printing, where we significantly outperform the\ncurrent state-of-the-art in terms of computational speed and final surface\nerror.", "title": "N-SfC: Robust and Fast Shape Estimation from Caustic Images", "author": "Marc Kassubeck, Moritz Kappel, Susana Castillo, Marcus Magnor", "published": "2021-12-13", "arxiv_url": "http://arxiv.org/abs/2112.06705v1", "arxiv_id": "2112.06705v1"}
{"abstraction": "2D face recognition has been proven insecure for physical adversarial\nattacks. However, few studies have investigated the possibility of attacking\nreal-world 3D face recognition systems. 3D-printed attacks recently proposed\ncannot generate adversarial points in the air. In this paper, we attack 3D face\nrecognition systems through elaborate optical noises. We took structured light\n3D scanners as our attack target. End-to-end attack algorithms are designed to\ngenerate adversarial illumination for 3D faces through the inherent or an\nadditional projector to produce adversarial points at arbitrary positions.\nNevertheless, face reflectance is a complex procedure because the skin is\ntranslucent. To involve this projection-and-capture procedure in optimization\nloops, we model it by Lambertian rendering model and use SfSNet to estimate the\nalbedo. Moreover, to improve the resistance to distance and angle changes while\nmaintaining the perturbation unnoticeable, a 3D transform invariant loss and\ntwo kinds of sensitivity maps are introduced. Experiments are conducted in both\nsimulated and physical worlds. We successfully attacked point-cloud-based and\ndepth-image-based 3D face recognition algorithms while needing fewer\nperturbations than previous state-of-the-art physical-world 3D adversarial\nattacks.", "title": "Physical-World Optical Adversarial Attacks on 3D Face Recognition", "author": "Yanjie Li, Yiquan Li, Xuelong Dai, Songtao Guo, Bin Xiao", "published": "2022-05-26", "arxiv_url": "http://arxiv.org/abs/2205.13412v3", "arxiv_id": "2205.13412v3"}
{"abstraction": "Multi-modality Fluorodeoxyglucose (FDG) positron emission tomography /\ncomputed tomography (PET/CT) has been routinely used in the assessment of\ncommon cancers, such as lung cancer, lymphoma, and melanoma. This is mainly\nattributed to the fact that PET/CT combines the high sensitivity for tumor\ndetection of PET and anatomical information from CT. In PET/CT image\nassessment, automatic tumor segmentation is an important step, and in recent\nyears, deep learning based methods have become the state-of-the-art.\nUnfortunately, existing methods tend to over-segment the tumor regions and\ninclude regions such as the normal high uptake organs, inflammation, and other\ninfections. In this study, we introduce a false positive reduction network to\novercome this limitation. We firstly introduced a self-supervised pre-trained\nglobal segmentation module to coarsely delineate the candidate tumor regions\nusing a self-supervised pre-trained encoder. The candidate tumor regions were\nthen refined by removing false positives via a local refinement module. Our\nexperiments with the MICCAI 2022 Automated Lesion Segmentation in Whole-Body\nFDG-PET/CT (AutoPET) challenge dataset showed that our method achieved a dice\nscore of 0.9324 with the preliminary testing data and was ranked 1st place in\ndice on the leaderboard. Our method was also ranked in the top 7 methods on the\nfinal testing data, the final ranking will be announced during the 2022 MICCAI\nAutoPET workshop. Our code is available at:\nhttps://github.com/YigePeng/AutoPET_False_Positive_Reduction.", "title": "Automatic Tumor Segmentation via False Positive Reduction Network for Whole-Body Multi-Modal PET/CT Images", "author": "Yige Peng, Jinman Kim, Dagan Feng, Lei Bi", "published": "2022-09-16", "arxiv_url": "http://arxiv.org/abs/2209.07705v1", "arxiv_id": "2209.07705v1"}
{"abstraction": "Recently, the first foundation model developed specifically for image\nsegmentation tasks was developed, termed the \"Segment Anything Model\" (SAM).\nSAM can segment objects in input imagery based on cheap input prompts, such as\none (or more) points, a bounding box, or a mask. The authors examined the\n\\textit{zero-shot} image segmentation accuracy of SAM on a large number of\nvision benchmark tasks and found that SAM usually achieved recognition accuracy\nsimilar to, or sometimes exceeding, vision models that had been trained on the\ntarget tasks. The impressive generalization of SAM for segmentation has major\nimplications for vision researchers working on natural imagery. In this work,\nwe examine whether SAM's performance extends to overhead imagery problems and\nhelp guide the community's response to its development. We examine SAM's\nperformance on a set of diverse and widely studied benchmark tasks. We find\nthat SAM does often generalize well to overhead imagery, although it fails in\nsome cases due to the unique characteristics of overhead imagery and its common\ntarget objects. We report on these unique systematic failure cases for remote\nsensing imagery that may comprise useful future research for the community.", "title": "Segment anything, from space?", "author": "Simiao Ren, Francesco Luzi, Saad Lahrichi, Kaleb Kassaw, Leslie M. Collins, Kyle Bradbury, Jordan M. Malof", "published": "2023-04-25", "arxiv_url": "http://arxiv.org/abs/2304.13000v4", "arxiv_id": "2304.13000v4"}
{"abstraction": "The stochastic formation of defects during Laser Powder Bed Fusion (L-PBF)\nnegatively impacts its adoption for high-precision use cases. Optical\nmonitoring techniques can be used to identify defects based on layer-wise\nimaging, but these methods are difficult to scale to high resolutions due to\ncost and memory constraints. Therefore, we implement generative deep learning\nmodels to link low-cost, low-resolution images of the build plate to detailed\nhigh-resolution optical images of the build plate, enabling cost-efficient\nprocess monitoring. To do so, a conditional latent probabilistic diffusion\nmodel is trained to produce realistic high-resolution images of the build plate\nfrom low-resolution webcam images, recovering the distribution of small-scale\nfeatures and surface roughness. We first evaluate the performance of the model\nby analyzing the reconstruction quality of the generated images using\npeak-signal-to-noise-ratio (PSNR), structural similarity index measure (SSIM)\nand wavelet covariance metrics that describe the preservation of high-frequency\ninformation. Additionally, we design a framework based upon the Segment\nAnything foundation model to recreate the 3D morphology of the printed part and\nanalyze the surface roughness of the reconstructed samples. Finally, we explore\nthe zero-shot generalization capabilities of the implemented framework to other\npart geometries by creating synthetic low-resolution data.", "title": "Deep Learning based Optical Image Super-Resolution via Generative Diffusion Models for Layerwise in-situ LPBF Monitoring", "author": "Francis Ogoke, Sumesh Kalambettu Suresh, Jesse Adamczyk, Dan Bolintineanu, Anthony Garland, Michael Heiden, Amir Barati Farimani", "published": "2024-09-20", "arxiv_url": "http://arxiv.org/abs/2409.13171v1", "arxiv_id": "2409.13171v1"}
{"abstraction": "This study explores the potential of the Ping 360 sonar device, primarily\nused for navigation, in detecting complex underwater obstacles. The key\nmotivation behind this research is the device's affordability and open-source\nnature, offering a cost-effective alternative to more expensive imaging sonar\nsystems. The investigation focuses on understanding the behaviour of the Ping\n360 in controlled environments and assessing its suitability for object\ndetection, particularly in scenarios where human operators are unavailable for\ninspecting offshore structures in shallow waters. Through a series of carefully\ndesigned experiments, we examined the effects of surface reflections and object\nshadows in shallow underwater environments. Additionally, we developed a\nmanually annotated sonar image dataset to train a U-Net segmentation model. Our\nfindings indicate that while the Ping 360 sonar demonstrates potential in\nsimpler settings, its performance is limited in more cluttered or reflective\nenvironments unless extensive data pre-processing and annotation are applied.\nTo our knowledge, this is the first study to evaluate the Ping 360's\ncapabilities for complex object detection. By investigating the feasibility of\nlow-cost sonar devices, this research provides valuable insights into their\nlimitations and potential for future AI-based interpretation, marking a unique\ncontribution to the field.", "title": "Exploring the Feasibility of Affordable Sonar Technology: Object Detection in Underwater Environments Using the Ping 360", "author": "Md Junayed Hasan, Somasundar Kannan, Ali Rohan, Mohd Asif Shah", "published": "2024-11-07", "arxiv_url": "http://arxiv.org/abs/2411.05863v1", "arxiv_id": "2411.05863v1"}
{"abstraction": "We report on an educational pilot program for low-cost physics\nexperimentation run in Ecuador, South Africa, and the United States. The\nprogram was developed after having needs-based discussions with African\neducators, researchers, and leaders. It was determined that the need and desire\nfor low-cost, skills-building, and active-learning tools is very high. From\nthis, we developed a 3D-printable, Raspberry Pi-based multispectral camera (15\nto 25 spectral channels in the visible and near-IR) for as little as $100. The\nprogram allows students to learn 3D modeling, 3D printing, feedback, control,\nimage analysis, Python programming, systems integration and artificial\nintelligence as well as spectroscopy. After completing their cameras, the\nstudents in the program studied plant health, plant stress, post-harvest fruit\nripeness, and polarization and spectral analysis of nanostructured insect\nwings, the latter of which won the ``best-applied research\" award at a\nconference poster session and will be highlighted in this paper. Importantly,\nthese cameras can be an integral part of any developing country's agricultural,\nrecycling, medical, and pharmaceutical infrastructure. Thus, we believe this\nexperiment can play an important role at the intersection of student training\nand developing countries' capacity building.", "title": "Raspberry Pi multispectral imaging camera system (PiMICS): a low-cost, skills-based physics educational tool", "author": "John C. Howell, Brian Flores, Juan Javier Naranjo, Angel Mendez, Cesar Costa-Vera, Chris Koumriqian, Juliana Jordan, Pieter H. Neethling, Calvin Groenewald, Michael A. C. Lovemore, Patrick A. T. Kinsey, Tjaart P. J. Kruger", "published": "2024-12-06", "arxiv_url": "http://arxiv.org/abs/2412.04679v1", "arxiv_id": "2412.04679v1"}
{"abstraction": "Melt pool (MP) temperature is one of the determining factors and key\nsignatures for the properties of printed components during metal additive\nmanufacturing (AM). The state-of-the art measurement systems are hindered by\nboth the equipment cost and the large-scale data acquisition and processing\ndemands. In this work, we introduce a novel coaxial high-speed single-camera\ntwo-wavelength imaging pyrometer (STWIP) system as opposed to the typical\nutilization of multiple cameras for measuring MP temperature profiles through a\nlaser powder bed fusion (LPBF) process. Developed on a commercial LPBF machine\n(EOS M290), the STWIP system is demonstrated to be able to quantitatively\nmonitor MP temperature and variation for 50 layers at high framerates (> 30,000\nfps) during a print of five standard fatigue specimens. High performance\ncomputing is employed to analyze the acquired big data of MP images for\ndetermining each MPs average temperature and 2D temperature profile. The MP\ntemperature evolution in the gage section of a fatigue specimen is also\nexamined at a temporal resolution of 1ms by evaluating the derived MP\ntemperatures of the printed samples first, middle and last layers. This paper\nis first of its kind on monitoring MP temperature distribution and evolution at\nsuch a large, detailed scale for longer durations in practical applications.\nFuture work includes MP registration and machine learning of MP-Part Property\nrelations.", "title": "Single-camera Two-Wavelength Imaging Pyrometry for Melt Pool Temperature Measurement and Monitoring in Laser Powder Bed Fusion based Additive Manufacturing", "author": "Chaitanya Krishna Prasad Vallabh, Xiayun Zhao", "published": "2021-09-14", "arxiv_url": "http://arxiv.org/abs/2109.07472v1", "arxiv_id": "2109.07472v1"}
{"abstraction": "Human Identity verification has always been an eye-catching goal in digital\nbased security system. Authentication or identification systems developed using\nhuman characteristics such as face, finger print, hand geometry, iris, and\nvoice are denoted as biometric systems. Among the various characteristics, Iris\nrecognition trusts on the idiosyncratic human iris patterns to find out and\ncorroborate the identity of a person. The image is normally contemplated as a\ngathering of information. Existence of noises in the input or processed image\neffects degradation in the image superiority. It should be paramount to restore\noriginal image from noises for attaining maximum amount of information from\ncorrupted images. Noisy images in biometric identification system cannot give\naccurate identity. So Image related data or information tends to loss or\ndamage. Images are affected by various sorts of noises. This paper mainly\nfocuses on Salt and Pepper noise, Gaussian noise, Uniform noise, Speckle noise.\nDifferent filtering techniques can be adapted for noise diminution to develop\nthe visual quality as well as understandability of images. In this paper, four\ntypes of noises have been undertaken and applied on some images. The filtering\nof these noises uses different types of filters like Mean, Median, Weiner,\nGaussian filter etc. A relative interpretation is performed using four\ndifferent categories of filter with finding the value of quality determined\nparameters like mean square error (MSE), peak signal to noise ratio (PSNR),\naverage difference value (AD) and maximum difference value (MD).", "title": "Ramifications and Diminution of Image Noise in Iris Recognition System", "author": "Prajoy Podder, A. H. M Shahariar Parvez, Md. Mizanur Rahman, Tanvir Zaman Khan", "published": "2020-02-08", "arxiv_url": "http://arxiv.org/abs/2002.03125v1", "arxiv_id": "2002.03125v1"}
{"abstraction": "Learning robust representations to discriminate cell phenotypes based on\nmicroscopy images is important for drug discovery. Drug development efforts\ntypically analyse thousands of cell images to screen for potential treatments.\nEarly works focus on creating hand-engineered features from these images or\nlearn such features with deep neural networks in a fully or weakly-supervised\nframework. Both require prior knowledge or labelled datasets. Therefore,\nsubsequent works propose unsupervised approaches based on generative models to\nlearn these representations. Recently, representations learned with\nself-supervised contrastive loss-based methods have yielded state-of-the-art\nresults on various imaging tasks compared to earlier unsupervised approaches.\nIn this work, we leverage a contrastive learning framework to learn appropriate\nrepresentations from single-cell fluorescent microscopy images for the task of\nMechanism-of-Action classification. The proposed work is evaluated on the\nannotated BBBC021 dataset, and we obtain state-of-the-art results in NSC, NCSB\nand drop metrics for an unsupervised approach. We observe an improvement of 10%\nin NCSB accuracy and 11% in NSC-NSCB drop over the previously best unsupervised\nmethod. Moreover, the performance of our unsupervised approach ties with the\nbest supervised approach. Additionally, we observe that our framework performs\nwell even without post-processing, unlike earlier methods. With this, we\nconclude that one can learn robust cell representations with contrastive\nlearning.", "title": "Contrastive Learning of Single-Cell Phenotypic Representations for Treatment Classification", "author": "Alexis Perakis, Ali Gorji, Samriddhi Jain, Krishna Chaitanya, Simone Rizza, Ender Konukoglu", "published": "2021-03-30", "arxiv_url": "http://arxiv.org/abs/2103.16670v1", "arxiv_id": "2103.16670v1"}
{"abstraction": "Imaging of biological cells and tissues often relies on fluorescent labels,\nwhich offer high contrast with molecular specificity. The use of exogenous\nlabeling agents, however, may alter the normal physiology of the bio-specimens.\nComplementary to the established fluorescence microscopy, label-free\nquantitative phase imaging provides an objective morphological measurement tool\nfor bio-specimens and is free of variability introduced by contrast agents.\nHere we report a simple and low-cost microscope add-on, termed Ptychographic\nModulation Engine (PME), for super-resolution quantitative phase imaging. In\nthis microscope add-on module, we attach a diffuser to a 3D-printed holder that\ncan be mechanically moved to different x-y positions. We then use two\nvibrational motors to introduce random positional shifts to the diffuser. The\nadd-on module can be placed between the objective lens and the specimen in most\nexisting microscope platforms. Thanks to the diffuser modulation process, the\notherwise inaccessible high-resolution object information can now be encoded\ninto the captured images. In the ptychographic phase retrieval process, we\njointly recover the complex object wavefront, the complex diffuser profile, and\nthe unknown positional shifts of the diffuser. We demonstrate a 4-fold\nresolution gain over the diffraction limit of the employed 2X objective lens.\nWe also test our approach for in-vivo cell imaging, where we are able to adjust\nthe focus after the data has been captured. The reported microscope add-on\nprovides a turnkey solution for super-resolution quantitative phase imaging. It\nmay find applications in label-free bio-imaging where both large field-of-view\nand high resolution are needed.", "title": "Ptychographic modulation engine (PME): a low-cost DIY microscope add-on for coherent super-resolution imaging", "author": "Zichao Bian, Shaowei Jiang, Pengming Song, He Zhang, Pouria Hoveida, Kazunori Hoshino, Guoan Zheng", "published": "2019-08-13", "arxiv_url": "http://arxiv.org/abs/1908.05761v2", "arxiv_id": "1908.05761v2"}
{"abstraction": "The goal of this work is to characterize the representational impact that\nfoveation operations have for machine vision systems, inspired by the foveated\nhuman visual system, which has higher acuity at the center of gaze and\ntexture-like encoding in the periphery. To do so, we introduce models\nconsisting of a first-stage \\textit{fixed} image transform followed by a\nsecond-stage \\textit{learnable} convolutional neural network, and we varied the\nfirst stage component. The primary model has a foveated-textural input stage,\nwhich we compare to a model with foveated-blurred input and a model with\nspatially-uniform blurred input (both matched for perceptual compression), and\na final reference model with minimal input-based compression. We find that: 1)\nthe foveated-texture model shows similar scene classification accuracy as the\nreference model despite its compressed input, with greater i.i.d.\ngeneralization than the other models; 2) the foveated-texture model has greater\nsensitivity to high-spatial frequency information and greater robustness to\nocclusion, w.r.t the comparison models; 3) both the foveated systems, show a\nstronger center image-bias relative to the spatially-uniform systems even with\na weight sharing constraint. Critically, these results are preserved over\ndifferent classical CNN architectures throughout their learning dynamics.\nAltogether, this suggests that foveation with peripheral texture-based\ncomputations yields an efficient, distinct, and robust representational format\nof scene information, and provides symbiotic computational insight into the\nrepresentational consequences that texture-based peripheral encoding may have\nfor processing in the human visual system, while also potentially inspiring the\nnext generation of computer vision models via spatially-adaptive computation.\nCode + Data available here: https://github.com/ArturoDeza/EmergentProperties", "title": "Emergent Properties of Foveated Perceptual Systems", "author": "Arturo Deza, Talia Konkle", "published": "2020-06-14", "arxiv_url": "http://arxiv.org/abs/2006.07991v3", "arxiv_id": "2006.07991v3"}
{"abstraction": "The main success stories of deep learning, starting with ImageNet, depend on\ndeep convolutional networks, which on certain tasks perform significantly\nbetter than traditional shallow classifiers, such as support vector machines,\nand also better than deep fully connected networks; but what is so special\nabout deep convolutional networks? Recent results in approximation theory\nproved an exponential advantage of deep convolutional networks with or without\nshared weights in approximating functions with hierarchical locality in their\ncompositional structure. More recently, the hierarchical structure was proved\nto be hard to learn from data, suggesting that it is a powerful prior embedded\nin the architecture of the network. These mathematical results, however, do not\nsay which real-life tasks correspond to input-output functions with\nhierarchical locality. To evaluate this, we consider a set of visual tasks\nwhere we disrupt the local organization of images via \"deterministic\nscrambling\" to later perform a visual task on these images structurally-altered\nin the same way for training and testing. For object recognition we find, as\nexpected, that scrambling does not affect the performance of shallow or deep\nfully connected networks contrary to the out-performance of convolutional\nnetworks. Not all tasks involving images are however affected. Texture\nperception and global color estimation are much less sensitive to deterministic\nscrambling showing that the underlying functions corresponding to these tasks\nare not hierarchically local; and also counter-intuitively showing that these\ntasks are better approximated by networks that are not deep (texture) nor\nconvolutional (color). Altogether, these results shed light into the importance\nof matching a network architecture with its embedded prior of the task to be\nlearned.", "title": "Hierarchically Compositional Tasks and Deep Convolutional Networks", "author": "Arturo Deza, Qianli Liao, Andrzej Banburski, Tomaso Poggio", "published": "2020-06-24", "arxiv_url": "http://arxiv.org/abs/2006.13915v3", "arxiv_id": "2006.13915v3"}
{"abstraction": "Most studies in computational modeling of visual attention encompass\ntask-free observation of images. Free-viewing saliency considers limited\nscenarios of daily life. Most visual activities are goal-oriented and demand a\ngreat amount of top-down attention control. Visual search task demands more\ntop-down control of attention, compared to free-viewing. In this paper, we\npresent two approaches to model visual attention and distraction of observers\nduring visual search. Our first approach adapts a light-weight free-viewing\nsaliency model to predict eye fixation density maps of human observers over\npixels of search images, using a two-stream convolutional encoder-decoder\nnetwork, trained and evaluated on COCO-Search18 dataset. This method predicts\nwhich locations are more distracting when searching for a particular target.\nOur network achieves good results on standard saliency metrics (AUC-Judd=0.95,\nAUC-Borji=0.85, sAUC=0.84, NSS=4.64, KLD=0.93, CC=0.72, SIM=0.54, and IG=2.59).\nOur second approach is object-based and predicts the distractor and target\nobjects during visual search. Distractors are all objects except the target\nthat observers fixate on during search. This method uses a Mask-RCNN\nsegmentation network pre-trained on MS-COCO and fine-tuned on COCO-Search18\ndataset. We release our segmentation annotations of targets and distractors in\nCOCO-Search18 for three target categories: bottle, bowl, and car. The average\nscores over the three categories are: F1-score=0.64, MAP(iou:0.5)=0.57,\nMAR(iou:0.5)=0.73. Our implementation code in Tensorflow is publicly available\nat https://github.com/ManooshSamiei/Distraction-Visual-Search .", "title": "Predicting Visual Attention and Distraction During Visual Search Using Convolutional Neural Networks", "author": "Manoosh Samiei, James J. Clark", "published": "2022-10-27", "arxiv_url": "http://arxiv.org/abs/2210.15093v1", "arxiv_id": "2210.15093v1"}
{"abstraction": "Background: X-ray imaging is widely used for the non-destructive detection of\ndefects in industrial products on a conveyor belt. In-line detection requires\nhighly accurate, robust, and fast algorithms. Deep Convolutional Neural\nNetworks (DCNNs) satisfy these requirements when a large amount of labeled data\nis available. To overcome the challenge of collecting these data, different\nmethods of X-ray image generation are considered.\n  Objective: Depending on the desired degree of similarity to real data,\ndifferent physical effects should either be simulated or can be ignored. X-ray\nscattering is known to be computationally expensive to simulate, and this\neffect can greatly affect the accuracy of a generated X-ray image. We aim to\nquantitatively evaluate the effect of scattering on defect detection.\n  Methods: Monte-Carlo simulation is used to generate X-ray scattering\ndistribution. DCNNs are trained on the data with and without scattering and\napplied to the same test datasets. Probability of Detection (POD) curves are\ncomputed to compare their performance, characterized by the size of the\nsmallest detectable defect.\n  Results: We apply the methodology to a model problem of defect detection in\ncylinders. When trained on data without scattering, DCNNs reliably detect\ndefects larger than 1.3 mm, and using data with scattering improves performance\nby less than 5%. If the analysis is performed on the cases with large\nscattering-to-primary ratio ($1 < SPR < 5$), the difference in performance\ncould reach 15% (approx. 0.4 mm).\n  Conclusion: Excluding the scattering signal from the training data has the\nlargest effect on the smallest detectable defects, and the difference decreases\nfor larger defects. The scattering-to-primary ratio has a significant effect on\ndetection performance and the required accuracy of data generation.", "title": "Quantifying the effect of X-ray scattering for data generation in real-time defect detection", "author": "Vladyslav Andriiashen, Robert van Liere, Tristan van Leeuwen, K. Joost Batenburg", "published": "2023-05-22", "arxiv_url": "http://arxiv.org/abs/2305.12822v2", "arxiv_id": "2305.12822v2"}
{"abstraction": "The increasing availability of biomedical data is helping to design more\nrobust deep learning (DL) algorithms to analyze biomedical samples. Currently,\none of the main limitations to train DL algorithms to perform a specific task\nis the need for medical experts to label data. Automatic methods to label data\nexist, however automatic labels can be noisy and it is not completely clear\nwhen automatic labels can be adopted to train DL models. This paper aims to\ninvestigate under which circumstances automatic labels can be adopted to train\na DL model on the classification of Whole Slide Images (WSI). The analysis\ninvolves multiple architectures, such as Convolutional Neural Networks (CNN)\nand Vision Transformer (ViT), and over 10000 WSIs, collected from three use\ncases: celiac disease, lung cancer and colon cancer, which one including\nrespectively binary, multiclass and multilabel data. The results allow\nidentifying 10% as the percentage of noisy labels that lead to train\ncompetitive models for the classification of WSIs. Therefore, an algorithm\ngenerating automatic labels needs to fit this criterion to be adopted. The\napplication of the Semantic Knowledge Extractor Tool (SKET) algorithm to\ngenerate automatic labels leads to performance comparable to the one obtained\nwith manual labels, since it generates a percentage of noisy labels between\n2-5%. Automatic labels are as effective as manual ones, reaching solid\nperformance comparable to the one obtained training models with manual labels.", "title": "Automatic Labels are as Effective as Manual Labels in Biomedical Images Classification with Deep Learning", "author": "Niccolò Marini, Stefano Marchesin, Lluis Borras Ferris, Simon Püttmann, Marek Wodzinski, Riccardo Fratti, Damian Podareanu, Alessandro Caputo, Svetla Boytcheva, Simona Vatrano, Filippo Fraggetta, Iris Nagtegaal, Gianmaria Silvello, Manfredo Atzori, Henning Müller", "published": "2024-06-20", "arxiv_url": "http://arxiv.org/abs/2406.14351v1", "arxiv_id": "2406.14351v1"}
{"abstraction": "Despite renewed awareness of the importance of articulation, it remains a\nchallenge for instructors to handle the pronunciation needs of language\nlearners. There are relatively scarce pedagogical tools for pronunciation\nteaching and learning. Unlike inefficient, traditional pronunciation\ninstructions like listening and repeating, electronic visual feedback (EVF)\nsystems such as ultrasound technology have been employed in new approaches.\nRecently, an ultrasound-enhanced multimodal method has been developed for\nvisualizing tongue movements of a language learner overlaid on the face-side of\nthe speaker's head. That system was evaluated for several language courses via\na blended learning paradigm at the university level. The result was asserted\nthat visualizing the articulator's system as biofeedback to language learners\nwill significantly improve articulation learning efficiency. In spite of the\nsuccessful usage of multimodal techniques for pronunciation training, it still\nrequires manual works and human manipulation. In this article, we aim to\ncontribute to this growing body of research by addressing difficulties of the\nprevious approaches by proposing a new comprehensive, automatic, real-time\nmultimodal pronunciation training system, benefits from powerful artificial\nintelligence techniques. The main objective of this research was to combine the\nadvantages of ultrasound technology, three-dimensional printing, and deep\nlearning algorithms to enhance the performance of previous systems. Our\npreliminary pedagogical evaluation of the proposed system revealed a\nsignificant improvement in flexibility, control, robustness, and autonomy.", "title": "Real-time Ultrasound-enhanced Multimodal Imaging of Tongue using 3D Printable Stabilizer System: A Deep Learning Approach", "author": "M. Hamed Mozaffari, Won-Sook Lee", "published": "2019-11-22", "arxiv_url": "http://arxiv.org/abs/1911.09840v1", "arxiv_id": "1911.09840v1"}
{"abstraction": "Additive Manufacturing (AM) is a crucial component of the smart industry. In\nthis paper, we propose an automated quality grading system for the AM process\nusing a deep convolutional neural network (CNN) model. The CNN model is trained\noffline using the images of the internal and surface defects in the\nlayer-by-layer deposition of materials and tested online by studying the\nperformance of detecting and classifying the failure in AM process at different\nextruder speeds and temperatures. The model demonstrates the accuracy of 94%\nand specificity of 96%, as well as above 75% in three classifier measures of\nthe Fscore, the sensitivity, and precision for classifying the quality of the\nprinting process in five grades in real-time. The proposed online model adds an\nautomated, consistent, and non-contact quality control signal to the AM process\nthat eliminates the manual inspection of parts after they are entirely built.\nThe quality monitoring signal can also be used by the machine to suggest\nremedial actions by adjusting the parameters in real-time. The proposed quality\npredictive model serves as a proof-of-concept for any type of AM machines to\nproduce reliable parts with fewer quality hiccups while limiting the waste of\nboth time and materials.", "title": "Toward Enabling a Reliable Quality Monitoring System for Additive Manufacturing Process using Deep Convolutional Neural Networks", "author": "Yaser Banadaki, Nariman Razaviarab, Hadi Fekrmandi, Safura Sharifi", "published": "2020-03-06", "arxiv_url": "http://arxiv.org/abs/2003.08749v1", "arxiv_id": "2003.08749v1"}
{"abstraction": "A multispectral camera records image data in various wavelengths across the\nelectromagnetic spectrum to acquire additional information that a conventional\ncamera fails to capture. With the advent of high-resolution image sensors and\ncolour filter technologies, multispectral imagers in the visible wavelengths\nhave become popular with increasing commercial viability in the last decade.\nHowever, multispectral imaging in longwave infrared (LWIR: 8 to 14 microns) is\nstill an emerging area due to the limited availability of optical materials,\nfilter technologies, and high-resolution sensors. Images from LWIR\nmultispectral cameras can capture emission spectra of objects to extract\nadditional information that a human eye fails to capture and thus have\nimportant applications in precision agriculture, forestry, medicine, and object\nidentification. In this work, we experimentally demonstrate an LWIR\nmultispectral image sensor with three wavelength bands using optical elements\nmade of an aluminum-based plasmonic filter array sandwiched in germanium. To\nrealize the multispectral sensor, the filter arrays are then integrated into a\n3D printed wheel stacked on a low-resolution monochrome thermal sensor. Our\nprototype device is calibrated using a blackbody and its thermal output has\nbeen enhanced with computer vision methods. By applying a state-of-the-art deep\nlearning method, we have also reconstructed multispectral images to a better\nspatial resolution. Scientifically, our work demonstrates a versatile spectral\nthermography technique for detecting target signatures in the LWIR range and\nother advanced spectral analyses.", "title": "Longwave infrared multispectral image sensor system using aluminum-germanium plasmonic filter arrays", "author": "Noor E Karishma Shaik, Bryce Widdicombe, Dechuan Sun, Sam E John, Dongryeol Ryu, Ampalavanapillai Nirmalathas, Ranjith R Unnithan", "published": "2023-03-03", "arxiv_url": "http://arxiv.org/abs/2303.01661v1", "arxiv_id": "2303.01661v1"}
{"abstraction": "Extended Reality (XR) technologies are gaining traction as effective tools\nfor medical training and procedural guidance, particularly in complex cardiac\ninterventions. This paper presents a novel system for real-time 3D tracking and\nvisualization of intracardiac echocardiography (ICE) catheters, with precise\nmeasurement of the roll angle. A custom 3D-printed setup, featuring orthogonal\ncameras, captures biplane video of the catheter, while a specialized computer\nvision algorithm reconstructs its 3D trajectory, localizing the tip with\nsub-millimeter accuracy and tracking the roll angle in real-time. The system's\ndata is integrated into an interactive Unity-based environment, rendered\nthrough the Meta Quest 3 XR headset, combining a dynamically tracked catheter\nwith a patient-specific 3D heart model. This immersive environment allows the\ntesting of the importance of 3D depth perception, in comparison to 2D\nprojections, as a form of visualization in XR. Our experimental study,\nconducted using the ICE catheter with six participants, suggests that 3D\nvisualization is not necessarily beneficial over 2D views offered by the XR\nsystem; although all cardiologists saw its utility for pre-operative training,\nplanning, and intra-operative guidance. The proposed system qualitatively shows\ngreat promise in transforming catheter-based interventions, particularly ICE\nprocedures, by improving visualization, interactivity, and skill development.", "title": "Advanced XR-Based 6-DOF Catheter Tracking System for Immersive Cardiac Intervention Training", "author": "Mohsen Annabestani, Sandhya Sriram, S. Chiu Wong, Alexandros Sigaras, Bobak Mosadegh", "published": "2024-11-04", "arxiv_url": "http://arxiv.org/abs/2411.02611v1", "arxiv_id": "2411.02611v1"}
